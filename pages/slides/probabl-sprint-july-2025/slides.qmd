---
title: "Probabl Sprint - July 2025"
title-block-banner: true
date: 2025-07-09
subtitle: "Machine Learning with Dataframes"
author: "Riccardo Cappuzzo"
format: 
    revealjs:
        slide-number: c/t
        show-slide-number: all
        preview-links: auto
        embed-resources: false
        transition: slide
        theme: [simple]
        logo: images/skrub.svg
        css: style.css
incremental: true

---
## Fun facts

- I'm Italian, but I don't drink coffee, wine, and I like pizza with fries
- I did my PhD in Côte d'Azur, and I moved away because it was too sunny and 
I don't like the sea

# Boost your productivity with `skrub`! {auto-animate="true"}

`skrub` simplifies many tedious data preparation operations

## An example pipeline
1. Gather some data
2. Explore the data
3. Pre-process the data 
4. Perform feature engineering
5. Build a scikit-learn pipeline
6. ???
7. Profit?  


## Exploring the data {.smaller auto-animate="true"}
```python
import pandas as pd
import matplotlib.pyplot as plt
import skrub

dataset = skrub.datasets.fetch_employee_salaries()
employees, salaries = dataset.X, dataset.y

df = pd.DataFrame(employees)

# Plot the distribution of the numerical values using a histogram
fig, axs = plt.subplots(2,1, figsize=(10, 6))
ax1, ax2 = axs

ax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)
ax1.set_xlabel('Year first hired')
ax1.set_ylabel('Frequency')
ax1.grid(True, linestyle='--', alpha=0.5)

# Count the frequency of each category
category_counts = df['department'].value_counts()

# Create a bar plot
category_counts.plot(kind='bar', edgecolor='black', ax=ax2)

# Add labels and title
ax2.set_xlabel('Department')
ax2.set_ylabel('Frequency')
ax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis

fig.suptitle("Distribution of values")

# Show the plot
plt.show()
```
## Exploring the data {.smaller auto-animate="true"}

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import skrub
from skrub.datasets import fetch_employee_salaries
from pprint import pprint

dataset = fetch_employee_salaries()
employees, salaries = dataset.X, dataset.y

df = pd.DataFrame(employees)

# Plot the distribution of the numerical values using a histogram
fig, axs = plt.subplots(2,1, figsize=(10, 6))
ax1, ax2 = axs

ax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)
ax1.set_xlabel('Year first hired')
ax1.set_ylabel('Frequency')
ax1.grid(True, linestyle='--', alpha=0.5)

# Count the frequency of each category
category_counts = df['department'].value_counts()

# Create a bar plot
category_counts.plot(kind='bar', edgecolor='black', ax=ax2)

# Add labels and title
ax2.set_xlabel('Department')
ax2.set_ylabel('Frequency')
ax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis

fig.suptitle("Distribution of values")

# Show the plot
plt.show()
```


## Exploring the data with `skrub` {.smaller auto-animate="true"}

```{.python}
from skrub import TableReport
TableReport(employee_salaries)
```
[Preview](https://skrub-data.org/skrub-reports/examples/employee_salaries.html){preview-link="true"}


::: {.fragment}
::: {.nonincremental}
Main features:

- Obtain high-level statistics about the data
- Explore the distribution of values and find outliers
- Discover highly correlated columns 
- Export and share the report as an `html` file
:::
:::

## Data cleaning with Pandas {.smaller auto-animate="true"}
```{python}
#| echo: true
import pandas as pd
import numpy as np

data = {
    'Constant int': [1, 1, 1],  # Single unique value
    'B': [2, 3, 2],  # Multiple unique values
    'Constant str': ['x', 'x', 'x'],  # Single unique value
    'D': [4, 5, 6],  # Multiple unique values
    'All nan': [np.nan, np.nan, np.nan],  # All missing values 
    'All empty': ['', '', ''],  # All empty strings
    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],
}
df = pd.DataFrame(data)
display(df)
```


## Data cleaning with Pandas {.smaller auto-animate="true"}
```{python}
#| echo: true
# Parse the datetime strings with a specific format
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')

# Drop columns with only a single unique value
df_cleaned = df.loc[:, df.nunique(dropna=True) > 1]

# Function to drop columns with only missing values or empty strings
def drop_empty_columns(df):
    # Drop columns with only missing values
    df_cleaned = df.dropna(axis=1, how='all')
    # Drop columns with only empty strings
    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]
    df_cleaned = df_cleaned.drop(columns=empty_string_cols)
    return df_cleaned

# Apply the function to the DataFrame
df_cleaned = drop_empty_columns(df_cleaned)
display(df_cleaned)
```

## Lightweight data cleaning: `Cleaner` {.smaller auto-animate="true"}

```{python}
#| echo: true
from skrub import Cleaner
cleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')
df_cleaned = cleaner.fit_transform(df)
display(df_cleaned)
```


## Encoding datetime features with Pandas {.smaller}
```{python}
#| echo: true
import pandas as pd
data = {
    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],
    'value': [10, 20, 30]
}
df = pd.DataFrame(data)
df_expanded = df.copy()
datetime_column = "date"
df_expanded[datetime_column] = pd.to_datetime(df_expanded[datetime_column], errors='coerce')

df_expanded['year'] = df_expanded[datetime_column].dt.year
df_expanded['month'] = df_expanded[datetime_column].dt.month
df_expanded['day'] = df_expanded[datetime_column].dt.day
df_expanded['hour'] = df_expanded[datetime_column].dt.hour
df_expanded['minute'] = df_expanded[datetime_column].dt.minute
df_expanded['second'] = df_expanded[datetime_column].dt.second
```

## Encoding datetime features with Pandas {.smaller}
```{python}
#| echo: true
df_expanded['hour_sin'] = np.sin(2 * np.pi * df_expanded['hour'] / 24)
df_expanded['hour_cos'] = np.cos(2 * np.pi * df_expanded['hour'] / 24)

df_expanded['month_sin'] = np.sin(2 * np.pi * df_expanded['month'] / 12)
df_expanded['month_cos'] = np.cos(2 * np.pi * df_expanded['month'] / 12)

print("Original DataFrame:")
print(df)
print("\nDataFrame with expanded datetime columns:")
print(df_expanded)
```

## Encoding datetime features `skrub.DatetimeEncoder` {auto-animate="true" visibility="uncounted" .smaller}
```{python}
#| echo: true
from skrub import DatetimeEncoder, ToDatetime

de = DatetimeEncoder(periodic_encoding="circular")
X_date = ToDatetime().fit_transform(df["date"])
X_enc = de.fit_transform(X_date)
print(X_enc)
```

## What periodic features look like
![](images/periodic_features.png){fig-align="center"}



## Encoding _all the features_: `TableVectorizer` {.smaller auto-animate="true"}

![](images/skrub-table-vectorizer.png)

## Build a predictive pipeline {auto-animate="true"}
```{.python}
from sklearn.linear_model import Ridge
model = Ridge()
```

## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{.python code-line-numbers="3-6"}
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

model = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())
```


## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{.python code-line-numbers="3,5,6-17|"}
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_selector as selector
from sklearn.compose import make_column_transformer

categorical_columns = selector(dtype_include=object)(employees)
numerical_columns = selector(dtype_exclude=object)(employees)

ct = make_column_transformer(
      (StandardScaler(),
       numerical_columns),
      (OneHotEncoder(handle_unknown="ignore"),
       categorical_columns))

model = make_pipeline(ct, SimpleImputer(), Ridge())
```
## Build a predictive pipeline with `tabular_learner` {auto-animate="true" .smaller}
```{python}
#| echo: true
import skrub
from sklearn.linear_model import Ridge
model = skrub.tabular_learner(Ridge())
model
```


## 
![](images/drakeno.png){fig-align="center"}

## We now have a pipeline! {.smaller}

1. Gather some data
    - `skrub.datasets`, or user data
2. Explore the data
    - `skrub.TableReport`
3. Pre-process the data 
    - `skrub.TableVectorizer`, `Cleaner`, `DatetimeEncoder` ... 
4. Perform feature engineering
    - `skrub.TableVectorizer`, `TextEncoder`, `StringEncoder `...
5. Build a scikit-learn pipeline
    - `tabular_learner`, `sklearn.pipeline.make_pipeline` ... 
6. ???
7. Profit 📈 


# What if we had a *better* pipeline? 

## A realistic scenario
A data scientist needs to train a ML model, but features are spread across 
multiple tables. 

::: {.fragment}
::: {.callout-warning}
Many issues with this! 
:::

:::

::: {.incremental}
- `scikit-learn` pipelines support only a single feature matrix `X`
- Dataframe operations cannot be tuned
- Data leakage must be accounted for
- Persisting and reproducing operations is complex
:::

## `skrub` DataOps
When a normal pipe is not enough...

::: {.fragment style="font-size:2em;"}
... the `skrub` DataOps come to the rescue 🚒
:::


## DataOps...
- Extend the `scikit-learn` machinery to complex multi-table operations
- Track all operations with a computational graph (a *data plan*)
- Allow tuning any operation in the data plan
- Can be persisted and shared easily by generating a `learner`

## DataOps, Data Plans, `learner`s: oh my!  
- A `DataOp` (singular) wraps a single operation, and can be combined and concatenated with other `DataOps`. 

- The **Data Plan** is a collective name for a sequence and combination of `DataOps`. 

- The Data Plan can be exported as a standalone object called `learner`. The `learner` takes a dictionary of values rather than just `X` and `y`. 


## How do DataOps work, though? 
DataOps **wrap** around *user operations*, where user operations are:

- any dataframe operation (e.g., merge, group by, aggregate etc.)
- scikit-learn estimators (a Random Forest, RidgeCV etc.)
- custom user code (load data from a path, fetch from an URL etc.)

::: {.fragment}

::: {.callout-important}
DataOps _record_ user operations, so that they can later be _replayed_ in the same
order and with the same arguments on unseen data. 
:::
::: 


## Starting with the `DataOps`

```{python}
#| echo: true
data = skrub.datasets.fetch_credit_fraud()

baskets = skrub.var("baskets", data.baskets)
X = baskets[["ID"]].skb.mark_as_X()
y = baskets["fraud_flag"].skb.mark_as_y()

products = skrub.var("products", data.products) # add a new variable
```

:::{.incremental}
- `X`, `y`, `products` represent inputs to the pipeline.
- `skrub` splits `X` and `y` when training. 
:::

##  Building a full data plan
```{.python code-line-numbers="|4,5|1,5|6|7|8|9|"}
from skrub import selectors as s
from sklearn.ensemble import ExtraTreesClassifier  

vectorizer = skrub.TableVectorizer(high_cardinality=skrub.StringEncoder(), n_jobs=-1)
vectorized_products = products.skb.apply(vectorizer, cols=s.all() - "basket_ID")
aggregated_products = vectorized_products.groupby("basket_ID").agg("mean").reset_index()
features = X.merge(aggregated_products, left_on="ID", right_on="basket_ID")
features = features.drop(columns=["ID", "basket_ID"])
predictions = features.skb.apply(ExtraTreesClassifier(n_jobs=-1), y=y)
```

## Inspecting the data plan
```{.python}
predictions.skb.full_report()
```

<br/>

<a href="expression_report/index.html" target="_blank">report</a>

## Exporting the plan in a `learner` {.smaller}
The data plan can be exported as a `learner`:
```{.python}
# anywhere
learner = predictions.skb.make_learner()
# search is a HPO object
best_learner = search.skb.best_learner_
```
::: {.fragment}
Then, the `learner` can be pickled ...
```{.python}
import pickle

with open("learner.bin", "wb") as fp:
    pickle.dump(learner, fp)
```
:::

::: {.fragment}

... and loaded

```{.python}
with open("learner.bin", "rb") as fp:
    learner = pickle.load(fp)

learner.predict({"baskets": new_baskets, "products": new_products})
```
:::



## Hyperparameter tuning in a Data Plan 
`skrub` implements four `choose_*` functions:

- `choose_from`: select from the given list of options
- `choose_int`: select an integer within a range
- `choose_float`: select a float within a range
- `choose_bool`: select a bool 
- `optional`: chooses between a value or DataOp and no op


## Hyperparameter tuning in a Data Plan  {auto-animate="true"}
It's possible to nest these functions to create complex grids:
```python
X.skb.apply(
    skrub.choose_from(
        {
            "PCA": PCA(n_components=skrub.choose_int(10, 30)),
            "SelectKBest": SelectKBest(k=skrub.choose_int(10, 30))
        }, name="dim_reduction"
    )
)
```

## Tuning in `scikit-learn` can be complex {.smaller auto-animate="true"}

```{.python}
pipe = Pipeline([("dim_reduction", PCA()), ("regressor", Ridge())])
grid = [
    {
        "dim_reduction": [PCA()],
        "dim_reduction__n_components": [10, 20, 30],
        "regressor": [Ridge()],
        "regressor__alpha": loguniform(0.1, 10.0),
    },
    {
        "dim_reduction": [SelectKBest()],
        "dim_reduction__k": [10, 20, 30],
        "regressor": [Ridge()],
        "regressor__alpha": loguniform(0.1, 10.0),
    },
    {
        "dim_reduction": [PCA()],
        "dim_reduction__n_components": [10, 20, 30],
        "regressor": [RandomForestClassifier()],
        "regressor__n_estimators": loguniform(20, 200),
    },
    {
        "dim_reduction": [SelectKBest()],
        "dim_reduction__k": [10, 20, 30],
        "regressor": [RandomForestClassifier()],
        "regressor__n_estimators": loguniform(20, 200),
    },
]
model = RandomizedSearchCV(pipe, grid)
```
## Tuning with `DataOps` is simple! {.smaller} 

```python
dim_reduction = X.skb.apply(
    skrub.choose_from(
        {
            "PCA": PCA(n_components=skrub.choose_int(10, 30)),
            "SelectKBest": SelectKBest(k=skrub.choose_int(10, 30))
        }, name="dim_reduction"
    )
)
regressor = dim_reduction.skb.apply(
    skrub.choose_from(
        {
            "Ridge": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),
            "RandomForest": RandomForestClassifier(
                n_estimators=skrub.choose_int(20, 200, log=True)
            )
        }, name="regressor"
    )
)
regressor.skb.get_randomized_search(scoring="roc_auc", fitted=True)
```

## Observe the impact of the hyperparameters {auto-animate="true"} 

```{.python}
search = pred.skb.get_randomized_search(scoring="roc_auc", fitted=True)

search.plot_parallel_coord()
```

![](images/plot-parallel-coord.png){fig-align="center"}


## tl;dw
`skrub` provides

::: {.nonincremental}
- interactive data exploration
- automated pre-processing of pandas and polars dataframes
- powerful feature engineering
- soon™️, DataOps, data plans, hyperparameter tuning, (almost) no leakage 
:::

# That's it!

##  Getting involved
::: {.nonincremental}
- [Skrub website](https://skrub-data.org/stable/) (QR code below!)
- [Skrub materials website](https://skrub-data.org/skrub-materials/index.html)
- [Git repository](https://github.com/skrub-data/skrub/)
- [Discord server](https://discord.gg/ABaPnm7fDC)
- [Bluesky](https://bsky.app/profile/skrub-data.bsky.social)
:::

![](images/qr-code.png){.absolute bottom=0 right=0 width="250" height="250"}
