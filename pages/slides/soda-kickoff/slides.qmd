---
title: "Skrub"
title-block-banner: true
date: 2025-06-03
subtitle: "SODA Kickoff 2025 - Machine Learning with Dataframes"
author: "Riccardo Cappuzzo"
format: 
    revealjs:
        slide-number: c/t
        show-slide-number: all
        preview-links: auto
        embed-resources: false
        transition: slide
        theme: [simple]
        logo: images/skrub.svg
        css: style.css
incremental: true

---
## Fun facts

- I'm Italian, but I don't drink coffee, wine, and I like pizza with fries
- I did my PhD in C√¥te d'Azur, and I moved away because it was too sunny and 
I don't like the sea

## Fun facts {auto-animate="true"}
::: {.nonincremental}
- I'm mildly obsessed with matplotlib
:::
![](images/ob.jpg){fig-align="center"}

# Boost your productivity with `skrub`! {auto-animate="true"}

`skrub` simplifies many tedious data preparation operations

## An example pipeline
1. Gather some data
2. Explore the data
3. Pre-process the data 
4. Perform feature engineering
5. Build a scikit-learn pipeline
6. ???
7. Profit?  


## Exploring the data {.smaller auto-animate="true"}
```python
import pandas as pd
import matplotlib.pyplot as plt
import skrub

dataset = skrub.datasets.fetch_employee_salaries()
employees, salaries = dataset.X, dataset.y

df = pd.DataFrame(employees)

# Plot the distribution of the numerical values using a histogram
fig, axs = plt.subplots(2,1, figsize=(10, 6))
ax1, ax2 = axs

ax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)
ax1.set_xlabel('Year first hired')
ax1.set_ylabel('Frequency')
ax1.grid(True, linestyle='--', alpha=0.5)

# Count the frequency of each category
category_counts = df['department'].value_counts()

# Create a bar plot
category_counts.plot(kind='bar', edgecolor='black', ax=ax2)

# Add labels and title
ax2.set_xlabel('Department')
ax2.set_ylabel('Frequency')
ax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis

fig.suptitle("Distribution of values")

# Show the plot
plt.show()
```
## Exploring the data {.smaller auto-animate="true"}

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import skrub
from skrub.datasets import fetch_employee_salaries
from pprint import pprint

dataset = fetch_employee_salaries()
employees, salaries = dataset.X, dataset.y

df = pd.DataFrame(employees)

# Plot the distribution of the numerical values using a histogram
fig, axs = plt.subplots(2,1, figsize=(10, 6))
ax1, ax2 = axs

ax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)
ax1.set_xlabel('Year first hired')
ax1.set_ylabel('Frequency')
ax1.grid(True, linestyle='--', alpha=0.5)

# Count the frequency of each category
category_counts = df['department'].value_counts()

# Create a bar plot
category_counts.plot(kind='bar', edgecolor='black', ax=ax2)

# Add labels and title
ax2.set_xlabel('Department')
ax2.set_ylabel('Frequency')
ax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis

fig.suptitle("Distribution of values")

# Show the plot
plt.show()
```


## Exploring the data with `skrub` {.smaller auto-animate="true"}

```{.python}
from skrub import TableReport
TableReport(employee_salaries)
```
[Preview](https://skrub-data.org/skrub-reports/examples/employee_salaries.html){preview-link="true"}


::: {.fragment}
::: {.nonincremental}
Main features:

- Obtain high-level statistics about the data
- Explore the distribution of values and find outliers
- Discover highly correlated columns 
- Export and share the report as an `html` file
:::
:::

## Data cleaning with Pandas {.smaller auto-animate="true"}
```{python}
#| echo: true
import pandas as pd
import numpy as np

data = {
    'A': [1, 1, 1],  # Single unique value
    'B': [2, 3, 2],  # Multiple unique values
    'C': ['x', 'x', 'x'],  # Single unique value
    'D': [4, 5, 6],  # Multiple unique values
    'E': [np.nan, np.nan, np.nan],  # All missing values 
    'F': ['', '', ''],  # All empty strings
    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],
}
df = pd.DataFrame(data)
display(df)
```


## Data cleaning with Pandas {.smaller auto-animate="true"}
```{python}
#| echo: true
# Parse the datetime strings with a specific format
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')

# Drop columns with only a single unique value
df_cleaned = df.loc[:, df.nunique(dropna=True) > 1]

# Function to drop columns with only missing values or empty strings
def drop_empty_columns(df):
    # Drop columns with only missing values
    df_cleaned = df.dropna(axis=1, how='all')
    # Drop columns with only empty strings
    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]
    df_cleaned = df_cleaned.drop(columns=empty_string_cols)
    return df_cleaned

# Apply the function to the DataFrame
df_cleaned = drop_empty_columns(df_cleaned)
display(df_cleaned)
```

## Lightweight data cleaning: `Cleaner` {.smaller auto-animate="true"}

```{python}
#| echo: true
from skrub import Cleaner
cleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')
df_cleaned = cleaner.fit_transform(df)
display(df_cleaned)
```


## Encoding datetime features with Pandas {.smaller}
```{python}
#| echo: true
import pandas as pd
data = {
    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],
    'value': [10, 20, 30]
}
df = pd.DataFrame(data)
df_expanded = df.copy()
datetime_column = "date"
df_expanded[datetime_column] = pd.to_datetime(df_expanded[datetime_column], errors='coerce')

df_expanded['year'] = df_expanded[datetime_column].dt.year
df_expanded['month'] = df_expanded[datetime_column].dt.month
df_expanded['day'] = df_expanded[datetime_column].dt.day
df_expanded['hour'] = df_expanded[datetime_column].dt.hour
df_expanded['minute'] = df_expanded[datetime_column].dt.minute
df_expanded['second'] = df_expanded[datetime_column].dt.second
```

## Encoding datetime features with Pandas {.smaller}
```{python}
#| echo: true
df_expanded['hour_sin'] = np.sin(2 * np.pi * df_expanded['hour'] / 24)
df_expanded['hour_cos'] = np.cos(2 * np.pi * df_expanded['hour'] / 24)

df_expanded['month_sin'] = np.sin(2 * np.pi * df_expanded['month'] / 12)
df_expanded['month_cos'] = np.cos(2 * np.pi * df_expanded['month'] / 12)

print("Original DataFrame:")
print(df)
print("\nDataFrame with expanded datetime columns:")
print(df_expanded)
```

## Encoding datetime features `skrub.DatetimeEncoder` {auto-animate="true" visibility="uncounted" .smaller}
```{python}
#| echo: true
from skrub import DatetimeEncoder, ToDatetime

de = DatetimeEncoder(periodic_encoding="circular")
X_date = ToDatetime().fit_transform(df["date"])
X_enc = de.fit_transform(X_date)
print(X_enc)
```
}
## Encoding _all the features_: `TableVectorizer` {.smaller auto-animate="true"}

![](images/skrub-table-vectorizer.png)

## Build a predictive pipeline {auto-animate="true"}
```{.python}
from sklearn.linear_model import Ridge
model = Ridge()
```

## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{.python code-line-numbers="3-6"}
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

model = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())
```


## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{.python code-line-numbers="3,5,6-17|"}
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_selector as selector
from sklearn.compose import make_column_transformer

categorical_columns = selector(dtype_include=object)(employees)
numerical_columns = selector(dtype_exclude=object)(employees)

ct = make_column_transformer(
      (StandardScaler(),
       numerical_columns),
      (OneHotEncoder(handle_unknown="ignore"),
       categorical_columns))

model = make_pipeline(ct, SimpleImputer(), Ridge())
```
## Build a predictive pipeline with `tabular_learner` {auto-animate="true" .smaller}
```{python}
#| echo: true
import skrub
from sklearn.linear_model import Ridge
model = skrub.tabular_learner(Ridge())
model
```
## 
![](images/drakeno.png){fig-align="center"}

## We now have a pipeline! {.smaller}

1. Gather some data
    - `skrub.datasets`, or user data
2. Explore the data
    - `skrub.TableReport`
3. Pre-process the data 
    - `skrub.TableVectorizer`, `Cleaner`, `DatetimeEncoder` ... 
4. Perform feature engineering
    - `skrub.TableVectorizer`, `TextEncoder`, `StringEncoder `...
5. Build a scikit-learn pipeline
    - `tabular_learner`, `sklearn.pipeline.make_pipeline` ... 
6. ???
7. Profit üìà 


# What if we had a *better* pipeline? 

## `skrub` expressions
When a normal pipe is not enough...

::: {.fragment}
Expressions come to the rescue üöí:
:::

::: {.incremental}
- Keep track of train, validation and test splits to avoid data leakage
- Simplify hyperparameter tuning and reporting
- Handle complex pipelines that involve multiple tables and custom
- Persist all objects for reproducibility
:::


## Starting with expressions

```{.python}
data = skrub.datasets.fetch_credit_fraud()

X = skrub.X(data.baskets[["ID"]]) # mark as "X"
y = skrub.y(data.baskets["fraud_flag"]) # mark as "y"
products = skrub.var("products", data.products) # add a new variable
```

:::{.incremental}
- `X`, `y`, `products` represent inputs to the pipeline
- `skrub` keeps track of splits
:::


## Build and inspect complex pipelines
```{.python}
pred.skb.full_report()
```

<br/>

<a href="expression_report/index.html" target="_blank">report</a>

## Hyperparameter tuning in `scikit-learn` {.smaller auto-animate="true"}

```{.python}
pipe = Pipeline([("dim_reduction", PCA()), ("regressor", Ridge())])
grid = [
    {
        "dim_reduction": [PCA()],
        "dim_reduction__n_components": [10, 20, 30],
        "regressor": [Ridge()],
        "regressor__alpha": loguniform(0.1, 10.0),
    },
    {
        "dim_reduction": [SelectKBest()],
        "dim_reduction__k": [10, 20, 30],
        "regressor": [Ridge()],
        "regressor__alpha": loguniform(0.1, 10.0),
    },
    {
        "dim_reduction": [PCA()],
        "dim_reduction__n_components": [10, 20, 30],
        "regressor": [RandomForestClassifier()],
        "regressor__n_estimators": loguniform(20, 200),
    },
    {
        "dim_reduction": [SelectKBest()],
        "dim_reduction__k": [10, 20, 30],
        "regressor": [RandomForestClassifier()],
        "regressor__n_estimators": loguniform(20, 200),
    },
]
model = RandomizedSearchCV(pipe, grid)
```
## Hyperparameter tuning with `skrub` expressions {auto-animate="true" .smaller} 

```python
dim_reduction = X.skb.apply(
    skrub.choose_from(
        {
            "PCA": PCA(n_components=skrub.choose_int(10, 30)),
            "SelectKBest": SelectKBest(k=skrub.choose_int(10, 30))
        }, name="dim_reduction"
    )
)
regressor = dim_reduction.skb.apply(
    skrub.choose_from(
        {
            "Ridge": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),
            "RandomForest": RandomForestClassifier(
                n_estimators=skrub.choose_int(20, 200, log=True)
            )
        }, name="regressor"
    )
)
regressor.skb.get_randomized_search(scoring="roc_auc", fitted=True)
```

## Observe the impact of the hyperparameters {auto-animate="true"} 

```{.python }
search = pred.skb.get_randomized_search(scoring="roc_auc", fitted=True)

search.plot_parallel_coord()
```

![](images/plot-parallel-coord.png){fig-align="center"}

## tl;dw
`skrub` provides

::: {.nonincremental}
- interactive data exploration
- automated pre-processing of pandas and polars dataframes
- powerful feature engineering
- soon‚Ñ¢Ô∏è, complex pipelines, hyperparameter tuning, (almost) no leakage 
:::

# That's it!

##  Getting involved
::: {.nonincremental}
- [Skrub website](https://skrub-data.org/stable/) (QR code below!)
- [Skrub materials website](https://skrub-data.org/skrub-materials/index.html)
- [Git repository](https://github.com/skrub-data/skrub/)
- [Discord server](https://discord.gg/ABaPnm7fDC)
- [Bluesky](https://bsky.app/profile/skrub-data.bsky.social)
:::

![](images/qr-code.png){.absolute bottom=0 right=0 width="250" height="250"}
