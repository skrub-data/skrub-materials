---
title: "Skrub"
date: 2025-06-03
subtitle: "Machine Learning with Dataframes"
author: "Riccardo Cappuzzo"
format: 
    revealjs:
        slide-number: c/t
        show-slide-number: all
        preview-links: auto
        embed-resources: false
        transition: slide
        theme: simple
        logo: images/skrub.svg
incremental: true

---
# Presentation roadmap
- Introducing `skrub` with an example
- `skrub`'s main features
- Upcoming feature: `skrub` expressions

## In the beginning...
`skrub` is an evolution of `dirty_cat`, a package that provided support
for handling dirty columns and perform fuzzy joins across tables. 

<br>

::: {.fragment}
`skrub` now provides:

::: {.nonincremental}
- interactive data exploration
- automated pre-processing of pandas and polars dataframes
- powerful feature engineering
:::

:::

## Skrub's vision
- The goal of `skrub` is to facilitate building and deploying machine-learning models 
on pandas and polars dataframes, while matching the high 
quality standards of scikit-learn. 

- `skrub` fully supports both `pandas` and `polars` as dataframe engines, and 
we are planning to extend support to DB engines in the future.


# Let's set the stage {auto-animate="true"}

## An example use case {auto-animate="true"}
1. Gather some data
    - Employee salaries, census, customer churn...
2. Explore the data
    - Null values, dtypes, correlated features...
3. Pre-process the data 
4. Build a scikit-learn pipeline
5. ???
6. Profit ðŸ“ˆ 


## Exploring the data {.smaller}
```{python}
#| echo: true
import skrub
import pandas as pd
from skrub.datasets import fetch_employee_salaries

dataset = fetch_employee_salaries()
employees, salaries = dataset.X, dataset.y

employees.describe(include="all")
```

## Exploring the data... interactively! {.smaller}

```{.python}
from skrub import TableReport
TableReport(employee_salaries)
```
[Preview](https://skrub-data.org/skrub-reports/examples/employee_salaries.html){preview-link="true"}


::: {.fragment}
::: {.nonincremental}
Main features:

- Obtain high-level statistics about the data (number of uniques, missing values...)
- Explore the distribution of values and find outliers
- Discover highly correlated columns 
- Export and share the report as an `html` file
:::
:::

::: {.fragment}
[More examples here](https://skrub-data.org/skrub-reports/examples/)
:::

## Build a predictive pipeline {auto-animate="true" .smaller}
```{.python}
```

## Build a predictive pipeline {auto-animate="true"}
```{.python}
from sklearn.linear_model import Ridge
model = Ridge()
```

## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{.python code-line-numbers="2-5"}
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

model = make_pipeline(StandardScaler(), Ridge())
```

## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{.python code-line-numbers="4,6"}
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

model = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())
```


## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{.python code-line-numbers="3,5,6-17"}
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_selector as selector
from sklearn.compose import make_column_transformer

categorical_columns = selector(dtype_include=object)(employees)
numerical_columns = selector(dtype_exclude=object)(employees)

ct = make_column_transformer(
      (StandardScaler(),
       numerical_columns),
      (OneHotEncoder(handle_unknown="ignore"),
       categorical_columns))

model = make_pipeline(ct, SimpleImputer(), Ridge())
```


## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{.python}
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_selector as selector
from sklearn.compose import make_column_transformer

categorical_columns = selector(dtype_include=object)(employees)
numerical_columns = selector(dtype_exclude=object)(employees)

ct = make_column_transformer(
      (StandardScaler(),
       numerical_columns),
      (OneHotEncoder(handle_unknown="ignore"),
       categorical_columns))

model = make_pipeline(ct, SimpleImputer(), Ridge())
```


## Build a predictive pipeline {auto-animate="true" visibility="uncounted"}
```{python}
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import Ridge
from sklearn.compose import make_column_selector as selector
from sklearn.compose import make_column_transformer

categorical_columns = selector(dtype_include=object)(employees)
numerical_columns = selector(dtype_exclude=object)(employees)

ct = make_column_transformer(
      (StandardScaler(),
       numerical_columns),
      (OneHotEncoder(handle_unknown="ignore"),
       categorical_columns))
       
model = make_pipeline(ct, SimpleImputer(), Ridge())
model
```

::: {.fragment}
Kind of messy...
:::

## Enter: `skrub.tabular_learner` {auto-animate="true"}
```{.python}
import skrub
from sklearn.linear_model import Ridge
model = skrub.tabular_learner(Ridge())
```

## Enter: `skrub.tabular_learner` {auto-animate="true" .smaller }
```{python}
from sklearn.linear_model import Ridge
model = skrub.tabular_learner(Ridge())
model
```


## 
![](images/drakeno.png){fig-align="center"}


## A robust baseline: `tabular_learner` {.smaller}
Given a scikit-learn estimator, `tabular_learner` builds a scikit-learn pipeline that:

- performs feature engineering (parsing dates, encoding categorical features) 
- imputes missing values with `SimpleImputer` (optional)
- scales the data with `StandardScaler` (optional)
- concludes with the given estimator

::: {.fragment}
Alternatively, "`tabular_learner("regressor")`":
```{python}
skrub.tabular_learner("regressor")
```

:::

## We now have a pipeline! 

::: {.fragment}
![](images/oxi-clean-but-wait-theres-more.gif){fig-align="center"}
:::

# Skrub's main features

## Unmasking the `tabular_learner`
![](images/unmasking-meme-template.png){fig-align="center"}


## Advanced Feature Engineering: `TableVectorizer` {.smaller auto-animate="true"}
1. Pre-process the data
    - Detect missing values written as strings (e.g., `N/A`)
    - Drop **uninformative columns** (all missing, constant...)
    - Convert numerical dtypes to `np.float32`
    - Parse **datetimes**, ensuring consistent dtype and timezone
    - Identify low- and high-cardinality **categorical features** 
2. Convert complex data types (datetimes, text) into numerical features 

## Advanced Feature Engineering: `TableVectorizer` {.smaller auto-animate="true"}

![](images/skrub-table-vectorizer.png)

## Advanced Feature Engineering: `TableVectorizer` {.smaller auto-animate="true"}
Convert complex data types (datetimes, text) into numerical features 

- Encode dates with `DateTimeEncoder`
- Encode low-cardinality features (<=40 cat.) with `OneHotEncoder`
- Encode high-cardinality features (>40 cat.) with:
    - `StringEncoder`: Best trade-off between compute cost and embeddings quality. Tf-idf followed by SVD. 
    - `GapEncoder`: Relatively slow, easily interpretable, good quality embeddings. Target encoding and hashing. 
    - `MinHashEncoder`: Very fast, somewhat low quality embeddings. Hashing ngrams.
    - `TextEncoder`: Very slow, relies on language models, best solution for text and when context is available.

## Advanced Feature Engineering: `TableVectorizer` {.smaller auto-animate="true"}
::: {.fragment}
```{python}
#| echo: true
vectorizer = skrub.TableVectorizer()
transformed = vectorizer.fit_transform(employees)
from pprint import pprint

pprint(vectorizer.column_to_kind_)
```
:::

::: {.fragment}
```{python}
#| echo: true
pprint(vectorizer.all_processing_steps_["date_first_hired"])
```
:::

## Lightweight data cleaning: `Cleaner` {.smaller}
The `Cleaner` performs the same cleaning steps as the `TableVectorizer`, but does not 
convert numerical data and does not add new features.

The `Cleaner` is useful for:

- Having a consistent representation of missing values
- Filtering out uninformative columns
- Parsing dates

## Lightweight data cleaning: `Cleaner` {.smaller auto-animate="true" visibility="uncounted"}
```{python}
#| echo: true
data = fetch_employee_salaries()
X = data.X
print("X")
pprint(X.dtypes)
```

::: {.fragment}
```{python}
#| echo: true
cleaner = skrub.Cleaner()
X_clean = cleaner.fit_transform(X)
print("X_clean")
pprint(X_clean.dtypes)
```
:::

## Encoding datetime features: `DatetimeEncoder` {auto-animate="true"}
```{.python}
import skrub
data = skrub.datasets.fetch_bike_sharing()
X = data.X
```

## Encoding datetime features: `DatetimeEncoder` {auto-animate="true" visibility="uncounted"}
```{.python code-line-numbers="5-9"}
import skrub
data = skrub.datasets.fetch_bike_sharing()
X = data.X

from skrub import DatetimeEncoder, ToDatetime

de = DatetimeEncoder(periodic_encoding="spline", add_weekday=True)
X_date = ToDatetime().fit_transform(X["date"])
X_enc = de.fit_transform(X_date)

```

## Encoding datetime features: `DatetimeEncoder` {auto-animate="true" visibility="uncounted"}
```{python}
#| echo: true
import skrub
data = skrub.datasets.fetch_bike_sharing()
X = data.X

from skrub import DatetimeEncoder, ToDatetime

de = DatetimeEncoder(periodic_encoding="spline", add_weekday=True)
X_date = ToDatetime().fit_transform(X["date"])
X_enc = de.fit_transform(X_date)

de.all_outputs_
```

## Encoding datetime features: `DatetimeEncoder` {auto-animate="true"}
![](images/demo_periodic_slides.png)

## Augmenting features through joining {.smaller}
- `Joiner`:  Perform fuzzy-joining: join columns that contain similar-looking values. 
- `AggJoiner`: Aggregate an auxiliary dataframe before joining it on a base dataframe, and create new features that aggregate (sum, mean, mode...) the values in the columns. 
- `MultiAggJoiner` extends `AggJoiner` to a multi-table scenario.
- `InterpolationJoiner`: Perform an equi-join and estimate what missing rows would contain if they existed in the table.


## Back to our pipeline {.smaller}

1. Gather some data
    - `skrub.datasets`, or user data
2. Explore the data
    - `skrub.TableReport`
3. Pre-process the data 
    - `skrub.TableVectorizer`, `Cleaner`, `DatetimeEncoder` ... 
4. Build a scikit-learn pipeline
    - `tabular_learner`, `sklearn.pipeline.make_pipeline` ... 
5. ???
6. Profit ðŸ“ˆ 

::: {.fragment}

::: {.callout-important}
What if we could do better? 
:::

:::

# Upcoming major feature: `skrub` expressions

## Building complex pipelines

- Our pipeline may involve several data-processing steps
  - joining tables
  - selecting columns
  - applying machine-learning estimators
- Some steps have states that need to be fitted
- Often several tables and aggregations are involved
- Hyperparameter optimization should be performed
- Custom code may be needed 

## Example: e-commerce fraud detection 

- We have e-commerce check-out baskets
- Each basket contains one or more products
- Target: Predict if the transaction is fraudulent

<br/>

::: {.fragment}
<a target="_blank" href="dataset-overview.html">Dataset</a>
:::

## A first attempt with scikit-learn... 

## Loading data

```{.python}
data = skrub.datasets.fetch_credit_fraud()

X = data.baskets[["ID"]]
y = data.baskets["fraud_flag"]
products = data.products
```
Here we load two tables (`X` and `products`), but scikit-learn assumes 
a single table `X` with the right shape.

## Encoding the products {auto-animate="true"} 

```{.python}
product_vectorizer = skrub.TableVectorizer(
    high_cardinality=skrub.StringEncoder(n_components=5)
)

vectorized_products = product_vectorizer.fit_transform(data.products)
```

![](images/credit-fraud.svg){fig-align="center"}

## Encoding the products {auto-animate="true" visibility="uncounted"} 
```{.python}
product_vectorizer = skrub.TableVectorizer(
    high_cardinality=skrub.StringEncoder(n_components=5)
)

vectorized_products = product_vectorizer.fit_transform(data.products)
```
ðŸ¤”

::: {.incremental}

- How to store `product_vectorizer`?
- The vectorizer is fitted on whole `products` table: **data leakage**
- Hyper-parameters cannot be tuned easily
- Transforming only some columns is hard
  - `ColumnTransformer` ðŸ˜ŸðŸ˜°  

:::
## Joining the product features {auto-animate="true"} 

```{.python}
aggregated_products = (
    vectorized_products.groupby("basket_ID").agg("mean").reset_index()
)
X = X.merge(aggregated_products, left_on="ID", right_on="basket_ID").drop(
    columns=["ID", "basket_ID"]
)
```
::: {.fragment}
ðŸ¤”
:::

::: {.incremental}

- How to keep track of these transformations?
- Choices cannot be tuned 
    - what if we wanted to test different aggregation functions? 

:::

## Adding the supervised estimator

```{.python}
classifier = HistGradientBoostingClassifier()

cross_val_score(classifier, X, y, scoring="roc_auc", n_jobs=5)
```

## Skrub to the rescue

- Build complex pipelines involving multiple tables
- Avoid leakage by keeping track of crossvalidation splits
- Simplify hyperparameter tuning
- Seamlessly embed dataframe    and custom code

## Loading data, take \#2

```{.python}
data = skrub.datasets.fetch_credit_fraud()

X = skrub.X(data.baskets[["ID"]]) # mark as "X"
y = skrub.y(data.baskets["fraud_flag"]) # mark as "y"
products = skrub.var("products", data.products) # add a new variable
```

:::{.incremental}

- `X`, `y`, `products` represent inputs to the pipeline
- Operations on those objects are evaluated lazily
- A preview is available for interactive development

:::

## Encoding the products {auto-animate="true"}

```{.python code-line-numbers="3"}
from skrub import selectors as s

products = products[products["basket_ID"].isin(X["ID"])] # dataframe function

product_vectorizer = skrub.TableVectorizer(
    high_cardinality=skrub.StringEncoder(n_components=5)
)
vectorized_products = products.skb.apply(
    product_vectorizer, cols=s.all() - "basket_ID" )
```

We can filter `products` based on `X` $\rightarrow$ *No leakage!*


## Encoding the products {auto-animate="true" visibility="uncounted"}

```{.python code-line-numbers="8-10"}
from skrub import selectors as s

products = products[products["basket_ID"].isin(X["ID"])]

product_vectorizer = skrub.TableVectorizer(
    high_cardinality=skrub.StringEncoder(n_components=5)
)
vectorized_products = products.skb.apply(
    product_vectorizer, cols=s.all() - "basket_ID" # all cols except "basked_ID"
)
```
`product_vectorizer` is added to the pipeline 


## Encoding the products {auto-animate="true" visibility="uncounted"} 

```{.python code-line-numbers="1,9"}
from skrub import selectors as s

products = products[products["basket_ID"].isin(X["ID"])]

product_vectorizer = skrub.TableVectorizer(
    high_cardinality=skrub.StringEncoder(n_components=5)
)
vectorized_products = products.skb.apply(
    product_vectorizer, cols=s.all() - "basket_ID"
)
```
We can select columns to transform using selectors


## Encoding the products {auto-animate="true" visibility="uncounted"} 

```{.python code-line-numbers="7"}
from skrub import selectors as s

products = products[products["basket_ID"].isin(X["ID"])]

product_vectorizer = skrub.TableVectorizer(
    high_cardinality=skrub.StringEncoder(
        n_components=skrub.choose_int(2, 20)
    )
)
vectorized_products = products.skb.apply(
    product_vectorizer, cols=s.all() - "basket_ID"
)
```
We can tune hyperparameters (more later)


## Joining the product features

```{.python}
aggregated_products = (
    vectorized_products.groupby("basket_ID").agg("mean").reset_index()
)
X = X.merge(aggregated_products, left_on="ID", right_on="basket_ID").drop(
    columns=["ID", "basket_ID"]
)
```

- Transformations are added to the model
- We can tune choices
- We retain access to all the dataframe's functionality

## Adding the supervised estimator

```{.python}
classifier = HistGradientBoostingClassifier()
pred = X.skb.apply(classifier, y=y)
```

Evaluation

```{.python}
pred.skb.cross_validate(scoring="roc_auc", n_jobs=5)
```

Training, saving & re-using a model

```{.python filename="train.py"}
pipeline = pred.skb.get_pipeline(fitted=True)
with open("pipeline.pickle", "wb") as ostream:
    pickle.dump(pipeline, ostream)
```

```{.python filename="predict.py"}
with open("pipeline.pickle", "rb") as istream:
    pipeline = pickle.load(istream)

pipeline.predict({'X': unseen_baskets, 'products': unseen_products})
```

## Easy inspection of the pipeline

```{.python}
pred.skb.full_report()
```

<br/>

<a href="expression_report/index.html" target="_blank">report</a>

## Hyperparameter tuning

- Any choice in the pipeline can be tuned
- Options are specified inline
- Inspecting results is easy

## Hyperparameter tuning {.smaller auto-animate="true"}

Without `skrub`: ðŸ˜­ðŸ˜­ðŸ˜­

```{.python}
pipe = Pipeline([("dim_reduction", PCA()), ("regressor", Ridge())])
grid = [
    {
        "dim_reduction": [PCA()],
        "dim_reduction__n_components": [10, 20, 30],
        "regressor": [Ridge()],
        "regressor__alpha": loguniform(0.1, 10.0),
    },
    {
        "dim_reduction": [SelectKBest()],
        "dim_reduction__k": [10, 20, 30],
        "regressor": [Ridge()],
        "regressor__alpha": loguniform(0.1, 10.0),
    },
    {
        "dim_reduction": [PCA()],
        "dim_reduction__n_components": [10, 20, 30],
        "regressor": [RandomForestClassifier()],
        "regressor__n_estimators": loguniform(20, 200),
    },
    {
        "dim_reduction": [SelectKBest()],
        "dim_reduction__k": [10, 20, 30],
        "regressor": [RandomForestClassifier()],
        "regressor__n_estimators": loguniform(20, 200),
    },
]
model = RandomizedSearchCV(pipe, grid)
```
## Hyperparameter tuning {auto-animate="true"} 


```{.python code-line-numbers="3-4,9"}
encoder_text_like = skrub.choose_from(
    {
        "string": StringEncoder(n_components=skrub.choose_int(10, 30)),
        "minhash": MinHashEncoder(n_components=skrub.choose_int(10, 30)),
    },
    name="encoder_text_like",
)
hgb = HistGradientBoostingRegressor(
    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name="lr")
)
```

With `skrub`: replace any value with a range


## Hyperparameter tuning {auto-animate="true" visibility="uncounted"} 
```{.python code-line-numbers="9"}
encoder_text_like = skrub.choose_from(
    {
        "string": StringEncoder(n_components=skrub.choose_int(10, 30)),
        "minhash": MinHashEncoder(n_components=skrub.choose_int(10, 30)),
    },
    name="encoder_text_like",
)
hgb = HistGradientBoostingRegressor(
    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name="lr")
)
```
Generate choices for scikit-learn estimators

## Hyperparameter tuning {auto-animate="true" visibility="uncounted"} 

```{.python code-line-numbers="1-7"}
encoder_text_like = skrub.choose_from(
    {
        "string": StringEncoder(n_components=skrub.choose_int(10, 30)),
        "minhash": MinHashEncoder(n_components=skrub.choose_int(10, 30)),
    },
    name="encoder_text_like",
)
hgb = HistGradientBoostingRegressor(
    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name="lr")
)
```
Use scikit-learn estimators as choice values

## Observe the impact of the hyperparameters {auto-animate="true"} 

```{.python code-line-numbers="|11"}
product_vectorizer = skrub.TableVectorizer(
    high_cardinality=skrub.StringEncoder(
        n_components=skrub.choose_int(2, 20)
    )
)

# ...

search = pred.skb.get_randomized_search(scoring="roc_auc", fitted=True)

search.plot_parallel_coord()
```
<br/>

:::{.fragment}

<a href="parallel_coord.html" target="_blank">parallel coordinates plot</a>

:::


# Getting involved

## Install skrub
:::fragment
Base installation:
```shell
# with pip
pip install skrub -U
# with conda
conda install -c conda-forge skrub
```
:::

:::fragment
For deep learning features such as `TextEncoder`:
```shell
# with pip
pip install skrub[transformers] -U
# with conda
conda install -c conda-forge skrub[transformers]
```
:::

:::fragment
[Documentation](https://skrub-data.org/stable/install.html){preview-link="true"}
:::

## Join the community 

::: {.nonincremental}
- [Skrub website](https://skrub-data.org/stable/)
- [Skrub materials website](https://skrub-data.org/skrub-materials/index.html)
- [Git repository](https://github.com/skrub-data/skrub/)
- [Discord server](https://discord.gg/ABaPnm7fDC)
- [Bluesky](https://bsky.app/profile/skrub-data.bsky.social)
:::

## Contribute to skrub
::: {.nonincremental}
- Open an [issue](https://github.com/skrub-data/skrub/issues) on GitHub
- Check out the [documentation](https://skrub-data.org/stable/CONTRIBUTING.html) on how to contribute
:::