---
title: Introduction to Skrub
author: Jérôme Dockès
date: 2025/01/29
format:
  html:
    code-fold: false
jupyter:
  jupytext:
    cell_metadata_filter: '-all'
    main_language: python
    notebook_metadata_filter: '-all'
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
    path: /home/jerome/miniforge3/envs/skb/share/jupyter/kernels/python3
---

**Machine Learning for dataframes**

# Machine learning and tabular data

[skrub-data.org](https://skrub-data.org/)

- ML expects numeric arrays
- Real data is more complex:
  - Multiple tables
  - Dates, categories, text, locations, …

**Skrub: bridge the gap between dataframes and scikit-learn.**

## Skrub helps at several stages of a tabular learning project

1. What's in the data? (EDA)
1. Can we learn anything? (baselines)
1. How do I represent the data? (feature extraction)
1. How do I bring it all together? (building a pipeline)

# 1. What's in the data?

```{python}
import skrub
import pandas as pd

employee_salaries = pd.read_csv("https://figshare.com/ndownloader/files/51755822")
employees = employee_salaries.drop("current_annual_salary", axis=1)
salaries = employee_salaries["current_annual_salary"]
employees.iloc[0]
```


## `TableReport`: interactive display of a dataframe

```{python}
skrub.TableReport(employees, verbose=0)
```

We can tell skrub to patch the default display of polars and pandas dataframes.

```{python}
skrub.patch_display(verbose=0)
```

# 2. Can we learn anything?

```{python}
X, y = employees, salaries
```

## `tabular_learner`: a pre-made robust baseline

```{python}
learner = skrub.tabular_learner("regressor")
learner
```

```{python}
from sklearn.model_selection import cross_val_score

cross_val_score(learner, X, y, scoring="r2")
```

The `tabular_learner` adapts to the supervised estimator we choose

```{python}
from sklearn.linear_model import Ridge

learner = skrub.tabular_learner(Ridge())
learner
```

```{python}
cross_val_score(learner, X, y, scoring="r2")
```

# 3. How do I represent the data?

Skrub helps extract informative features from tabular data.

## `TableVectorizer`: apply an appropriate transformer to each column

```{python}
vectorizer = skrub.TableVectorizer()
transformed = vectorizer.fit_transform(X)
```

The `TableVectorizer` identifies several kinds of columns:

- categorical, low cardinality
- categorical, high cardinality
- datetime
- numeric
- ... we may add more

```{python}
from pprint import pprint

pprint(vectorizer.column_to_kind_)
```

For each kind, it applies an appropriate transformer

```{python}
vectorizer.transformers_["department"]  # low-cardinality categorical
```

```{python}
vectorizer.transformers_["employee_position_title"]  # high-cardinality categorical
```

```{python}
vectorizer.transformers_["date_first_hired"]  # datetime
```

... and those transformers turn the input into numeric features that can be used for ML

```{python}
transformed[vectorizer.input_to_outputs_["date_first_hired"]]
```

For high-cardinality categorical columns the default `GapEncoder` identifies
sparse topics (more later).

```{python}
transformed[vectorizer.input_to_outputs_["employee_position_title"]]
```

The transformer used for each column kind can be easily configured.

## Preprocessing in the `TableVectorizer`

The `TableVectorizer` actually performs a lot of preprocessing before
applying the final transformers, such as:

- ensuring consistent column names
- detecting missing values such as `"N/A"`
- dropping empty columns
- handling pandas dtypes -- `float64`, `nan` vs `Float64`, `NA`
- parsing numbers
- parsing dates, ensuring consistent dtype and timezone
- converting numbers to float32 for faster computation & less memory downstream
- ...

```{python}
pprint(vectorizer.all_processing_steps_["date_first_hired"])
```

## Extracting good features

Skrub offers several encoders to extract features from different columns.
In particular from categorical columns.

### `GapEncoder`

Categories are somewhere between text and an enumeration...
The `GapEncoder` is somewhere between a topic model and a one-hot encoder!

```{python}
#| lines_to_next_cell: 2
import seaborn as sns
from matplotlib import pyplot as plt

gap = skrub.GapEncoder()
pos_title = X["employee_position_title"]
loadings = gap.fit_transform(pos_title).set_index(pos_title.values).head()

loadings.columns = [c.split(": ")[1] for c in loadings.columns]
sns.heatmap(loadings)
_ = plt.setp(plt.gca().get_xticklabels(), rotation=45, ha="right")
```

### `TextEncoder`

Extract embeddings from a text column using any model from the HuggingFace Hub.

```{python}
import pandas as pd

X = pd.Series(["airport", "flight", "plane", "pineapple", "fruit"])
encoder = skrub.TextEncoder(model_name="all-MiniLM-L6-v2", n_components=None)
embeddings = encoder.fit_transform(X).set_index(X.values)

sns.heatmap(embeddings @ embeddings.T)
```

### `MinHashEncoder`

A fast, stateless way of encoding strings that works especially well with
models based on decision trees (gradient boosting, random forest).

# 4. How do I bring it all together?

Skrub has several transformers that allow peforming typical dataframe
operations such as projections, joins and aggregations _inside a scikit-learn pipeline_.

Performing these operations in the machine-learning pipeline has several advantages:

- Choices / hyperparameters can be optimized
- Relevant state can be stored to ensure consistent transformations
- All transformations are packaged together in an estimator

There are several transformers such as `SelectCols`, `Joiner` (fuzzy joining),
`InterpolationJoiner`, `AggJoiner`, ...

A toy example using the `AggJoiner`:

```{python}
from skrub import AggJoiner

airports = pd.DataFrame(
    {
        "airport_id": [1, 2],
        "airport_name": ["Charles de Gaulle", "Aeroporto Leonardo da Vinci"],
        "city": ["Paris", "Roma"],
    }
)
airports
```

```{python}
flights = pd.DataFrame(
    {
        "flight_id": range(1, 7),
        "from_airport": [1, 1, 1, 2, 2, 2],
        "total_passengers": [90, 120, 100, 70, 80, 90],
        "company": ["DL", "AF", "AF", "DL", "DL", "TR"],
    }
)
flights
```

```{python}
agg_joiner = AggJoiner(
    aux_table=flights,
    main_key="airport_id",
    aux_key="from_airport",
    cols=["total_passengers"],
    operations=["mean", "std"],
)
agg_joiner.fit_transform(airports)
```

## More interactive and expressive pipelines

To go further than what can be done with scikit-learn Pipelines and the skrub
transformers shown above, we are developing new utilities to easily define
and inspect flexible pipelines that can process several dataframes.

A prototype will be shown in a separate notebook.

