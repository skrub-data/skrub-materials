[
  {
    "objectID": "pages/notebooks/categorical-encoders/categorical-encoders.html",
    "href": "pages/notebooks/categorical-encoders/categorical-encoders.html",
    "title": "Whatâ€™s the best way to encode categorical features? A use case with Skrub encoders",
    "section": "",
    "text": "Encoding categorical values (such as names, addresses, but also textual data) is a very common problem when it comes to prepare tabular data for training ML models, and Skrub provides four encoders to this end:\n\nskrub.MinhashEncoder, a simple encoder based on hashing categories.\nskrub.GapEncoder, which encoders strings based on latent categories estimated from the data.\nskrub.TextEncoder, a language model-based encoder that uses pre-trained language models to produce vectors for each string.\nskrub.StringEncoder, an encoder that vectorizes data with tf-idf and then applies SVD to reduce the number of features.\n\nThe objective of this post is to test the performance of each encoder on a few datasets in order to find out which methods should be considered in various circumstances.\n\nPreparing the datasets\nWe begin by importing and preparing the datasets that should be used for the experiments:\nfrom skrub.datasets import fetch_toxicity, fetch_movielens, fetch_employee_salaries, fetch_open_payments\n\ndatasets = {}\n## Open Payments (Classification)\ndataset = fetch_open_payments()\nX, y = dataset.X, dataset.y\ny = y.map({\"disallowed\": 0, \"allowed\": 1})\ndatasets[\"Open Payments\"] = (X,y,\"classification\")\n\n## Toxicity (Classification)\ndataset = fetch_toxicity()\nX, y = dataset.X, dataset.y\nX[\"is_toxic\"] = y\ny = X.pop(\"is_toxic\").map({\"Toxic\": 1, \"Not Toxic\": 0})\ndatasets[\"Toxicity\"] = (X,y, \"classification\")\n\n## Movielens (Regression)\ndataset = fetch_movielens()\nX, y = pl.from_pandas(dataset.movies), pl.from_pandas(dataset.ratings)\nX = (\n    X.join(y, on=\"movieId\")\n    .group_by(\"movieId\", \"title\", \"genres\")\n    .agg(target=pl.mean(\"rating\"))\n)\ny = X[\"target\"].to_numpy()\nX = X.drop(\"target\")\ndatasets[\"Movielens\"]=(X,y, \"regression\")\n\n## Employee salaries (Regression)\ndataset = fetch_employee_salaries()\nX = pl.from_pandas(dataset.employee_salaries)\ny = X[\"current_annual_salary\"]\nX = X.drop(\"current_annual_salary\")\n\ndatasets[\"Employee salaries\"]=(X,y, \"regression\")\n\n\nSetting up the experiments\nWe can test each method by building a scikit-learn pipeline for each categorical encoder, using the default HistGradientBoostingClassifier and HistGradientBoostingRegressor as prediction model.\nThem, we use the cross_validate function to track the fit and score time, as well as the prediction performance of each pipeline over different splits. For simplicity, we are not performing hyperparameter optimization for either the categorical encoder or the learner.\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import (\n    HistGradientBoostingClassifier,\n    HistGradientBoostingRegressor,\n)\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import make_pipeline\nimport polars as pl\nfrom skrub import (\n    GapEncoder,\n    MinHashEncoder,\n    StringEncoder,\n    TableVectorizer,\n    TextEncoder,\n)\n\ndef run_experiments(X, y, task, dataset_name):\n    if task == \"regression\":\n        model = HistGradientBoostingRegressor()\n        scoring = \"r2\"\n    else:\n        model = HistGradientBoostingClassifier()\n        scoring = \"roc_auc\"\n\n    results = []\n\n    # For each encoder, create a new pipeline\n    gap_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=GapEncoder(n_components=30)), model\n    )\n    minhash_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=MinHashEncoder(n_components=30)), model\n    )\n    text_encoder = TextEncoder(\n        \"sentence-transformers/paraphrase-albert-small-v2\",\n        device=\"cpu\",\n    )\n    text_encoder_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=text_encoder),\n        model,\n    )\n    string_encoder = StringEncoder(ngram_range=(3, 4), analyzer=\"char_wb\")\n    string_encoder_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=string_encoder),\n        model,\n    )\n\n    pipes = [\n        (\"GapEncoder\", gap_pipe),\n        (\"MinHashEncoder\", minhash_pipe),\n        (\"TextEncoder\", text_encoder_pipe),\n        (\"StringEncoder\", string_encoder_pipe),\n    ]\n\n    for name, p in pipes:\n        cross_validate_results = cross_validate(p, X, y, scoring=scoring)\n        results.append(\n            pl.DataFrame(add_results(name, dataset_name, cross_validate_results))\n        )\n    df_results = pl.concat(results).with_columns(task=pl.lit(task))\n    return df_results\n\n\nRunning the experiments and saving the results\nFinally, I ran the crossvalidation step for each encoder on all dataset and I recorded the files in a csv file. This step took quite some time, and was done offline in a separater script.\nall_results = []\nfor dataset_name,v in datasets.items():\n    X, y, task = v\n    results = run_experiments(X, y, task, dataset_name) \n    all_results.append(results)\ndf_all_results = pl.concat(all_results)\ndf_all_results.write_csv(\"results-encoder_benchmark.csv\")\n\n\nPlotting the results\nNow, we can load the results and start plotting the results. We first split the results in two subtables based on the specific task (either regression or classification), to avoid mixing metrics.\n\nimport polars as pl\nimport matplotlib.pyplot as plt\ndf = pl.read_csv(\"results-encoder_benchmark.csv\")\ndf_regression = df.filter(task=\"regression\")\ndf_classification = df.filter(task=\"classification\")\n\nTo see the tradeoff between fit time and prediction performance, we use a scatterplot with error bars to find the average performance and run time for each method.\nThen, we can plot the prediction performance as a function of the run time.\n\ndef plot_scatter_errorbar(df, ylabel, sharey=False, suptitle=\"\"):\n    # Fixing the colors for each cluster of points and the error bars\n    tab10_colors = plt.get_cmap('tab10').colors\n    colors = dict(zip(df[\"estimator\"].unique().sort().to_list(),tab10_colors[:4]))\n    fig, axs = plt.subplots(1,2, sharey=sharey, layout=\"constrained\", figsize=(8,3))\n    # Each dataset gets a subplot\n    for idx, (dataset, g) in enumerate(df.group_by(\"dataset\")):\n        ax=axs[idx]\n        # Each estimator is plotted separately as a cluster of points\n        for estimator, gdf in g.group_by(\"estimator\"):\n            estim = estimator[0]\n            color = colors[estim]\n            x = gdf[\"fit_time\"].to_numpy()\n            y = gdf[\"test_score\"].to_numpy()\n            label = estim if idx == 0 else \"_\" + estim\n            ax.scatter(x=x, y=y, label=label, color=color)\n            \n            # find the mean and the error bars \n            xerr_mean = gdf[\"fit_time\"].mean()\n            yerr_mean = gdf[\"test_score\"].mean()\n            x_err = gdf[\"fit_time\"].std()\n            y_err = gdf[\"test_score\"].std()\n            # plot the error bars\n            ax.errorbar(xerr_mean, yerr_mean, xerr=x_err, fmt=\"none\", color=color)\n            ax.errorbar(xerr_mean, yerr_mean, yerr=y_err, fmt=\"none\", color=color)\n            \n        ax.set_title(dataset[0])\n        ax.set_xlabel(\"Fit time (s)\")\n        ax.set_ylabel(ylabel)\n        ax.set_xscale(\"log\")\n    fig.suptitle(suptitle)\n    fig.legend(loc=\"lower center\", ncols=2)\n\n\nplot_scatter_errorbar(df_classification, \"ROC-AUC\", sharey=True, suptitle=\"Classification\")\nplot_scatter_errorbar(df_regression, \"R2 score\", sharey=False, suptitle=\"Regression\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prediction performance is fairly consistent across methods, although this depends on the table under observation. MinhashEncoder and StringEncoder are consistently faster than the alternatives.\nTextEncoder is always much slower than the other methods, however it must be noted that this example was run on a CPU, rather than a much faster GPU.\nTo have a better idea of why some methods may outperform others, we should take a look at the actual tables. We can do so very easily thanks to the skrub TableReport object.\n\nfrom skrub import TableReport\nfrom skrub.datasets import fetch_toxicity, fetch_movielens, fetch_employee_salaries, fetch_open_payments\n\n\n# OPEN PAYMENTS\ndataset = fetch_open_payments()\nX, y = dataset.X, dataset.y\nTableReport(X)\n\nProcessing column   5 / 5\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# TOXICITY\ndataset = fetch_toxicity()\nX, y = dataset.X, dataset.y\nTableReport(X)\n\nProcessing column   1 / 1\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# MOVIELENS\ndataset = fetch_movielens()\nX, y = pl.from_pandas(dataset.movies), pl.from_pandas(dataset.ratings)\nX = (\n    X.join(y, on=\"movieId\")\n    .group_by(\"movieId\", \"title\", \"genres\")\n    .agg(target=pl.mean(\"rating\"))\n)\nX = X.drop(\"target\")\n\nTableReport(X)\n\nProcessing column   3 / 3\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# EMPLOYEE SALARIES\ndataset = fetch_employee_salaries()\nX = pl.from_pandas(dataset.employee_salaries)\nX = X.drop(\"current_annual_salary\")\nTableReport(X)\n\nProcessing column   8 / 8\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nAll datasets include high cardinality features which must be encoded using one of the skrub Encoders. The Toxicity dataset is different from the others in that it involves free-flowing text as tweets, while all other tables include a (possibly) large number of unique categories.\nThis explains why the TextEncoder is so much better than the other encoders on Toxicity, while its performance on the other datasets is more in line with the others.\nOn the other hand, the StringEncodershows a strong performance in all cases, while being top-2 on average for the fit time.\n\n\nIn summary\nThe skrub TableVectorizer transforms categorical features into numbers so that ML models can make better use of the information they contain. The StringEncoder can be considered the best all-rounder, being fast to train in most cases, while maintaining strong performance in general. The TextEncoder shines when textual data is available as it can make full use of the pre-trained language models it relies on. The MinHashEncoder and the GapEncoder are more specialized models that may work better than the alternatives in specific circumstances."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html",
    "title": "Skrub",
    "section": "",
    "text": "skrub-data.org\nLess wrangling, more machine learning"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#skrub-helps-at-several-stages-of-a-tabular-learning-project",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#skrub-helps-at-several-stages-of-a-tabular-learning-project",
    "title": "Skrub",
    "section": "Skrub helps at several stages of a tabular learning project",
    "text": "Skrub helps at several stages of a tabular learning project\n\nWhatâ€™s in the data? (EDA)\nCan we learn anything? (baselines)\nHow do I represent the data? (feature extraction)\nHow do I bring it all together? (building a pipeline)"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tablereport-interactive-display-of-a-dataframe",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tablereport-interactive-display-of-a-dataframe",
    "title": "Skrub",
    "section": "TableReport: interactive display of a dataframe",
    "text": "TableReport: interactive display of a dataframe\n\nskrub.TableReport(employees, verbose=0)\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWe can tell skrub to patch the default display of polars and pandas dataframes.\n\nskrub.patch_display(verbose=0)"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tabular_learner-a-pre-made-robust-baseline",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tabular_learner-a-pre-made-robust-baseline",
    "title": "Skrub",
    "section": "tabular_learner: a pre-made robust baseline",
    "text": "tabular_learner: a pre-made robust baseline\n\nlearner = skrub.tabular_learner(\"regressor\")\nlearner\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())]) tablevectorizer: TableVectorizerTableVectorizer(low_cardinality=ToCategorical()) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder() low_cardinalityToCategoricalToCategorical() high_cardinalityStringEncoderStringEncoder() HistGradientBoostingRegressor?Documentation for HistGradientBoostingRegressorHistGradientBoostingRegressor() \n\n\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(learner, X, y, scoring=\"r2\")\n\narray([0.90121408, 0.8776637 , 0.91159274, 0.92363764, 0.92534873])\n\n\nThe tabular_learner adapts to the supervised estimator we choose\n\nfrom sklearn.linear_model import Ridge\n\nlearner = skrub.tabular_learner(Ridge())\nlearner\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())]) tablevectorizer: TableVectorizerTableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline')) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder(periodic_encoding='spline') low_cardinalityOneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False) high_cardinalityStringEncoderStringEncoder() SimpleImputer?Documentation for SimpleImputerSimpleImputer(add_indicator=True) StandardScaler?Documentation for StandardScalerStandardScaler() Ridge?Documentation for RidgeRidge() \n\n\n\ncross_val_score(learner, X, y, scoring=\"r2\")\n\narray([0.77747104, 0.74636601, 0.78608641, 0.77574873, 0.79073481])"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tablevectorizer-apply-an-appropriate-transformer-to-each-column",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tablevectorizer-apply-an-appropriate-transformer-to-each-column",
    "title": "Skrub",
    "section": "TableVectorizer: apply an appropriate transformer to each column",
    "text": "TableVectorizer: apply an appropriate transformer to each column\n\nvectorizer = skrub.TableVectorizer()\ntransformed = vectorizer.fit_transform(X)\n\nThe TableVectorizer identifies several kinds of columns:\n\ncategorical, low cardinality\ncategorical, high cardinality\ndatetime\nnumeric\nâ€¦ we may add more\n\n\nfrom pprint import pprint\n\npprint(vectorizer.column_to_kind_)\n\n{'assignment_category': 'low_cardinality',\n 'date_first_hired': 'datetime',\n 'department': 'low_cardinality',\n 'department_name': 'low_cardinality',\n 'division': 'high_cardinality',\n 'employee_position_title': 'high_cardinality',\n 'gender': 'low_cardinality',\n 'year_first_hired': 'numeric'}\n\n\nFor each kind, it applies an appropriate transformer\n\nvectorizer.transformers_[\"department\"]  # low-cardinality categorical\n\nOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneHotEncoder?Documentation for OneHotEncoderiFittedOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False) \n\n\n\nvectorizer.transformers_[\"employee_position_title\"]  # high-cardinality categorical\n\nStringEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StringEncoderiFittedStringEncoder() \n\n\n\nvectorizer.transformers_[\"date_first_hired\"]  # datetime\n\nDatetimeEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DatetimeEncoderiFittedDatetimeEncoder() \n\n\nâ€¦ and those transformers turn the input into numeric features that can be used for ML\n\ntransformed[vectorizer.input_to_outputs_[\"date_first_hired\"]]\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nFor high-cardinality categorical columns the default GapEncoder identifies sparse topics (more later).\n\ntransformed[vectorizer.input_to_outputs_[\"employee_position_title\"]]\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe transformer used for each column kind can be easily configured."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#preprocessing-in-the-tablevectorizer",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#preprocessing-in-the-tablevectorizer",
    "title": "Skrub",
    "section": "Preprocessing in the TableVectorizer",
    "text": "Preprocessing in the TableVectorizer\nThe TableVectorizer actually performs a lot of preprocessing before applying the final transformers, such as:\n\nensuring consistent column names\ndetecting missing values such as \"N/A\"\ndropping empty columns\nhandling pandas dtypes â€“ float64, nan vs Float64, NA\nparsing numbers\nparsing dates, ensuring consistent dtype and timezone\nconverting numbers to float32 for faster computation & less memory downstream\nâ€¦\n\n\npprint(vectorizer.all_processing_steps_[\"date_first_hired\"])\n\n[CleanNullStrings(),\n DropUninformative(),\n ToDatetime(),\n DatetimeEncoder(),\n {'date_first_hired_day': ToFloat32(), 'date_first_hired_month': ToFloat32(), ...}]"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#extracting-good-features",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#extracting-good-features",
    "title": "Skrub",
    "section": "Extracting good features",
    "text": "Extracting good features\nSkrub offers several encoders to extract features from different columns. In particular from categorical columns.\n\nGapEncoder\nCategories are somewhere between text and an enumerationâ€¦ The GapEncoder is somewhere between a topic model and a one-hot encoder!\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\ngap = skrub.GapEncoder()\npos_title = X[\"employee_position_title\"]\nloadings = gap.fit_transform(pos_title).set_index(pos_title.values).head()\n\nloadings.columns = [c.split(\": \")[1] for c in loadings.columns]\nsns.heatmap(loadings)\n_ = plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\")\n\n\n\n\n\n\n\n\n\n\nTextEncoder\nExtract embeddings from a text column using any model from the HuggingFace Hub.\n\nimport pandas as pd\n\nX = pd.Series([\"airport\", \"flight\", \"plane\", \"pineapple\", \"fruit\"])\nencoder = skrub.TextEncoder(model_name=\"all-MiniLM-L6-v2\", n_components=None)\nembeddings = encoder.fit_transform(X).set_index(X.values)\n\nsns.heatmap(embeddings @ embeddings.T)\n\n\n\n\n\n\n\n\n\n\nMinHashEncoder\nA fast, stateless way of encoding strings that works especially well with models based on decision trees (gradient boosting, random forest)."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#more-interactive-and-expressive-pipelines",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#more-interactive-and-expressive-pipelines",
    "title": "Skrub",
    "section": "More interactive and expressive pipelines",
    "text": "More interactive and expressive pipelines\nTo go further than what can be done with scikit-learn Pipelines and the skrub transformers shown above, we are developing new utilities to easily define and inspect flexible pipelines that can process several dataframes.\nA prototype will be shown in a separate notebook."
  },
  {
    "objectID": "pages/slides/index_slides.html",
    "href": "pages/slides/index_slides.html",
    "title": "Skrub talks",
    "section": "",
    "text": "This page is used to track all the talks about skrub, together with some useful additional material."
  },
  {
    "objectID": "pages/slides/index_slides.html#all-talks",
    "href": "pages/slides/index_slides.html#all-talks",
    "title": "Skrub talks",
    "section": "All talks",
    "text": "All talks"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#fun-facts",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#fun-facts",
    "title": "Probabl Sprint - July 2025",
    "section": "Fun facts",
    "text": "Fun facts\n\nIâ€™m Italian, but I donâ€™t drink coffee, wine, and I like pizza with fries\nI did my PhD in CÃ´te dâ€™Azur, and I moved away because it was too sunny and I donâ€™t like the sea"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#an-example-pipeline",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#an-example-pipeline",
    "title": "Probabl Sprint - July 2025",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPre-process the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data",
    "title": "Probabl Sprint - July 2025",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data-with-skrub",
    "title": "Probabl Sprint - July 2025",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#data-cleaning-with-pandas",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#data-cleaning-with-pandas",
    "title": "Probabl Sprint - July 2025",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Constant int': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'Constant str': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n    'All empty': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\ndf = pd.DataFrame(data)\ndisplay(df)\n\n\n\n\n\n\n\n\nConstant int\nB\nConstant str\nD\nAll nan\nAll empty\nDate\n\n\n\n\n0\n1\n2\nx\n4\nNaN\n\n01/01/2023\n\n\n1\n1\n3\nx\n5\nNaN\n\n02/01/2023\n\n\n2\n1\n2\nx\n6\nNaN\n\n03/01/2023"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#data-cleaning-with-pandas-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#data-cleaning-with-pandas-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\n# Parse the datetime strings with a specific format\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n\n# Drop columns with only a single unique value\ndf_cleaned = df.loc[:, df.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_cleaned = drop_empty_columns(df_cleaned)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#lightweight-data-cleaning-cleaner",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#lightweight-data-cleaning-cleaner",
    "title": "Probabl Sprint - July 2025",
    "section": "Lightweight data cleaning: Cleaner",
    "text": "Lightweight data cleaning: Cleaner\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-with-pandas",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-with-pandas",
    "title": "Probabl Sprint - July 2025",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf = pd.DataFrame(data)\ndf_expanded = df.copy()\ndatetime_column = \"date\"\ndf_expanded[datetime_column] = pd.to_datetime(df_expanded[datetime_column], errors='coerce')\n\ndf_expanded['year'] = df_expanded[datetime_column].dt.year\ndf_expanded['month'] = df_expanded[datetime_column].dt.month\ndf_expanded['day'] = df_expanded[datetime_column].dt.day\ndf_expanded['hour'] = df_expanded[datetime_column].dt.hour\ndf_expanded['minute'] = df_expanded[datetime_column].dt.minute\ndf_expanded['second'] = df_expanded[datetime_column].dt.second"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-with-pandas-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-with-pandas-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\ndf_expanded['hour_sin'] = np.sin(2 * np.pi * df_expanded['hour'] / 24)\ndf_expanded['hour_cos'] = np.cos(2 * np.pi * df_expanded['hour'] / 24)\n\ndf_expanded['month_sin'] = np.sin(2 * np.pi * df_expanded['month'] / 12)\ndf_expanded['month_cos'] = np.cos(2 * np.pi * df_expanded['month'] / 12)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nDataFrame with expanded datetime columns:\")\nprint(df_expanded)\n\nOriginal DataFrame:\n                  date  value\n0  2023-01-01 12:34:56     10\n1  2023-02-15 08:45:23     20\n2  2023-03-20 18:12:45     30\n\nDataFrame with expanded datetime columns:\n                 date  value  year  month  day  hour  minute  second  \\\n0 2023-01-01 12:34:56     10  2023      1    1    12      34      56   \n1 2023-02-15 08:45:23     20  2023      2   15     8      45      23   \n2 2023-03-20 18:12:45     30  2023      3   20    18      12      45   \n\n       hour_sin      hour_cos  month_sin     month_cos  \n0  1.224647e-16 -1.000000e+00   0.500000  8.660254e-01  \n1  8.660254e-01 -5.000000e-01   0.866025  5.000000e-01  \n2 -1.000000e+00 -1.836970e-16   1.000000  6.123234e-17"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "title": "Probabl Sprint - July 2025",
    "section": "Encoding datetime features skrub.DatetimeEncoder",
    "text": "Encoding datetime features skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\nX_date = ToDatetime().fit_transform(df[\"date\"])\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\n   date_year  date_total_seconds  date_month_circular_0  \\\n0     2023.0        1.672577e+09               0.500000   \n1     2023.0        1.676451e+09               0.866025   \n2     2023.0        1.679336e+09               1.000000   \n\n   date_month_circular_1  date_day_circular_0  date_day_circular_1  \\\n0           8.660254e-01         2.079117e-01             0.978148   \n1           5.000000e-01         1.224647e-16            -1.000000   \n2           6.123234e-17        -8.660254e-01            -0.500000   \n\n   date_hour_circular_0  date_hour_circular_1  \n0          1.224647e-16         -1.000000e+00  \n1          8.660254e-01         -5.000000e-01  \n2         -1.000000e+00         -1.836970e-16"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#what-periodic-features-look-like",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#what-periodic-features-look-like",
    "title": "Probabl Sprint - July 2025",
    "section": "What periodic features look like",
    "text": "What periodic features look like"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "Probabl Sprint - July 2025",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline",
    "title": "Probabl Sprint - July 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-2",
    "title": "Probabl Sprint - July 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "title": "Probabl Sprint - July 2025",
    "section": "Build a predictive pipeline with tabular_learner",
    "text": "Build a predictive pipeline with tabular_learner\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_learner(Ridge())\nmodel\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('tablevectorizer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_thresholdÂ \n40\n\n\n\nlow_cardinalityÂ \nOneHotEncoder..._output=False)\n\n\n\nhigh_cardinalityÂ \nStringEncoder()\n\n\n\nnumericÂ \nPassThrough()\n\n\n\ndatetimeÂ \nDatetimeEncod...ding='spline')\n\n\n\nspecific_transformersÂ \n()\n\n\n\ndrop_null_fractionÂ \n1.0\n\n\n\ndrop_if_constantÂ \nFalse\n\n\n\ndrop_if_uniqueÂ \nFalse\n\n\n\ndatetime_formatÂ \nNone\n\n\n\nn_jobsÂ \nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolutionÂ \n'hour'\n\n\n\nadd_weekdayÂ \nFalse\n\n\n\nadd_total_secondsÂ \nTrue\n\n\n\nadd_day_of_yearÂ \nFalse\n\n\n\nperiodic_encodingÂ \n'spline'\n\n\n\n\n            \n        \n    low_cardinalityOneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategoriesÂ \n'auto'\n\n\n\ndropÂ \n'if_binary'\n\n\n\nsparse_outputÂ \nFalse\n\n\n\ndtypeÂ \n'float32'\n\n\n\nhandle_unknownÂ \n'ignore'\n\n\n\nmin_frequencyÂ \nNone\n\n\n\nmax_categoriesÂ \nNone\n\n\n\nfeature_name_combinerÂ \n'concat'\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_componentsÂ \n30\n\n\n\nvectorizerÂ \n'tfidf'\n\n\n\nngram_rangeÂ \n(3, ...)\n\n\n\nanalyzerÂ \n'char_wb'\n\n\n\nstop_wordsÂ \nNone\n\n\n\nrandom_stateÂ \nNone\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_valuesÂ \nnan\n\n\n\nstrategyÂ \n'mean'\n\n\n\nfill_valueÂ \nNone\n\n\n\ncopyÂ \nTrue\n\n\n\nadd_indicatorÂ \nTrue\n\n\n\nkeep_empty_featuresÂ \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopyÂ \nTrue\n\n\n\nwith_meanÂ \nTrue\n\n\n\nwith_stdÂ \nTrue\n\n\n\n\n            \n        \n    Ridge?Documentation for Ridge\n        \n            \n                Parameters\n                \n\n\n\n\nalphaÂ \n1.0\n\n\n\nfit_interceptÂ \nTrue\n\n\n\ncopy_XÂ \nTrue\n\n\n\nmax_iterÂ \nNone\n\n\n\ntolÂ \n0.0001\n\n\n\nsolverÂ \n'auto'\n\n\n\npositiveÂ \nFalse\n\n\n\nrandom_stateÂ \nNone"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#we-now-have-a-pipeline",
    "title": "Probabl Sprint - July 2025",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\n\nskrub.datasets, or user data\n\nExplore the data\n\nskrub.TableReport\n\nPre-process the data\n\nskrub.TableVectorizer, Cleaner, DatetimeEncoder â€¦\n\nPerform feature engineering\n\nskrub.TableVectorizer, TextEncoder, StringEncoderâ€¦\n\nBuild a scikit-learn pipeline\n\ntabular_learner, sklearn.pipeline.make_pipeline â€¦\n\n???\nProfit ðŸ“ˆ"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#a-realistic-scenario",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#a-realistic-scenario",
    "title": "Probabl Sprint - July 2025",
    "section": "A realistic scenario",
    "text": "A realistic scenario\nA data scientist needs to train a ML model, but features are spread across multiple tables.\n\n\n\n\n\n\n\nWarning\n\n\nMany issues with this!\n\n\n\n\n\nscikit-learn pipelines support only a single feature matrix X\nDataframe operations cannot be tuned\nData leakage must be accounted for\nPersisting and reproducing operations is complex"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#skrub-dataops",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#skrub-dataops",
    "title": "Probabl Sprint - July 2025",
    "section": "skrub DataOps",
    "text": "skrub DataOps\nWhen a normal pipe is not enoughâ€¦\n\nâ€¦ the skrub DataOps come to the rescue ðŸš’"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#dataops",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#dataops",
    "title": "Probabl Sprint - July 2025",
    "section": "DataOpsâ€¦",
    "text": "DataOpsâ€¦\n\nExtend the scikit-learn machinery to complex multi-table operations\nTrack all operations with a computational graph (a data plan)\nAllow tuning any operation in the data plan\nCan be persisted and shared easily by generating a learner"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#dataops-data-plans-learners-oh-my",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#dataops-data-plans-learners-oh-my",
    "title": "Probabl Sprint - July 2025",
    "section": "DataOps, Data Plans, learners: oh my!",
    "text": "DataOps, Data Plans, learners: oh my!\n\nA DataOp (singular) wraps a single operation, and can be combined and concatenated with other DataOps.\nThe Data Plan is a collective name for a sequence and combination of DataOps.\nThe Data Plan can be exported as a standalone object called learner. The learner takes a dictionary of values rather than just X and y."
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#how-do-dataops-work-though",
    "title": "Probabl Sprint - July 2025",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#starting-with-the-dataops",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#starting-with-the-dataops",
    "title": "Probabl Sprint - July 2025",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\n\nX, y, products represent inputs to the pipeline.\nskrub splits X and y when training."
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#building-a-full-data-plan",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#building-a-full-data-plan",
    "title": "Probabl Sprint - July 2025",
    "section": "Building a full data plan",
    "text": "Building a full data plan\nfrom skrub import selectors as s\nfrom sklearn.ensemble import ExtraTreesClassifier  \n\nvectorizer = skrub.TableVectorizer(high_cardinality=skrub.StringEncoder(), n_jobs=-1)\nvectorized_products = products.skb.apply(vectorizer, cols=s.all() - \"basket_ID\")\naggregated_products = vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\nfeatures = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\")\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])\npredictions = features.skb.apply(ExtraTreesClassifier(n_jobs=-1), y=y)"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#inspecting-the-data-plan",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#inspecting-the-data-plan",
    "title": "Probabl Sprint - July 2025",
    "section": "Inspecting the data plan",
    "text": "Inspecting the data plan\npredictions.skb.full_report()\n\nreport"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#exporting-the-plan-in-a-learner",
    "title": "Probabl Sprint - July 2025",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe data plan can be exported as a learner:\n# anywhere\nlearner = predictions.skb.make_learner()\n# search is a HPO object\nbest_learner = search.skb.best_learner_\n\nThen, the learner can be pickled â€¦\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\nâ€¦ and loaded\nwith open(\"learner.bin\", \"rb\") as fp:\n    learner = pickle.load(fp)\n\nlearner.predict({\"baskets\": new_baskets, \"products\": new_products})"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#hyperparameter-tuning-in-a-data-plan",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#hyperparameter-tuning-in-a-data-plan",
    "title": "Probabl Sprint - July 2025",
    "section": "Hyperparameter tuning in a Data Plan",
    "text": "Hyperparameter tuning in a Data Plan\nskrub implements four choose_* functions:\n\nchoose_from: select from the given list of options\nchoose_int: select an integer within a range\nchoose_float: select a float within a range\nchoose_bool: select a bool\noptional: chooses between a value or DataOp and no op"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#hyperparameter-tuning-in-a-data-plan-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#hyperparameter-tuning-in-a-data-plan-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Hyperparameter tuning in a Data Plan",
    "text": "Hyperparameter tuning in a Data Plan\nItâ€™s possible to nest these functions to create complex grids:\nX.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "Probabl Sprint - July 2025",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#tuning-with-dataops-is-simple",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#tuning-with-dataops-is-simple",
    "title": "Probabl Sprint - July 2025",
    "section": "Tuning with DataOps is simple!",
    "text": "Tuning with DataOps is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)\nregressor.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#observe-the-impact-of-the-hyperparameters",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#observe-the-impact-of-the-hyperparameters",
    "title": "Probabl Sprint - July 2025",
    "section": "Observe the impact of the hyperparameters",
    "text": "Observe the impact of the hyperparameters\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#tldw",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#tldw",
    "title": "Probabl Sprint - July 2025",
    "section": "tl;dw",
    "text": "tl;dw\nskrub provides\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering\nsoonâ„¢ï¸, DataOps, data plans, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#getting-involved",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#getting-involved",
    "title": "Probabl Sprint - July 2025",
    "section": "Getting involved",
    "text": "Getting involved\n\nSkrub website (QR code below!)\nSkrub materials website\nGit repository\nDiscord server\nBluesky"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#whoami",
    "href": "pages/slides/pydata-2025/slides.html#whoami",
    "title": "PyData Paris 2025",
    "section": "whoami",
    "text": "whoami\n\nI am a research engineer at Inria as part of the P16 project, and I am the lead developer of skrub \nIâ€™m Italian, but I donâ€™t drink coffee, wine, and I like pizza with fries \nI did my PhD in CÃ´te dâ€™Azur, and I moved away because it was too sunny and I donâ€™t like the sea"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#a-teaser-for-later",
    "href": "pages/slides/pydata-2025/slides.html#a-teaser-for-later",
    "title": "PyData Paris 2025",
    "section": "A teaser for laterâ€¦",
    "text": "A teaser for laterâ€¦\nInspect all the steps of your pipeline: Execution report"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#a-teaser-for-later-1",
    "href": "pages/slides/pydata-2025/slides.html#a-teaser-for-later-1",
    "title": "PyData Paris 2025",
    "section": "A teaser for laterâ€¦",
    "text": "A teaser for laterâ€¦\nExplore your hyperparameter search space"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#skrub-compatibility",
    "href": "pages/slides/pydata-2025/slides.html#skrub-compatibility",
    "title": "PyData Paris 2025",
    "section": "skrub compatibility",
    "text": "skrub compatibility\n\nSkrub is fully compatible with pandas and polars\nSkrub transformers are fully compatible with scikit-learn"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#an-example-pipeline",
    "href": "pages/slides/pydata-2025/slides.html#an-example-pipeline",
    "title": "PyData Paris 2025",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPreprocess the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#exploring-the-data",
    "href": "pages/slides/pydata-2025/slides.html#exploring-the-data",
    "title": "PyData Paris 2025",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#exploring-the-data-1",
    "href": "pages/slides/pydata-2025/slides.html#exploring-the-data-1",
    "title": "PyData Paris 2025",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/pydata-2025/slides.html#exploring-the-data-with-skrub",
    "title": "PyData Paris 2025",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nTableReport Preview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\n\nMore examples"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#data-cleaning-with-pandaspolars-setup",
    "href": "pages/slides/pydata-2025/slides.html#data-cleaning-with-pandaspolars-setup",
    "title": "PyData Paris 2025",
    "section": "Data cleaning with pandas/polars: setup",
    "text": "Data cleaning with pandas/polars: setup\n\nPandasPolars\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pd = pd.DataFrame(data)\ndisplay(df_pd)\n\n\n\n\n\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\n\n\n0\n2\nx\nfoo\nNaN\n\n01 Jan 2023\n\n\n1\n3\nx\nbar\nNaN\n\n02 Jan 2023\n\n\n2\n2\nx\nbaz\nNaN\n\n03 Jan 2023\n\n\n\n\n\n\n\n\n\n\nimport polars as pl\nimport numpy as np\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pl = pl.DataFrame(data)\ndisplay(df_pl)\n\n\nshape: (3, 6)\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\ni64\nstr\nstr\nf64\nstr\nstr\n\n\n\n\n2\n\"x\"\n\"foo\"\nNaN\n\"\"\n\"01 Jan 2023\"\n\n\n3\n\"x\"\n\"bar\"\nNaN\n\"\"\n\"02 Jan 2023\"\n\n\n2\n\"x\"\n\"baz\"\nNaN\n\"\"\n\"03 Jan 2023\""
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "href": "pages/slides/pydata-2025/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "title": "PyData Paris 2025",
    "section": "Nulls, datetimes, constant columns with pandas/polars",
    "text": "Nulls, datetimes, constant columns with pandas/polars\n\nPandasPolars\n\n\n\n# Parse the datetime strings with a specific format\ndf_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d %b %Y')\n\n# Drop columns with only a single unique value\ndf_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_pd_cleaned = drop_empty_columns(df_pd_cleaned)\n\n\n\n\n# Parse the datetime strings with a specific format\ndf_pl = df_pl.with_columns([\n    pl.col(\"Date\").str.strptime(pl.Date, \"%d %b %Y\", strict=False).alias(\"Date\")\n])\n\n# Drop columns with only a single unique value\ndf_pl_cleaned = df_pl.select([\n    col for col in df_pl.columns if df_pl[col].n_unique() &gt; 1\n])\n\n# Import selectors for dtype selection\nimport polars.selectors as cs\n\n# Drop columns with only missing values or only empty strings\ndef drop_empty_columns(df):\n    all_nan = df.select(\n        [\n            col for col in df.select(cs.numeric()).columns if \n            df [col].is_nan().all()\n        ]\n    ).columns\n    \n    all_empty = df.select(\n        [\n            col for col in df.select(cs.string()).columns if \n            (df[col].str.strip_chars().str.len_chars()==0).all()\n        ]\n    ).columns\n\n    to_drop = all_nan + all_empty\n\n    return df.drop(to_drop)\n\ndf_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#data-cleaning-with-skrub.cleaner",
    "href": "pages/slides/pydata-2025/slides.html#data-cleaning-with-skrub.cleaner",
    "title": "PyData Paris 2025",
    "section": "Data cleaning with skrub.Cleaner",
    "text": "Data cleaning with skrub.Cleaner\n\nPandasPolars\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pd)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nInt\nStr\nDate\n\n\n\n\n0\n2\nfoo\n2023-01-01\n\n\n1\n3\nbar\n2023-01-02\n\n\n2\n2\nbaz\n2023-01-03\n\n\n\n\n\n\n\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pl)\ndisplay(df_cleaned)\n\n\nshape: (3, 3)\n\n\n\nInt\nStr\nDate\n\n\ni64\nstr\ndate\n\n\n\n\n2\n\"foo\"\n2023-01-01\n\n\n3\n\"bar\"\n2023-01-02\n\n\n2\n\"baz\"\n2023-01-03"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-datetime-features-with-pandaspolars",
    "href": "pages/slides/pydata-2025/slides.html#encoding-datetime-features-with-pandaspolars",
    "title": "PyData Paris 2025",
    "section": "Encoding datetime features with pandas/polars",
    "text": "Encoding datetime features with pandas/polars\n\nPandasPolars\n\n\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pd = pd.DataFrame(data)\ndatetime_column = \"date\"\ndf_pd[datetime_column] = pd.to_datetime(df_pd[datetime_column], errors='coerce')\n\ndf_pd['year'] = df_pd[datetime_column].dt.year\ndf_pd['month'] = df_pd[datetime_column].dt.month\ndf_pd['day'] = df_pd[datetime_column].dt.day\ndf_pd['hour'] = df_pd[datetime_column].dt.hour\ndf_pd['minute'] = df_pd[datetime_column].dt.minute\ndf_pd['second'] = df_pd[datetime_column].dt.second\n\n\n\n\nimport polars as pl\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pl = pl.DataFrame(data)\ndf_pl = df_pl.with_columns(date=pl.col(\"date\").str.to_datetime())\n\ndf_pl = df_pl.with_columns(\n    year=pl.col(\"date\").dt.year(),\n    month=pl.col(\"date\").dt.month(),\n    day=pl.col(\"date\").dt.day(),\n    hour=pl.col(\"date\").dt.hour(),\n    minute=pl.col(\"date\").dt.minute(),\n    second=pl.col(\"date\").dt.second(),\n)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-datetime-features-with-skrub.datetimeencoder",
    "href": "pages/slides/pydata-2025/slides.html#encoding-datetime-features-with-skrub.datetimeencoder",
    "title": "PyData Paris 2025",
    "section": "Encoding datetime features with skrub.DatetimeEncoder",
    "text": "Encoding datetime features with skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nX_date = ToDatetime().fit_transform(df[\"date\"])\nde = DatetimeEncoder(resolution=\"second\")\n# de = DatetimeEncoder(periodic_encoding=\"spline\")\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\nshape: (3, 7)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ date_year â”† date_month â”† date_day â”† date_hour â”† date_minute â”† date_second â”† date_total_seconds â”‚\nâ”‚ ---       â”† ---        â”† ---      â”† ---       â”† ---         â”† ---         â”† ---                â”‚\nâ”‚ f32       â”† f32        â”† f32      â”† f32       â”† f32         â”† f32         â”† f32                â”‚\nâ•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 2023.0    â”† 1.0        â”† 1.0      â”† 12.0      â”† 34.0        â”† 56.0        â”† 1.6726e9           â”‚\nâ”‚ 2023.0    â”† 2.0        â”† 15.0     â”† 8.0       â”† 45.0        â”† 23.0        â”† 1.6765e9           â”‚\nâ”‚ 2023.0    â”† 3.0        â”† 20.0     â”† 18.0      â”† 12.0        â”† 45.0        â”† 1.6793e9           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#what-periodic-features-look-like",
    "href": "pages/slides/pydata-2025/slides.html#what-periodic-features-look-like",
    "title": "PyData Paris 2025",
    "section": "What periodic features look like",
    "text": "What periodic features look like"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "href": "pages/slides/pydata-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "title": "PyData Paris 2025",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler\n\n\n\n\n\n\n\n\n\n\nSkrub wants to solve ML problems based partly on solid engineering and partly on statistical notions. The SquashingScaler is based on the second part, and is taken from a recent paper that evaluates different techniques for improving the performance of NNs."
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "href": "pages/slides/pydata-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "title": "PyData Paris 2025",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-categorical-stringtext-features",
    "href": "pages/slides/pydata-2025/slides.html#encoding-categorical-stringtext-features",
    "title": "PyData Paris 2025",
    "section": "Encoding categorical (string/text) features",
    "text": "Encoding categorical (string/text) features\nCategorical features have a â€œcardinalityâ€: the number of unique values\n\nLow cardinality: OneHotEncoder\nHigh cardinality (&gt;40 unique values): skrub.StringEncoder\nText: skrub.TextEncoder and pretrained models from HuggingFace Hub"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/pydata-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "PyData Paris 2025",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer\nfrom skrub import TableVectorizer, TextEncoder\n\ntable_vec = TableVectorizer()\ndf_encoded = table_vec.fit_transform(df)\n\n\nApply the Cleaner to all columns\nSplit columns by dtype and # of unique values\nEncode each column separately"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-all-the-features-tablevectorizer-1",
    "href": "pages/slides/pydata-2025/slides.html#encoding-all-the-features-tablevectorizer-1",
    "title": "PyData Paris 2025",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline",
    "title": "PyData Paris 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline-1",
    "title": "PyData Paris 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "href": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "title": "PyData Paris 2025",
    "section": "Build a predictive pipeline with tabular_pipeline",
    "text": "Build a predictive pipeline with tabular_pipeline\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_pipeline(Ridge())"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/pydata-2025/slides.html#we-now-have-a-pipeline",
    "title": "PyData Paris 2025",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\nExplore the data\n\nTableReport\n\nPre-process the data\n\nCleaner, ToDatetime â€¦\n\nPerform feature engineering\n\nTableVectorizer,SquashingScaler, TextEncoder, StringEncoderâ€¦\n\nBuild a scikit-learn pipeline\n\ntabular_pipeline â€¦\n\n???\nProfit ðŸ“ˆ"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#what-if",
    "href": "pages/slides/pydata-2025/slides.html#what-if",
    "title": "PyData Paris 2025",
    "section": "What ifâ€¦",
    "text": "What ifâ€¦\n\nYour data is spread over multiple tables?\nYou want to avoid data leakage?\nYou want to tune more than just the hyperparameters of your model?\nYou want to guarantee that your pipeline is replayed exactly on new data?"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#section-2",
    "href": "pages/slides/pydata-2025/slides.html#section-2",
    "title": "PyData Paris 2025",
    "section": "",
    "text": "When a normal pipe is not enoughâ€¦\n\nâ€¦ the skrub DataOps come to the rescue ðŸš’"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#dataops",
    "href": "pages/slides/pydata-2025/slides.html#dataops",
    "title": "PyData Paris 2025",
    "section": "DataOpsâ€¦",
    "text": "DataOpsâ€¦\n\nExtend the scikit-learn machinery to complex multi-table operations, and take care of data leakage\nTrack all operations with a computational graph (a Data Ops plan)\nAre transparent and give direct access to the underlying object\nAllow tuning any operation in the Data Ops plan\nGuarantee that all operations are reproducible\nCan be persisted and shared easily"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/pydata-2025/slides.html#how-do-dataops-work-though",
    "title": "PyData Paris 2025",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#starting-with-the-dataops",
    "href": "pages/slides/pydata-2025/slides.html#starting-with-the-dataops",
    "title": "PyData Paris 2025",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\nimport skrub\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\n\nbaskets and products represent inputs to the pipeline.\nSkrub tracks X and y so that training and test splits are never mixed."
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#applying-a-transformer",
    "href": "pages/slides/pydata-2025/slides.html#applying-a-transformer",
    "title": "PyData Paris 2025",
    "section": "Applying a transformer",
    "text": "Applying a transformer\n\nfrom skrub import selectors as s\n\nvectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder()\n)\nvectorized_products = products.skb.apply(\n    vectorizer, cols=s.all() - \"basket_ID\"\n)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#executing-dataframe-operations",
    "href": "pages/slides/pydata-2025/slides.html#executing-dataframe-operations",
    "title": "PyData Paris 2025",
    "section": "Executing dataframe operations",
    "text": "Executing dataframe operations\n\naggregated_products = vectorized_products.groupby(\n    \"basket_ID\"\n).agg(\"mean\").reset_index()\n\nfeatures = X.merge(\n    aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n)\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#applying-a-ml-model",
    "href": "pages/slides/pydata-2025/slides.html#applying-a-ml-model",
    "title": "PyData Paris 2025",
    "section": "Applying a ML model",
    "text": "Applying a ML model\n\nfrom sklearn.ensemble import ExtraTreesClassifier  \npredictions = features.skb.apply(\n    ExtraTreesClassifier(n_jobs=-1), y=y\n)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#inspecting-the-data-ops-plan",
    "href": "pages/slides/pydata-2025/slides.html#inspecting-the-data-ops-plan",
    "title": "PyData Paris 2025",
    "section": "Inspecting the Data Ops plan",
    "text": "Inspecting the Data Ops plan\npredictions.skb.full_report()\n\nExecution report\nEach node:\n\nShows a preview of the data resulting from the operation\nReports the location in the code where the code is defined\nShows the run time of the node\n\nThe plan is exported as HTML + JS (no need for kernels)."
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/pydata-2025/slides.html#exporting-the-plan-in-a-learner",
    "title": "PyData Paris 2025",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe Learner is a stand-alone object that works like a scikit-learn estimator that takes a dictionary as input rather than just X and y.\n\n\nlearner = predictions.skb.make_learner(fitted=True)\n\n\n\nThen, the learner can be pickled â€¦\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\nâ€¦ loaded and applied to new data:\nwith open(\"learner.bin\", \"rb\") as fp:\n    loaded_learner = pickle.load(fp)\ndata = skrub.datasets.fetch_credit_fraud(split=\"test\")\nnew_baskets = data.baskets\nnew_products = data.products\nloaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})\n\n\narray([0, 0, 0, ..., 0, 0, 0], shape=(31549,))"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#hyperparameter-tuning-in-a-data-plan",
    "href": "pages/slides/pydata-2025/slides.html#hyperparameter-tuning-in-a-data-plan",
    "title": "PyData Paris 2025",
    "section": "Hyperparameter tuning in a Data Plan",
    "text": "Hyperparameter tuning in a Data Plan\nSkrub implements four choose_* functions:\n\nchoose_from: select from the given list of options\nchoose_int: select an integer within a range\nchoose_float: select a float within a range\nchoose_bool: select a bool\noptional: chooses whether to execute the given operation"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/pydata-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "PyData Paris 2025",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestRegressor()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestRegressor()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#tuning-with-dataops-is-simple",
    "href": "pages/slides/pydata-2025/slides.html#tuning-with-dataops-is-simple",
    "title": "PyData Paris 2025",
    "section": "Tuning with DataOps is simple!",
    "text": "Tuning with DataOps is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestRegressor(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#run-hyperparameter-search",
    "href": "pages/slides/pydata-2025/slides.html#run-hyperparameter-search",
    "title": "PyData Paris 2025",
    "section": "Run hyperparameter search",
    "text": "Run hyperparameter search\n# fit the search \nsearch = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True, cv=5)\n\n# save the best learner\nbest_learner = search.best_learner_"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#tuning-with-dataops-is-not-limited-to-estimators",
    "href": "pages/slides/pydata-2025/slides.html#tuning-with-dataops-is-not-limited-to-estimators",
    "title": "PyData Paris 2025",
    "section": "Tuning with DataOps is not limited to estimators",
    "text": "Tuning with DataOps is not limited to estimators\n\nPandasPolars\n\n\n\ndf = pd.DataFrame(\n    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n)\n\ndf_do = skrub.var(\"grades\", df)\n\nagg_grades = df_do.groupby(\"subject\").agg(skrub.choose_from([\"count\", \"mean\"]))\nagg_grades.skb.describe_param_grid()\n\n\"- choose_from(['count', 'mean']): ['count', 'mean']\\n\"\n\n\n\n\n\ndf = pl.DataFrame(\n    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n)\n\ndf_do = skrub.var(\"grades\", df)\n\nagg_grades = df_do.group_by(\"subject\").agg(\n    skrub.choose_from([pl.mean(\"grade\"), pl.count(\"grade\")])\n)\nagg_grades.skb.describe_param_grid()\n\n'- choose_from([&lt;Expr [\\'col(\"grade\").mean()\\'] at 0x15B34D510&gt;, &lt;Expr [\\'col(\"grade\").count()\\'] at 0x15B34FE20&gt;]): [&lt;Expr [\\'col(\"grade\").mean()\\'] at 0x15B34D510&gt;, &lt;Expr [\\'col(\"grade\").count()\\'] at 0x15B34FE20&gt;]\\n'"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#a-parallel-coordinate-plot-to-explore-hyperparameters",
    "href": "pages/slides/pydata-2025/slides.html#a-parallel-coordinate-plot-to-explore-hyperparameters",
    "title": "PyData Paris 2025",
    "section": "A parallel coordinate plot to explore hyperparameters",
    "text": "A parallel coordinate plot to explore hyperparameters\nsearch = pred.skb.get_randomized_search(fitted=True)\nsearch.plot_parallel_coord()\n\n\n                            \n                                            \n\n\nsource"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#more-information-about-the-data-ops",
    "href": "pages/slides/pydata-2025/slides.html#more-information-about-the-data-ops",
    "title": "PyData Paris 2025",
    "section": "More information about the Data Ops",
    "text": "More information about the Data Ops\n\nSkrub example gallery\nSkrub user guide\nTutorial on timeseries forecasting at Euroscipy 2025\nKaggle notebook on the Titanic survival challenge"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#getting-involved",
    "href": "pages/slides/pydata-2025/slides.html#getting-involved",
    "title": "PyData Paris 2025",
    "section": "Getting involved",
    "text": "Getting involved\nDo you want to learn more?\n\nSkrub website\nSkrub materials website\nDiscord server\n\nFollow skrub on:\n\nBluesky\nLinkedIn\n\nStar skrub on GitHub, or contribute directly:\n\nGitHub repository"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#sprint-on-thursday",
    "href": "pages/slides/pydata-2025/slides.html#sprint-on-thursday",
    "title": "PyData Paris 2025",
    "section": "Sprint on Thursday!!!",
    "text": "Sprint on Thursday!!!"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#were-hiring",
    "href": "pages/slides/pydata-2025/slides.html#were-hiring",
    "title": "PyData Paris 2025",
    "section": "Weâ€™re hiring!!",
    "text": "Weâ€™re hiring!!\nCome talk to me or go to the P16 booth"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#tldw-skrub",
    "href": "pages/slides/pydata-2025/slides.html#tldw-skrub",
    "title": "PyData Paris 2025",
    "section": "tl;dw: skrub",
    "text": "tl;dw: skrub\n\ninteractive data exploration: TableReport\nautomated pre-processing of pandas and polars dataframes: Cleaner\npowerful feature engineering: TableVectorizer, tabular_pipeline\ncolumn- and dataframe-level operations: ApplyToCols, selectors\nDataOps, plans, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#fine-grained-column-transformations-with-applytocols",
    "href": "pages/slides/pydata-2025/slides.html#fine-grained-column-transformations-with-applytocols",
    "title": "PyData Paris 2025",
    "section": "Fine-grained column transformations with ApplyToCols",
    "text": "Fine-grained column transformations with ApplyToCols\n\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#fine-grained-column-transformations-with-applytocols-1",
    "href": "pages/slides/pydata-2025/slides.html#fine-grained-column-transformations-with-applytocols-1",
    "title": "PyData Paris 2025",
    "section": "Fine-grained column transformations with ApplyToCols",
    "text": "Fine-grained column transformations with ApplyToCols\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ntext_bar\ntext_baz\ntext_foo\nnumber\n\n\n\n\n0\n0.0\n0.0\n1.0\n-1.224745\n\n\n1\n1.0\n0.0\n0.0\n0.000000\n\n\n2\n0.0\n1.0\n0.0\n1.224745"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#building-complex-pipelines",
    "href": "pages/slides/expressions/expressions.html#building-complex-pipelines",
    "title": "Skrub learning materials",
    "section": "Building complex pipelines",
    "text": "Building complex pipelines\n\nOur learner contains several data-processing steps\n\njoining tables\nselecting columns\napplying machine-learning estimators\n\nSome steps have state that needs to be fitted\nOften several tables and aggregations are involved"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#example",
    "href": "pages/slides/expressions/expressions.html#example",
    "title": "Skrub learning materials",
    "section": "Example",
    "text": "Example\n\nWe have e-commerce check-out baskets\nEach containing one or more products\nPredict if the transaction is fraudulent\n\n\nDataset"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#a-first-attempt",
    "href": "pages/slides/expressions/expressions.html#a-first-attempt",
    "title": "Skrub learning materials",
    "section": "A first attempt â€¦",
    "text": "A first attempt â€¦\n\nScikit-learn assumes a single table X of the right shape"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#loading-data",
    "href": "pages/slides/expressions/expressions.html#loading-data",
    "title": "Skrub learning materials",
    "section": "Loading data",
    "text": "Loading data\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = data.baskets[[\"ID\"]]\ny = data.baskets[\"fraud_flag\"]\nproducts = data.products"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\n\nvectorized_products = product_vectorizer.fit_transform(data.products)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-1",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-1",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\n\nvectorized_products = product_vectorizer.fit_transform(data.products)\nðŸ¤”\n\nHow to store product_vectorizer?\nFitted on whole products table: data leakage\nCannot tune hyper-parameters\nTransforming only some columns is hard\n\nColumnTransformer ðŸ˜ŸðŸ˜°"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#joining-the-product-features",
    "href": "pages/slides/expressions/expressions.html#joining-the-product-features",
    "title": "Skrub learning materials",
    "section": "Joining the product features",
    "text": "Joining the product features\naggregated_products = (\n    vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\n)\nX = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\").drop(\n    columns=[\"ID\", \"basket_ID\"]\n)\nðŸ¤”\n\nHow to keep track of these transformations?\nCannot tune choices"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator",
    "href": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator",
    "title": "Skrub learning materials",
    "section": "Adding the supervised estimator",
    "text": "Adding the supervised estimator\nclassifier = HistGradientBoostingClassifier()\n\ncross_val_score(classifier, X, y, scoring=\"roc_auc\", n_jobs=5)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#skrub-to-the-rescue",
    "href": "pages/slides/expressions/expressions.html#skrub-to-the-rescue",
    "title": "Skrub learning materials",
    "section": "Skrub to the rescue",
    "text": "Skrub to the rescue\n\nBuild complex pipelines involving multiple tables"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#loading-data-1",
    "href": "pages/slides/expressions/expressions.html#loading-data-1",
    "title": "Skrub learning materials",
    "section": "Loading data",
    "text": "Loading data\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = skrub.X(data.baskets[[\"ID\"]])\ny = skrub.y(data.baskets[\"fraud_flag\"])\nproducts = skrub.var(\"products\", data.products)\n\nX, y, products represent inputs to the model\nOperations on those objects are evaluated lazily\n\nRecorded rather than evaluated immediately\nBut a preview is computed for interactive development\n\nThey forward all operations to the result of their evaluation\n\nFull API of the underlying object is available"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-2",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-2",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-3",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-3",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X\nproduct_vectorizer is added to the model"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-4",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-4",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X\nproduct_vectorizer is added to the model\nWe can select columns to transform"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-5",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-5",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(\n        n_components=skrub.choose_int(2, 20)\n    )\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can tune hyperparameters (more later)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#joining-the-product-features-1",
    "href": "pages/slides/expressions/expressions.html#joining-the-product-features-1",
    "title": "Skrub learning materials",
    "section": "Joining the product features",
    "text": "Joining the product features\naggregated_products = (\n    vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\n)\nX = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\").drop(\n    columns=[\"ID\", \"basket_ID\"]\n)\n\nTransformations added to the model\nCan tune choices\nWhile having access to all the dataframeâ€™s functionality"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator-1",
    "href": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator-1",
    "title": "Skrub learning materials",
    "section": "Adding the supervised estimator",
    "text": "Adding the supervised estimator\nclassifier = HistGradientBoostingClassifier()\npred = X.skb.apply(classifier, y=y)\nEvaluation\npred.skb.cross_validate(scoring=\"roc_auc\", n_jobs=5)\nTraining & using a model\n\n\ntrain.py\n\nestimator = pred.skb.get_estimator(fitted=True)\nwith open(\"estimator.pickle\", \"wb\") as ostream:\n    pickle.dump(estimator, ostream)\n\n\n\npredict.py\n\nwith open(\"estimator.pickle\", \"rb\") as istream:\n    estimator = pickle.load(istream)\n\nestimator.predict({'X': unseen_baskets, 'products': unseen_products})"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#easy-inspection",
    "href": "pages/slides/expressions/expressions.html#easy-inspection",
    "title": "Skrub learning materials",
    "section": "Easy inspection",
    "text": "Easy inspection\npred.skb.full_report()\n\nreport"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\n\nAny choice in the pipeline can be tuned\nOptions are specified inline\nInspecting results is easy"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning-1",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning-1",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nWithout skrub: ðŸ˜­ðŸ˜­ðŸ˜­\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#section",
    "href": "pages/slides/expressions/expressions.html#section",
    "title": "Skrub learning materials",
    "section": "",
    "text": "NO!"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning-2",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning-2",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nWith skrub: replace any value with a range\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(\n        n_components=skrub.choose_int(2, 20)\n    )\n)\n\n# ...\n\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()\n\n\nparallel coordinates plot"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#a-teaser-for-later",
    "href": "pages/slides/talk-edf/slides.html#a-teaser-for-later",
    "title": "EDF Talk: Skrub",
    "section": "A teaser for laterâ€¦",
    "text": "A teaser for laterâ€¦\nInspect all the steps of your pipeline: Execution report"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#a-teaser-for-later-1",
    "href": "pages/slides/talk-edf/slides.html#a-teaser-for-later-1",
    "title": "EDF Talk: Skrub",
    "section": "A teaser for laterâ€¦",
    "text": "A teaser for laterâ€¦\nExplore your hyperparameter search space"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#skrub-compatibility",
    "href": "pages/slides/talk-edf/slides.html#skrub-compatibility",
    "title": "EDF Talk: Skrub",
    "section": "skrub compatibility",
    "text": "skrub compatibility\n\nskrub is fully compatible with pandas and polars\nskrub transformers are fully compatible with scikit-learn"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#an-example-pipeline",
    "href": "pages/slides/talk-edf/slides.html#an-example-pipeline",
    "title": "EDF Talk: Skrub",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPreprocess the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#exploring-the-data",
    "href": "pages/slides/talk-edf/slides.html#exploring-the-data",
    "title": "EDF Talk: Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#exploring-the-data-1",
    "href": "pages/slides/talk-edf/slides.html#exploring-the-data-1",
    "title": "EDF Talk: Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/talk-edf/slides.html#exploring-the-data-with-skrub",
    "title": "EDF Talk: Skrub",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\n\nMore examples"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#data-cleaning-with-pandaspolars-setup",
    "href": "pages/slides/talk-edf/slides.html#data-cleaning-with-pandaspolars-setup",
    "title": "EDF Talk: Skrub",
    "section": "Data cleaning with pandas/polars: setup",
    "text": "Data cleaning with pandas/polars: setup\n\nPandasPolars\n\n\n\nimport pandas as pd\nimport numpy as np\ndata = {\n    'Constant int': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'Constant str': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n    'All empty': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\n\ndf_pd = pd.DataFrame(data)\ndisplay(df_pd)\n\n\n\n\n\n\n\n\nConstant int\nB\nConstant str\nD\nAll nan\nAll empty\nDate\n\n\n\n\n0\n1\n2\nx\n4\nNaN\n\n01/01/2023\n\n\n1\n1\n3\nx\n5\nNaN\n\n02/01/2023\n\n\n2\n1\n2\nx\n6\nNaN\n\n03/01/2023\n\n\n\n\n\n\n\n\n\n\nimport polars as pl\nimport numpy as np\ndata = {\n    'Constant int': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'Constant str': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n    'All empty': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\n\ndf_pl = pl.DataFrame(data)\ndisplay(df_pl)\n\n\nshape: (3, 7)\n\n\n\nConstant int\nB\nConstant str\nD\nAll nan\nAll empty\nDate\n\n\ni64\ni64\nstr\ni64\nf64\nstr\nstr\n\n\n\n\n1\n2\n\"x\"\n4\nNaN\n\"\"\n\"01/01/2023\"\n\n\n1\n3\n\"x\"\n5\nNaN\n\"\"\n\"02/01/2023\"\n\n\n1\n2\n\"x\"\n6\nNaN\n\"\"\n\"03/01/2023\""
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "href": "pages/slides/talk-edf/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "title": "EDF Talk: Skrub",
    "section": "Nulls, datetimes, constant columns with pandas/polars",
    "text": "Nulls, datetimes, constant columns with pandas/polars\n\nPandasPolars\n\n\n\n# Parse the datetime strings with a specific format\ndf_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d/%m/%Y')\n\n# Drop columns with only a single unique value\ndf_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_pd_cleaned = drop_empty_columns(df_pd_cleaned)\n\n\n\n\n# Parse the datetime strings with a specific format\ndf_pl = df_pl.with_columns([\n    pl.col(\"Date\").str.strptime(pl.Date, \"%d/%m/%Y\", strict=False).alias(\"Date\")\n])\n\n# Drop columns with only a single unique value\ndf_pl_cleaned = df_pl.select([\n    col for col in df_pl.columns if df_pl[col].n_unique() &gt; 1\n])\n\n# Import selectors for dtype selection\nimport polars.selectors as cs\n\n# Drop columns with only missing values or only empty strings\ndef drop_empty_columns(df):\n    all_nan = df.select(\n        [\n            col for col in df.select(cs.numeric()).columns if \n            df [col].is_nan().all()\n        ]\n    ).columns\n    \n    all_empty = df.select(\n        [\n            col for col in df.select(cs.string()).columns if \n            (df[col].str.strip_chars().str.len_chars()==0).all()\n        ]\n    ).columns\n\n    to_drop = all_nan + all_empty\n\n    return df.drop(to_drop)\n\ndf_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#data-cleaning-with-skrub.cleaner",
    "href": "pages/slides/talk-edf/slides.html#data-cleaning-with-skrub.cleaner",
    "title": "EDF Talk: Skrub",
    "section": "Data cleaning with skrub.Cleaner",
    "text": "Data cleaning with skrub.Cleaner\n\nPandasPolars\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df_pd)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03\n\n\n\n\n\n\n\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df_pl)\ndisplay(df_cleaned)\n\n\nshape: (3, 3)\n\n\n\nB\nD\nDate\n\n\ni64\ni64\ndate\n\n\n\n\n2\n4\n2023-01-01\n\n\n3\n5\n2023-01-02\n\n\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#comparison",
    "href": "pages/slides/talk-edf/slides.html#comparison",
    "title": "EDF Talk: Skrub",
    "section": "Comparison",
    "text": "Comparison\n\nPandasPolars\n\n\n\n\n\nprint(df_pd_cleaned)\n\n   B  D       Date\n0  2  4 2023-01-01\n1  3  5 2023-01-02\n2  2  6 2023-01-03\n\n\n\n\nprint(df_cleaned)\n\n   B  D       Date\n0  2  4 2023-01-01\n1  3  5 2023-01-02\n2  2  6 2023-01-03\n\n\n\n\n\n\n\n\nprint(df_pl_cleaned)\n\nshape: (3, 3)\nâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ B   â”† D   â”† Date       â”‚\nâ”‚ --- â”† --- â”† ---        â”‚\nâ”‚ i64 â”† i64 â”† date       â”‚\nâ•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 2   â”† 4   â”† 2023-01-01 â”‚\nâ”‚ 3   â”† 5   â”† 2023-01-02 â”‚\nâ”‚ 2   â”† 6   â”† 2023-01-03 â”‚\nâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nprint(df_cleaned)\n\nshape: (3, 3)\nâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ B   â”† D   â”† Date       â”‚\nâ”‚ --- â”† --- â”† ---        â”‚\nâ”‚ i64 â”† i64 â”† date       â”‚\nâ•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 2   â”† 4   â”† 2023-01-01 â”‚\nâ”‚ 3   â”† 5   â”† 2023-01-02 â”‚\nâ”‚ 2   â”† 6   â”† 2023-01-03 â”‚\nâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-datetime-features-with-pandaspolars",
    "href": "pages/slides/talk-edf/slides.html#encoding-datetime-features-with-pandaspolars",
    "title": "EDF Talk: Skrub",
    "section": "Encoding datetime features with pandas/polars",
    "text": "Encoding datetime features with pandas/polars\n\nPandasPolars\n\n\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pd = pd.DataFrame(data)\ndatetime_column = \"date\"\ndf_pd[datetime_column] = pd.to_datetime(df_pd[datetime_column], errors='coerce')\n\ndf_pd['year'] = df_pd[datetime_column].dt.year\ndf_pd['month'] = df_pd[datetime_column].dt.month\ndf_pd['day'] = df_pd[datetime_column].dt.day\ndf_pd['hour'] = df_pd[datetime_column].dt.hour\ndf_pd['minute'] = df_pd[datetime_column].dt.minute\ndf_pd['second'] = df_pd[datetime_column].dt.second\n\n\n\n\nimport polars as pl\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pl = pl.DataFrame(data)\ndf_pl = df_pl.with_columns(date=pl.col(\"date\").str.to_datetime())\n\ndf_pl = df_pl.with_columns(\n    year=pl.col(\"date\").dt.year(),\n    month=pl.col(\"date\").dt.month(),\n    day=pl.col(\"date\").dt.day(),\n    hour=pl.col(\"date\").dt.hour(),\n    minute=pl.col(\"date\").dt.minute(),\n    second=pl.col(\"date\").dt.second(),\n)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#adding-periodic-features-with-pandaspolars",
    "href": "pages/slides/talk-edf/slides.html#adding-periodic-features-with-pandaspolars",
    "title": "EDF Talk: Skrub",
    "section": "Adding periodic features with pandas/polars",
    "text": "Adding periodic features with pandas/polars\n\nPandasPolars\n\n\n\ndf_pd['hour_sin'] = np.sin(2 * np.pi * df_pd['hour'] / 24)\ndf_pd['hour_cos'] = np.cos(2 * np.pi * df_pd['hour'] / 24)\n\ndf_pd['month_sin'] = np.sin(2 * np.pi * df_pd['month'] / 12)\ndf_pd['month_cos'] = np.cos(2 * np.pi * df_pd['month'] / 12)\n\n\n\n\ndf_pl = df_pl.with_columns(\n    hour_sin = np.sin(2 * np.pi * pl.col(\"hour\") / 24),\n    hour_cos = np.cos(2 * np.pi * pl.col(\"hour\") / 24),\n    \n    month_sin = np.sin(2 * np.pi * pl.col(\"month\") / 12),\n    month_cos = np.cos(2 * np.pi * pl.col(\"month\") / 12),\n)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "href": "pages/slides/talk-edf/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "title": "EDF Talk: Skrub",
    "section": "Encoding datetime features skrub.DatetimeEncoder",
    "text": "Encoding datetime features skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nX_date = ToDatetime().fit_transform(df[\"date\"])\nde = DatetimeEncoder(periodic_encoding=\"circular\")\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\nshape: (3, 8)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ date_year â”† date_total â”† date_month â”† date_month â”† date_day_ â”† date_day_ â”† date_hour â”† date_hour â”‚\nâ”‚ ---       â”† _seconds   â”† _circular_ â”† _circular_ â”† circular_ â”† circular_ â”† _circular â”† _circular â”‚\nâ”‚ f32       â”† ---        â”† 0          â”† 1          â”† 0         â”† 1         â”† _0        â”† _1        â”‚\nâ”‚           â”† f32        â”† ---        â”† ---        â”† ---       â”† ---       â”† ---       â”† ---       â”‚\nâ”‚           â”†            â”† f64        â”† f64        â”† f64       â”† f64       â”† f64       â”† f64       â”‚\nâ•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 2023.0    â”† 1.6726e9   â”† 0.5        â”† 0.866025   â”† 0.207912  â”† 0.978148  â”† 1.2246e-1 â”† -1.0      â”‚\nâ”‚           â”†            â”†            â”†            â”†           â”†           â”† 6         â”†           â”‚\nâ”‚ 2023.0    â”† 1.6765e9   â”† 0.866025   â”† 0.5        â”† 1.2246e-1 â”† -1.0      â”† 0.866025  â”† -0.5      â”‚\nâ”‚           â”†            â”†            â”†            â”† 6         â”†           â”†           â”†           â”‚\nâ”‚ 2023.0    â”† 1.6793e9   â”† 1.0        â”† 6.1232e-17 â”† -0.866025 â”† -0.5      â”† -1.0      â”† -1.8370e- â”‚\nâ”‚           â”†            â”†            â”†            â”†           â”†           â”†           â”† 16        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#what-periodic-features-look-like",
    "href": "pages/slides/talk-edf/slides.html#what-periodic-features-look-like",
    "title": "EDF Talk: Skrub",
    "section": "What periodic features look like",
    "text": "What periodic features look like"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "href": "pages/slides/talk-edf/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "title": "EDF Talk: Skrub",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler\n\n\nSkrub wants to solve ML problems based partly on solid engineering and partly on statistical notions. The SquashingScaler is based on the second part, and is taken from a recent paper that evaluates different techniques for improving the performance of NNs."
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "href": "pages/slides/talk-edf/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "title": "EDF Talk: Skrub",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-categorical-stringtext-features",
    "href": "pages/slides/talk-edf/slides.html#encoding-categorical-stringtext-features",
    "title": "EDF Talk: Skrub",
    "section": "Encoding categorical (string/text) features",
    "text": "Encoding categorical (string/text) features\nCategorical features have a â€œcardinalityâ€: the number of unique values\n\nLow cardinality features: OneHotEncoder\nHigh cardinality features (&gt;40 unique values): skrub.StringEncoder\nTextual features: skrub.TextEncoder and pretrained models from HuggingFace Hub"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/talk-edf/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "EDF Talk: Skrub",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer\nfrom skrub import TableVectorizer, TextEncoder\n\ntext = TextEncoder()\ntable_vec = TableVectorizer(high_cardinality=text)\ndf_encoded = table_vec.fit_transform(df)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-all-the-features-tablevectorizer-1",
    "href": "pages/slides/talk-edf/slides.html#encoding-all-the-features-tablevectorizer-1",
    "title": "EDF Talk: Skrub",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#fine-grained-column-transformations-with-applytocols",
    "href": "pages/slides/talk-edf/slides.html#fine-grained-column-transformations-with-applytocols",
    "title": "EDF Talk: Skrub",
    "section": "Fine-grained column transformations with ApplyToCols",
    "text": "Fine-grained column transformations with ApplyToCols\n\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#fine-grained-column-transformations-with-applytocols-1",
    "href": "pages/slides/talk-edf/slides.html#fine-grained-column-transformations-with-applytocols-1",
    "title": "EDF Talk: Skrub",
    "section": "Fine-grained column transformations with ApplyToCols",
    "text": "Fine-grained column transformations with ApplyToCols\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ntext_bar\ntext_baz\ntext_foo\nnumber\n\n\n\n\n0\n0.0\n0.0\n1.0\n-1.224745\n\n\n1\n1.0\n0.0\n0.0\n0.000000\n\n\n2\n0.0\n1.0\n0.0\n1.224745"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline",
    "title": "EDF Talk: Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-1",
    "title": "EDF Talk: Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-2",
    "title": "EDF Talk: Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "href": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "title": "EDF Talk: Skrub",
    "section": "Build a predictive pipeline with tabular_pipeline",
    "text": "Build a predictive pipeline with tabular_pipeline\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_pipeline(Ridge())"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/talk-edf/slides.html#we-now-have-a-pipeline",
    "title": "EDF Talk: Skrub",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\nExplore the data\n\nskrub.TableReport\n\nPre-process the data\n\nCleaner, ToDatetime â€¦\n\nPerform feature engineering\n\nskrub.TableVectorizer,SquashingScaler, TextEncoder, StringEncoderâ€¦\n\nBuild a scikit-learn pipeline\n\ntabular_pipeline, sklearn.pipeline.make_pipeline â€¦\n\n???\nProfit ðŸ“ˆ"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#what-if",
    "href": "pages/slides/talk-edf/slides.html#what-if",
    "title": "EDF Talk: Skrub",
    "section": "What ifâ€¦",
    "text": "What ifâ€¦\n\nYour data is spread over multiple tables?\nYou want to avoid data leakage?\nYou want to tune more than just the hyperparameters of your model?\nYou want to guarantee that your pipeline is replayed exactly on new data?"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#section-2",
    "href": "pages/slides/talk-edf/slides.html#section-2",
    "title": "EDF Talk: Skrub",
    "section": "",
    "text": "When a normal pipe is not enoughâ€¦\n\nâ€¦ the skrub DataOps come to the rescue ðŸš’"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#dataops",
    "href": "pages/slides/talk-edf/slides.html#dataops",
    "title": "EDF Talk: Skrub",
    "section": "DataOpsâ€¦",
    "text": "DataOpsâ€¦\n\nExtend the scikit-learn machinery to complex multi-table operations, and take care of data leakage\nTrack all operations with a computational graph (a Data Ops plan)\nAllow tuning any operation in the data plan\nCan be persisted and shared easily"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/talk-edf/slides.html#how-do-dataops-work-though",
    "title": "EDF Talk: Skrub",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#dataops-plans-learners-oh-my",
    "href": "pages/slides/talk-edf/slides.html#dataops-plans-learners-oh-my",
    "title": "EDF Talk: Skrub",
    "section": "DataOps, Plans, learners: oh my!",
    "text": "DataOps, Plans, learners: oh my!\n\nA DataOp (singular) wraps a single operation, and can be combined and concatenated with other DataOps.\nThe Data Ops Plan is a collective name for the directed computational graph that tracks a sequence and combination of DataOps.\nThe plan can be exported as a standalone object called learner. The learner works like a scikit-learn estimator that takes a dictionary of values rather than just X and y."
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#starting-with-the-dataops",
    "href": "pages/slides/talk-edf/slides.html#starting-with-the-dataops",
    "title": "EDF Talk: Skrub",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\nimport skrub\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\n\nX, y, products represent inputs to the pipeline.\nskrub splits X and y when training."
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#building-a-full-data-plan",
    "href": "pages/slides/talk-edf/slides.html#building-a-full-data-plan",
    "title": "EDF Talk: Skrub",
    "section": "Building a full data plan",
    "text": "Building a full data plan\n\nfrom skrub import selectors as s\n\nvectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder()\n)\nvectorized_products = products.skb.apply(vectorizer, cols=s.all() - \"basket_ID\")"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#building-a-full-data-plan-1",
    "href": "pages/slides/talk-edf/slides.html#building-a-full-data-plan-1",
    "title": "EDF Talk: Skrub",
    "section": "Building a full data plan",
    "text": "Building a full data plan\n\naggregated_products = vectorized_products.groupby(\n    \"basket_ID\"\n).agg(\"mean\").reset_index()\n\nfeatures = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\")\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#building-a-full-data-plan-2",
    "href": "pages/slides/talk-edf/slides.html#building-a-full-data-plan-2",
    "title": "EDF Talk: Skrub",
    "section": "Building a full data plan",
    "text": "Building a full data plan\n\nfrom sklearn.ensemble import ExtraTreesClassifier  \npredictions = features.skb.apply(\n    ExtraTreesClassifier(n_jobs=-1), y=y\n)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#inspecting-the-data-plan",
    "href": "pages/slides/talk-edf/slides.html#inspecting-the-data-plan",
    "title": "EDF Talk: Skrub",
    "section": "Inspecting the data plan",
    "text": "Inspecting the data plan\npredictions.skb.full_report()\n\nExecution report\nEach node:\n\nShows a preview of the data resulting from the operation\nReports the location in the code where the code is defined\nShows the run time of the node (in the next release)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/talk-edf/slides.html#exporting-the-plan-in-a-learner",
    "title": "EDF Talk: Skrub",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe data plan can be exported as a learner:\n\n# anywhere\nlearner = predictions.skb.make_learner(fitted=True)\n\n\nThen, the learner can be pickled â€¦\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\nâ€¦ loaded â€¦\nwith open(\"learner.bin\", \"rb\") as fp:\n    loaded_learner = pickle.load(fp)\n\n\nâ€¦ and applied to new data:\n\ndata = skrub.datasets.fetch_credit_fraud(split=\"test\")\nnew_baskets = data.baskets\nnew_products = data.products\nloaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})\n\narray([0, 0, 0, ..., 0, 0, 0], shape=(31549,))"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#hyperparameter-tuning-in-a-data-plan",
    "href": "pages/slides/talk-edf/slides.html#hyperparameter-tuning-in-a-data-plan",
    "title": "EDF Talk: Skrub",
    "section": "Hyperparameter tuning in a Data Plan",
    "text": "Hyperparameter tuning in a Data Plan\nskrub implements four choose_* functions:\n\nchoose_from: select from the given list of options\nchoose_int: select an integer within a range\nchoose_float: select a float within a range\nchoose_bool: select a bool\noptional: chooses whether to execute the given operation"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/talk-edf/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "EDF Talk: Skrub",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#tuning-with-dataops-is-simple",
    "href": "pages/slides/talk-edf/slides.html#tuning-with-dataops-is-simple",
    "title": "EDF Talk: Skrub",
    "section": "Tuning with DataOps is simple!",
    "text": "Tuning with DataOps is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)\nsearch = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#tuning-with-dataops-is-not-limited-to-estimators",
    "href": "pages/slides/talk-edf/slides.html#tuning-with-dataops-is-not-limited-to-estimators",
    "title": "EDF Talk: Skrub",
    "section": "Tuning with DataOps is not limited to estimators",
    "text": "Tuning with DataOps is not limited to estimators\n\nPandasPolars\n\n\n\ndf = pd.DataFrame(\n    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n)\n\ndf_do = skrub.var(\"grades\", df)\n\nagg_grades = df_do.groupby(\"subject\").agg(skrub.choose_from([\"count\", \"mean\"]))\nagg_grades.skb.describe_param_grid()\n\n\"- choose_from(['count', 'mean']): ['count', 'mean']\\n\"\n\n\n\n\n\ndf = pl.DataFrame(\n    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n)\n\ndf_do = skrub.var(\"grades\", df)\n\nagg_grades = df_do.group_by(\"subject\").agg(\n    skrub.choose_from([pl.mean(\"grade\"), pl.count(\"grade\")])\n)\nagg_grades.skb.describe_param_grid()\n\n'- choose_from([&lt;Expr [\\'col(\"grade\").mean()\\'] at 0x783825B3A3D0&gt;, &lt;Expr [\\'col(\"grade\").count()\\'] at 0x78382C9A0090&gt;]): [&lt;Expr [\\'col(\"grade\").mean()\\'] at 0x783825B3A3D0&gt;, &lt;Expr [\\'col(\"grade\").count()\\'] at 0x78382C9A0090&gt;]\\n'"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#run-hyperparameter-search",
    "href": "pages/slides/talk-edf/slides.html#run-hyperparameter-search",
    "title": "EDF Talk: Skrub",
    "section": "Run hyperparameter search",
    "text": "Run hyperparameter search\n# fit the search \nsearch = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True, cv=5)\n\n# save the best learner\nbest_learner = search.best_learner_"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#observe-the-impact-of-the-hyperparameters",
    "href": "pages/slides/talk-edf/slides.html#observe-the-impact-of-the-hyperparameters",
    "title": "EDF Talk: Skrub",
    "section": "Observe the impact of the hyperparameters",
    "text": "Observe the impact of the hyperparameters\nData Ops provide a built-in parallel coordinate plot.\nsearch = pred.skb.get_randomized_search(fitted=True)\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#more-information-about-the-data-ops",
    "href": "pages/slides/talk-edf/slides.html#more-information-about-the-data-ops",
    "title": "EDF Talk: Skrub",
    "section": "More information about the Data Ops",
    "text": "More information about the Data Ops\n\nSkrub example gallery\nTutorial on timeseries forecasting at Euroscipy 2025\nSkrub User guide\nA Kaggle notebook on addressing the Titanic survival challenge with Data Ops"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#getting-involved",
    "href": "pages/slides/talk-edf/slides.html#getting-involved",
    "title": "EDF Talk: Skrub",
    "section": "Getting involved",
    "text": "Getting involved\nDo you want to learn more?\n\nSkrub website\nSkrub materials website\nDiscord server\n\nFollow skrub on:\n\nBluesky\nLinkedIn\n\nStar skrub on GitHub, or contribute directly:\n\nGit repository"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#tldw",
    "href": "pages/slides/talk-edf/slides.html#tldw",
    "title": "EDF Talk: Skrub",
    "section": "tl;dw",
    "text": "tl;dw\nskrub provides\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering\nDataOps, plans, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#plan-for-the-presentation",
    "href": "pages/slides/skrub-intro/skrub-intro.html#plan-for-the-presentation",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Plan for the presentation",
    "text": "Plan for the presentation\n\nIntroducing skrub\n\nExample use case\nDetailed explanation of the features\nGetting involved"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#in-the-beginning",
    "href": "pages/slides/skrub-intro/skrub-intro.html#in-the-beginning",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "In the beginningâ€¦",
    "text": "In the beginningâ€¦\nSkrub stems from the development of dirty_cat, a package that provided support for handling dirty columns and perform fuzzy joins across tables.\n\nIt has since evolved into a package that provides:\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#skrubs-vision",
    "href": "pages/slides/skrub-intro/skrub-intro.html#skrubs-vision",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Skrubâ€™s vision",
    "text": "Skrubâ€™s vision\nThe goal of skrub is to facilitate building and deploying machine-learning models on pandas and polars dataframes (later, SQL databasesâ€¦)\n\n\n\nSkrub is high-level, with a philosophy and an API matching that of scikit-learn. It strives to bridge the worlds of databases and machine-learning, enabling imperfect assembly and representations of the data when it is noisy."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#an-example-use-case",
    "href": "pages/slides/skrub-intro/skrub-intro.html#an-example-use-case",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "An example use case",
    "text": "An example use case\n\nGather some data\n\nEmployee salaries, census, customer churnâ€¦\n\nExplore the data\n\nNull values, dtypes, correlated featuresâ€¦\n\nPre-process the data\nBuild a scikit-learn estimator\n???\nProfit ðŸ“ˆ"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data",
    "href": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nimport skrub\nimport pandas as pd\nfrom skrub.datasets import fetch_employee_salaries\n\ndataset = fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\nemployees.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data-interactively",
    "href": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data-interactively",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Exploring the dataâ€¦ interactively!",
    "text": "Exploring the dataâ€¦ interactively!\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\n\nObtain high-level statistics about the data (number of uniques, missing valuesâ€¦)\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\nMore examples here"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-2",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nmodel = make_pipeline(StandardScaler(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-3",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(SimpleImputer(), StandardScaler(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-4",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-5",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-5",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-6",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-6",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['year_first_hired']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['gender', 'department',\n                                                   'department_name',\n                                                   'division',\n                                                   'assignment_category',\n                                                   'employee_position_title',\n                                                   'date_first_hired'])])),\n                ('simpleimputer', SimpleImputer()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('columntransformer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformersÂ \n[('standardscaler', ...), ('onehotencoder', ...)]\n\n\n\nremainderÂ \n'drop'\n\n\n\nsparse_thresholdÂ \n0.3\n\n\n\nn_jobsÂ \nNone\n\n\n\ntransformer_weightsÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\nverbose_feature_names_outÂ \nTrue\n\n\n\nforce_int_remainder_colsÂ \n'deprecated'\n\n\n\n\n            \n        \n    standardscaler['year_first_hired']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopyÂ \nTrue\n\n\n\nwith_meanÂ \nTrue\n\n\n\nwith_stdÂ \nTrue\n\n\n\n\n            \n        \n    onehotencoder['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title', 'date_first_hired']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategoriesÂ \n'auto'\n\n\n\ndropÂ \nNone\n\n\n\nsparse_outputÂ \nTrue\n\n\n\ndtypeÂ \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknownÂ \n'ignore'\n\n\n\nmin_frequencyÂ \nNone\n\n\n\nmax_categoriesÂ \nNone\n\n\n\nfeature_name_combinerÂ \n'concat'\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_valuesÂ \nnan\n\n\n\nstrategyÂ \n'mean'\n\n\n\nfill_valueÂ \nNone\n\n\n\ncopyÂ \nTrue\n\n\n\nadd_indicatorÂ \nFalse\n\n\n\nkeep_empty_featuresÂ \nFalse\n\n\n\n\n            \n        \n    Ridge?Documentation for Ridge\n        \n            \n                Parameters\n                \n\n\n\n\nalphaÂ \n1.0\n\n\n\nfit_interceptÂ \nTrue\n\n\n\ncopy_XÂ \nTrue\n\n\n\nmax_iterÂ \nNone\n\n\n\ntolÂ \n0.0001\n\n\n\nsolverÂ \n'auto'\n\n\n\npositiveÂ \nFalse\n\n\n\nrandom_stateÂ \nNone"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Enter: tabular_learner",
    "text": "Enter: tabular_learner\nimport skrub\nfrom sklearn.linear_model import Ridge\ntl = skrub.tabular_learner(Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Enter: tabular_learner",
    "text": "Enter: tabular_learner\n\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('tablevectorizer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_thresholdÂ \n40\n\n\n\nlow_cardinalityÂ \nOneHotEncoder..._output=False)\n\n\n\nhigh_cardinalityÂ \nStringEncoder()\n\n\n\nnumericÂ \nPassThrough()\n\n\n\ndatetimeÂ \nDatetimeEncod...ding='spline')\n\n\n\nspecific_transformersÂ \n()\n\n\n\ndrop_null_fractionÂ \n1.0\n\n\n\ndrop_if_constantÂ \nFalse\n\n\n\ndrop_if_uniqueÂ \nFalse\n\n\n\ndatetime_formatÂ \nNone\n\n\n\nn_jobsÂ \nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolutionÂ \n'hour'\n\n\n\nadd_weekdayÂ \nFalse\n\n\n\nadd_total_secondsÂ \nTrue\n\n\n\nadd_day_of_yearÂ \nFalse\n\n\n\nperiodic_encodingÂ \n'spline'\n\n\n\n\n            \n        \n    low_cardinalityOneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategoriesÂ \n'auto'\n\n\n\ndropÂ \n'if_binary'\n\n\n\nsparse_outputÂ \nFalse\n\n\n\ndtypeÂ \n'float32'\n\n\n\nhandle_unknownÂ \n'ignore'\n\n\n\nmin_frequencyÂ \nNone\n\n\n\nmax_categoriesÂ \nNone\n\n\n\nfeature_name_combinerÂ \n'concat'\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_componentsÂ \n30\n\n\n\nvectorizerÂ \n'tfidf'\n\n\n\nngram_rangeÂ \n(3, ...)\n\n\n\nanalyzerÂ \n'char_wb'\n\n\n\nstop_wordsÂ \nNone\n\n\n\nrandom_stateÂ \nNone\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_valuesÂ \nnan\n\n\n\nstrategyÂ \n'mean'\n\n\n\nfill_valueÂ \nNone\n\n\n\ncopyÂ \nTrue\n\n\n\nadd_indicatorÂ \nTrue\n\n\n\nkeep_empty_featuresÂ \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopyÂ \nTrue\n\n\n\nwith_meanÂ \nTrue\n\n\n\nwith_stdÂ \nTrue\n\n\n\n\n            \n        \n    Ridge?Documentation for Ridge\n        \n            \n                Parameters\n                \n\n\n\n\nalphaÂ \n1.0\n\n\n\nfit_interceptÂ \nTrue\n\n\n\ncopy_XÂ \nTrue\n\n\n\nmax_iterÂ \nNone\n\n\n\ntolÂ \n0.0001\n\n\n\nsolverÂ \n'auto'\n\n\n\npositiveÂ \nFalse\n\n\n\nrandom_stateÂ \nNone"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#a-robust-baseline-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#a-robust-baseline-tabular_learner",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "A robust baseline: tabular_learner",
    "text": "A robust baseline: tabular_learner\nGiven a scikit-learn estimator, tabular_learner:\n\nextracts numerical features\nimputes missing values with SimpleImputer (optional)\nscales the data with StandardScaler (optional)\n\n\nYou can also write â€œtabular_learner(\"regressor\")â€:\n\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('tablevectorizer', ...), ('histgradientboostingregressor', ...)]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_thresholdÂ \n40\n\n\n\nlow_cardinalityÂ \nToCategorical()\n\n\n\nhigh_cardinalityÂ \nStringEncoder()\n\n\n\nnumericÂ \nPassThrough()\n\n\n\ndatetimeÂ \nDatetimeEncoder()\n\n\n\nspecific_transformersÂ \n()\n\n\n\ndrop_null_fractionÂ \n1.0\n\n\n\ndrop_if_constantÂ \nFalse\n\n\n\ndrop_if_uniqueÂ \nFalse\n\n\n\ndatetime_formatÂ \nNone\n\n\n\nn_jobsÂ \nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolutionÂ \n'hour'\n\n\n\nadd_weekdayÂ \nFalse\n\n\n\nadd_total_secondsÂ \nTrue\n\n\n\nadd_day_of_yearÂ \nFalse\n\n\n\nperiodic_encodingÂ \nNone\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_componentsÂ \n30\n\n\n\nvectorizerÂ \n'tfidf'\n\n\n\nngram_rangeÂ \n(3, ...)\n\n\n\nanalyzerÂ \n'char_wb'\n\n\n\nstop_wordsÂ \nNone\n\n\n\nrandom_stateÂ \nNone\n\n\n\n\n            \n        \n    HistGradientBoostingRegressor?Documentation for HistGradientBoostingRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nlossÂ \n'squared_error'\n\n\n\nquantileÂ \nNone\n\n\n\nlearning_rateÂ \n0.1\n\n\n\nmax_iterÂ \n100\n\n\n\nmax_leaf_nodesÂ \n31\n\n\n\nmax_depthÂ \nNone\n\n\n\nmin_samples_leafÂ \n20\n\n\n\nl2_regularizationÂ \n0.0\n\n\n\nmax_featuresÂ \n1.0\n\n\n\nmax_binsÂ \n255\n\n\n\ncategorical_featuresÂ \n'from_dtype'\n\n\n\nmonotonic_cstÂ \nNone\n\n\n\ninteraction_cstÂ \nNone\n\n\n\nwarm_startÂ \nFalse\n\n\n\nearly_stoppingÂ \n'auto'\n\n\n\nscoringÂ \n'loss'\n\n\n\nvalidation_fractionÂ \n0.1\n\n\n\nn_iter_no_changeÂ \n10\n\n\n\ntolÂ \n1e-07\n\n\n\nverboseÂ \n0\n\n\n\nrandom_stateÂ \nNone"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#unmasking-the-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#unmasking-the-tabular_learner",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Unmasking the tabular_learner",
    "text": "Unmasking the tabular_learner"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\n\nPre-process the data\nConvert complex data types (datetimes, text) into numerical features"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\nPre-process the data\n\nEnsure consistent column names\nDetect missing values such as â€œN/Aâ€\nDrop empty columns\nCheck and convert dtypes to np.float32\nParse dates, ensuring consistent dtype and timezone\nIdentify which categorical features are low- and high-cardinality"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-2",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-3",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\nConvert complex data types (datetimes, text) into numerical features\n\nEncode dates with DateTimeEncoder\nEncode low-cardinality features (&lt;=30 cat.) with OneHotEncoder\nEncode high-cardinality features (&gt;30 cat.) with:\n\nGapEncoder: Relatively slow, easily interpretable, good quality embeddings. Target encoding and hashing.\nMinHashEncoder: Very fast, somewhat low quality embeddings. Hashing ngrams.\nTextEncoder: Very slow, relies on language models, best solution for text and when context is available.\nStringEncoder: Best trade-off between compute cost and embeddings quality. Tf-idf followed by SVD.\n\n\n\nHigh-cardinality encoders are robust in presence of typos and dirty data."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-4",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\n\nvectorizer = skrub.TableVectorizer()\ntransformed = vectorizer.fit_transform(employees)\nfrom pprint import pprint\n\npprint(vectorizer.column_to_kind_)\n\n{'assignment_category': 'low_cardinality',\n 'date_first_hired': 'datetime',\n 'department': 'low_cardinality',\n 'department_name': 'low_cardinality',\n 'division': 'high_cardinality',\n 'employee_position_title': 'high_cardinality',\n 'gender': 'low_cardinality',\n 'year_first_hired': 'numeric'}\n\n\n\npprint(vectorizer.all_processing_steps_[\"date_first_hired\"])\n\n[CleanNullStrings(),\n DropUninformative(),\n ToDatetime(),\n DatetimeEncoder(),\n {'date_first_hired_day': ToFloat32(), 'date_first_hired_month': ToFloat32(), ...}]"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nimport pandas as pd\n\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/skrub-data/datasets/master\"\n    \"/data/bike-sharing-dataset.csv\"\n)\n# Extract our input data (X) and the target column (y)\ny = data[\"cnt\"]\nX = data[[\"date\", \"holiday\", \"temp\", \"hum\", \"windspeed\", \"weathersit\"]]\n\nX\n\n\n\n\n\n\n\n\ndate\nholiday\ntemp\nhum\nwindspeed\nweathersit\n\n\n\n\n0\n2011-01-01 00:00:00\n0\n0.24\n0.81\n0.0000\n1\n\n\n1\n2011-01-01 01:00:00\n0\n0.22\n0.80\n0.0000\n1\n\n\n2\n2011-01-01 02:00:00\n0\n0.22\n0.80\n0.0000\n1\n\n\n3\n2011-01-01 03:00:00\n0\n0.24\n0.75\n0.0000\n1\n\n\n4\n2011-01-01 04:00:00\n0\n0.24\n0.75\n0.0000\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n17374\n2012-12-31 19:00:00\n0\n0.26\n0.60\n0.1642\n2\n\n\n17375\n2012-12-31 20:00:00\n0\n0.26\n0.60\n0.1642\n2\n\n\n17376\n2012-12-31 21:00:00\n0\n0.26\n0.60\n0.1642\n1\n\n\n17377\n2012-12-31 22:00:00\n0\n0.26\n0.56\n0.1343\n1\n\n\n17378\n2012-12-31 23:00:00\n0\n0.26\n0.65\n0.1343\n1\n\n\n\n\n17379 rows Ã— 6 columns"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nfrom pprint import pprint\nfrom skrub import TableVectorizer, DatetimeEncoder\n\ntable_vec_weekday = TableVectorizer(datetime=DatetimeEncoder(add_weekday=True)).fit(X)\npprint(table_vec_weekday.get_feature_names_out())\n\narray(['date_year', 'date_month', 'date_day', 'date_hour',\n       'date_total_seconds', 'date_weekday', 'holiday', 'temp', 'hum',\n       'windspeed', 'weathersit'], dtype='&lt;U18')"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-2",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\npipeline_weekday = make_pipeline(table_vec_weekday, HistGradientBoostingRegressor())\n\ncross_val_score(\n    pipeline_weekday, X, y, scoring=\"neg_mean_squared_error\",\n    cv=TimeSeriesSplit(n_splits=5),\n)\n\narray([ -3694.45159469,  -3180.1148674 , -15183.44808403,  -4824.29173547,\n        -5391.06731737])"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-3",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-4",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\n\n\nTimeseries support in skrub is still in its early stages! Please stay tuned for new developments."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-2",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-3",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values",
    "href": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "AggJoiner: automatically aggregate values",
    "text": "AggJoiner: automatically aggregate values"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "AggJoiner: automatically aggregate values",
    "text": "AggJoiner: automatically aggregate values\n\nimport skrub\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    \"UID\": [28, 32, 28], \n    \"Basket ID\": [1100, 1300, 1400]})\ndf2 = pd.DataFrame({\n    \"Basket ID\": [1100, 1100, 1100, 1300, 1400], \n    \"Product ID\": [\"A521\", \"B695\", \"F221\", \"W214\", \"B695\",], \n    \"Price\": [25, 30, 10, 320, 30]})\n\njoiner = skrub.AggJoiner(df2, operations=\"sum\", key=\"Basket ID\", cols=\"Price\")\njoiner.fit_transform(df1)\n\n\n\n\n\n\n\n\nUID\nBasket ID\nPrice_sum\n\n\n\n\n0\n28\n1100\n65\n\n\n1\n32\n1300\n320\n\n\n2\n28\n1400\n30"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-infer-missing-values",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-infer-missing-values",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "InterpolationJoiner: infer missing values",
    "text": "InterpolationJoiner: infer missing values\n\nimport pandas as pd\n\nfrom skrub.datasets import fetch_flight_delays\n\ndataset = fetch_flight_delays()\nweather = dataset.weather\nweather = weather.sample(100_000, random_state=0, ignore_index=True)\nstations = dataset.stations\nweather = stations.merge(weather, on=\"ID\")[\n    [\"LATITUDE\", \"LONGITUDE\", \"YEAR/MONTH/DAY\", \"TMAX\", \"PRCP\", \"SNOW\"]\n]\nweather[\"YEAR/MONTH/DAY\"] = pd.to_datetime(weather[\"YEAR/MONTH/DAY\"])\n\n\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nYEAR/MONTH/DAY\nTMAX\nPRCP\nSNOW\n\n\n\n\n0\n25.333\n55.517\n2008-11-16\n297.0\nNaN\nNaN\n\n\n1\n25.333\n55.517\n2008-04-12\n333.0\nNaN\nNaN\n\n\n2\n25.255\n55.364\n2008-08-28\n430.0\n0.0\nNaN\n\n\n3\n25.255\n55.364\n2008-02-17\n264.0\n0.0\nNaN\n\n\n4\n25.255\n55.364\n2008-11-25\n291.0\nNaN\nNaN"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "InterpolationJoiner",
    "text": "InterpolationJoiner\n\nfrom skrub import InterpolationJoiner\njoiner = InterpolationJoiner(\n    aux_table,\n    key=[\"LATITUDE\", \"LONGITUDE\", \"YEAR/MONTH/DAY\"],\n    suffix=\"_predicted\",\n).fit(main_table)\njoin = joiner.transform(main_table)\njoin.head()\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nYEAR/MONTH/DAY\nTMAX\nPRCP\nSNOW\nTMAX_predicted\nPRCP_predicted\nSNOW_predicted\n\n\n\n\n0\n25.333\n55.517\n2008-11-16\n297.0\nNaN\nNaN\n253.131219\n10.099097\n-0.001023\n\n\n1\n25.333\n55.517\n2008-04-12\n333.0\nNaN\nNaN\n292.267381\n8.770696\n0.142712\n\n\n2\n25.255\n55.364\n2008-08-28\n430.0\n0.0\nNaN\n330.768383\n22.541307\n-0.053227\n\n\n3\n25.255\n55.364\n2008-02-17\n264.0\n0.0\nNaN\n274.812990\n12.980406\n0.239012\n\n\n4\n25.255\n55.364\n2008-11-25\n291.0\nNaN\nNaN\n260.411121\n6.621378\n-0.014548"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "InterpolationJoiner",
    "text": "InterpolationJoiner"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-4",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining\n\nJoiner: Perform fuzzy-joining: join columns that contain similar-looking values.\nAggJoiner Aggregate an auxiliary dataframe before joining it on a base dataframe, and create new features that aggregate (sum, mean, modeâ€¦) the values in the columns.\nMultiAggJoiner extends AggJoiner to a multi-table scenario.\nInterpolationJoiner Perform an equi-join and estimate what missing rows would contain if they existed in the table.\n\n\nAll Joiner objects are scikit-learn estimators, so they can be used in a Pipeline."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-deduplication",
    "href": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-deduplication",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Additional goodies: deduplication",
    "text": "Additional goodies: deduplication\ndeduplicate misspelled categories\n\nfrom skrub import deduplicate\npprint(duplicated)\ndeduplicate_correspondence = deduplicate(duplicated)\npprint(deduplicate_correspondence.to_dict())\n\n['ulack', 'black', 'black', 'xudte', 'white', 'white']\n{'black': 'black', 'ulack': 'black', 'white': 'white', 'xudte': 'white'}\n\n\nDoc"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-wikipedia-embeddings-as-features",
    "href": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-wikipedia-embeddings-as-features",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Additional goodies: Wikipedia embeddings as features",
    "text": "Additional goodies: Wikipedia embeddings as features\nKEN embeddings capture relational information about all entities in Wikipedia.\nfrom skrub.datasets import fetch_ken_embeddings\nembedding_games = fetch_ken_embeddings(\n    search_types=\"game\",\n    exclude=\"companies|developer\",\n    embedding_table_id=\"games\",\n)\n\nDoc"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#install-skrub",
    "href": "pages/slides/skrub-intro/skrub-intro.html#install-skrub",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Install skrub",
    "text": "Install skrub\n\nBase installation:\n# with pip\npip install skrub -U\n# with conda\nconda install -c conda-forge skrub\n\n\nFor deep learning features such as TextEncoder:\n# with pip\npip install skrub[transformers] -U\n# with conda\nconda install -c conda-forge skrub[transformers]\n\n\nDocumentation"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#join-the-community",
    "href": "pages/slides/skrub-intro/skrub-intro.html#join-the-community",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Join the community",
    "text": "Join the community\n\nSkrub website\nSkrub materials website\nGit repository\nDiscord server\nBluesky"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#contribute-to-skrub",
    "href": "pages/slides/skrub-intro/skrub-intro.html#contribute-to-skrub",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Contribute to skrub",
    "text": "Contribute to skrub\n\nOpen an issue on GitHub\nCheck out the documentation on how to contribute"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skrub learning materials index",
    "section": "",
    "text": "Star\n\n\n\n\n\n\nNote\n\n\n\nPyData 2025 Slides:\nFind the slides for the PyData 2025 talk here.\n\n\nThis is the Skrub learning materials website: this is where youâ€™ll be able to find all the material that is used to teach and present Skrub to audiences.\nSlides for presentations, talks and lectures are in slides section. Notebooks and blog posts are instead located in the notebook section.\nNote that the material that is gathered here may become obsolete depending on the development of Skrub.\nFor up-to-date information on Skrub and its API refer to the documentation, while\nadditional examples are available in the main gallery. Material on the main website is constantly tested with the latest version of Skrub."
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object",
    "href": "pages/slides/discover/discover.html#the-discover-object",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object"
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object-1",
    "href": "pages/slides/discover/discover.html#the-discover-object-1",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object"
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object-2",
    "href": "pages/slides/discover/discover.html#the-discover-object-2",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object\n\nGive a high level overview of the content of a data lake\nBuild an (approximate) schema of the data\nSuggest tables that are relevant to what the user provides"
  },
  {
    "objectID": "pages/slides/discover/discover.html#planned-features",
    "href": "pages/slides/discover/discover.html#planned-features",
    "title": "Discover object",
    "section": "Planned features",
    "text": "Planned features\n\nIf no query is provided:\n\n\nGiven a collection of tables, profile them and produce aggregated statistics.\nDtypes, null values, shape of the tables.\n\n\nIf a query table is provided\n\n\nMeasure various pairwise metrics between columns in the query table, and the columns in the collection of tables.\nRank the columns based on the metrics to find those that are most relevant.\nJaccard containment will be the first metric.\nStatistics remain available to perform feature selection."
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover\n\npath_to_tables = \"./many_tables/\"\ndiscover = Discover(path_to_tables)\n\ndataframe_stats = discover.fit_transform()"
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code-1",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code-1",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover\nimport pandas as pd\n\npath_to_tables = \"./many_tables/\"\nquery_table = pd.read_csv(\"this_table.csv\")\n\ndiscover = Discover(path_to_tables)\n\nranking_by_column = discover.fit_transform(query_table)"
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code-2",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code-2",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover, MultiAggJoiner\nimport pandas as pd\n\npath_to_tables = \"./many_tables/\"\nquery_table = pd.read_csv(\"this_table.csv\")\n\ndiscover = Discover(path_to_tables)\n\nranking_by_column = discover.fit_transform(query_table)\n\njoiner = MultiAggJoiner(ranking_by_column)\njoined_table = joiner.fit_transform(query_table)"
  },
  {
    "objectID": "pages/slides/discover/discover.html#interface-with-the-data",
    "href": "pages/slides/discover/discover.html#interface-with-the-data",
    "title": "Discover object",
    "section": "Interface with the data",
    "text": "Interface with the data\n\nThe initial implementation will read from a path/glob\nLater version will target SQL databases\nWhat other technologies should we consider?"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#fun-facts",
    "href": "pages/slides/soda-kickoff/slides.html#fun-facts",
    "title": "SODA Kickoff 2025",
    "section": "Fun facts",
    "text": "Fun facts\n\nIâ€™m Italian, but I donâ€™t drink coffee, wine, and I like pizza with fries\nI did my PhD in CÃ´te dâ€™Azur, and I moved away because it was too sunny and I donâ€™t like the sea"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#fun-facts-1",
    "href": "pages/slides/soda-kickoff/slides.html#fun-facts-1",
    "title": "SODA Kickoff 2025",
    "section": "Fun facts",
    "text": "Fun facts\n\nIâ€™m mildly obsessed with matplotlib"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#an-example-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#an-example-pipeline",
    "title": "SODA Kickoff 2025",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPre-process the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data",
    "title": "SODA Kickoff 2025",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data-1",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data-1",
    "title": "SODA Kickoff 2025",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data-with-skrub",
    "title": "SODA Kickoff 2025",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas",
    "href": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas",
    "title": "SODA Kickoff 2025",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'A': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'C': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'E': [np.nan, np.nan, np.nan],  # All missing values \n    'F': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\ndf = pd.DataFrame(data)\ndisplay(df)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nDate\n\n\n\n\n0\n1\n2\nx\n4\nNaN\n\n01/01/2023\n\n\n1\n1\n3\nx\n5\nNaN\n\n02/01/2023\n\n\n2\n1\n2\nx\n6\nNaN\n\n03/01/2023"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas-1",
    "href": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas-1",
    "title": "SODA Kickoff 2025",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\n# Parse the datetime strings with a specific format\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n\n# Drop columns with only a single unique value\ndf_cleaned = df.loc[:, df.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_cleaned = drop_empty_columns(df_cleaned)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#lightweight-data-cleaning-cleaner",
    "href": "pages/slides/soda-kickoff/slides.html#lightweight-data-cleaning-cleaner",
    "title": "SODA Kickoff 2025",
    "section": "Lightweight data cleaning: Cleaner",
    "text": "Lightweight data cleaning: Cleaner\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas",
    "title": "SODA Kickoff 2025",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf = pd.DataFrame(data)\ndf_expanded = df.copy()\ndatetime_column = \"date\"\ndf_expanded[datetime_column] = pd.to_datetime(df_expanded[datetime_column], errors='coerce')\n\ndf_expanded['year'] = df_expanded[datetime_column].dt.year\ndf_expanded['month'] = df_expanded[datetime_column].dt.month\ndf_expanded['day'] = df_expanded[datetime_column].dt.day\ndf_expanded['hour'] = df_expanded[datetime_column].dt.hour\ndf_expanded['minute'] = df_expanded[datetime_column].dt.minute\ndf_expanded['second'] = df_expanded[datetime_column].dt.second"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas-1",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas-1",
    "title": "SODA Kickoff 2025",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\ndf_expanded['hour_sin'] = np.sin(2 * np.pi * df_expanded['hour'] / 24)\ndf_expanded['hour_cos'] = np.cos(2 * np.pi * df_expanded['hour'] / 24)\n\ndf_expanded['month_sin'] = np.sin(2 * np.pi * df_expanded['month'] / 12)\ndf_expanded['month_cos'] = np.cos(2 * np.pi * df_expanded['month'] / 12)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nDataFrame with expanded datetime columns:\")\nprint(df_expanded)\n\nOriginal DataFrame:\n                  date  value\n0  2023-01-01 12:34:56     10\n1  2023-02-15 08:45:23     20\n2  2023-03-20 18:12:45     30\n\nDataFrame with expanded datetime columns:\n                 date  value  year  month  day  hour  minute  second  \\\n0 2023-01-01 12:34:56     10  2023      1    1    12      34      56   \n1 2023-02-15 08:45:23     20  2023      2   15     8      45      23   \n2 2023-03-20 18:12:45     30  2023      3   20    18      12      45   \n\n       hour_sin      hour_cos  month_sin     month_cos  \n0  1.224647e-16 -1.000000e+00   0.500000  8.660254e-01  \n1  8.660254e-01 -5.000000e-01   0.866025  5.000000e-01  \n2 -1.000000e+00 -1.836970e-16   1.000000  6.123234e-17"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "title": "SODA Kickoff 2025",
    "section": "Encoding datetime features skrub.DatetimeEncoder",
    "text": "Encoding datetime features skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\nX_date = ToDatetime().fit_transform(df[\"date\"])\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\n   date_year  date_total_seconds  date_month_circular_0  \\\n0     2023.0        1.672577e+09               0.500000   \n1     2023.0        1.676451e+09               0.866025   \n2     2023.0        1.679336e+09               1.000000   \n\n   date_month_circular_1  date_day_circular_0  date_day_circular_1  \\\n0           8.660254e-01         2.079117e-01             0.978148   \n1           5.000000e-01         1.224647e-16            -1.000000   \n2           6.123234e-17        -8.660254e-01            -0.500000   \n\n   date_hour_circular_0  date_hour_circular_1  \n0          1.224647e-16         -1.000000e+00  \n1          8.660254e-01         -5.000000e-01  \n2         -1.000000e+00         -1.836970e-16  \n\n\n}"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "SODA Kickoff 2025",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline",
    "title": "SODA Kickoff 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-1",
    "title": "SODA Kickoff 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-2",
    "title": "SODA Kickoff 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "title": "SODA Kickoff 2025",
    "section": "Build a predictive pipeline with tabular_learner",
    "text": "Build a predictive pipeline with tabular_learner\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_learner(Ridge())\nmodel\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nstepsÂ \n[('tablevectorizer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_inputÂ \nNone\n\n\n\nmemoryÂ \nNone\n\n\n\nverboseÂ \nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_thresholdÂ \n40\n\n\n\nlow_cardinalityÂ \nOneHotEncoder..._output=False)\n\n\n\nhigh_cardinalityÂ \nStringEncoder()\n\n\n\nnumericÂ \nPassThrough()\n\n\n\ndatetimeÂ \nDatetimeEncod...ding='spline')\n\n\n\nspecific_transformersÂ \n()\n\n\n\ndrop_null_fractionÂ \n1.0\n\n\n\ndrop_if_constantÂ \nFalse\n\n\n\ndrop_if_uniqueÂ \nFalse\n\n\n\ndatetime_formatÂ \nNone\n\n\n\nn_jobsÂ \nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolutionÂ \n'hour'\n\n\n\nadd_weekdayÂ \nFalse\n\n\n\nadd_total_secondsÂ \nTrue\n\n\n\nadd_day_of_yearÂ \nFalse\n\n\n\nperiodic_encodingÂ \n'spline'\n\n\n\n\n            \n        \n    low_cardinalityOneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategoriesÂ \n'auto'\n\n\n\ndropÂ \n'if_binary'\n\n\n\nsparse_outputÂ \nFalse\n\n\n\ndtypeÂ \n'float32'\n\n\n\nhandle_unknownÂ \n'ignore'\n\n\n\nmin_frequencyÂ \nNone\n\n\n\nmax_categoriesÂ \nNone\n\n\n\nfeature_name_combinerÂ \n'concat'\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_componentsÂ \n30\n\n\n\nvectorizerÂ \n'tfidf'\n\n\n\nngram_rangeÂ \n(3, ...)\n\n\n\nanalyzerÂ \n'char_wb'\n\n\n\nstop_wordsÂ \nNone\n\n\n\nrandom_stateÂ \nNone\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_valuesÂ \nnan\n\n\n\nstrategyÂ \n'mean'\n\n\n\nfill_valueÂ \nNone\n\n\n\ncopyÂ \nTrue\n\n\n\nadd_indicatorÂ \nTrue\n\n\n\nkeep_empty_featuresÂ \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopyÂ \nTrue\n\n\n\nwith_meanÂ \nTrue\n\n\n\nwith_stdÂ \nTrue\n\n\n\n\n            \n        \n    Ridge?Documentation for Ridge\n        \n            \n                Parameters\n                \n\n\n\n\nalphaÂ \n1.0\n\n\n\nfit_interceptÂ \nTrue\n\n\n\ncopy_XÂ \nTrue\n\n\n\nmax_iterÂ \nNone\n\n\n\ntolÂ \n0.0001\n\n\n\nsolverÂ \n'auto'\n\n\n\npositiveÂ \nFalse\n\n\n\nrandom_stateÂ \nNone"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#we-now-have-a-pipeline",
    "title": "SODA Kickoff 2025",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\n\nskrub.datasets, or user data\n\nExplore the data\n\nskrub.TableReport\n\nPre-process the data\n\nskrub.TableVectorizer, Cleaner, DatetimeEncoder â€¦\n\nPerform feature engineering\n\nskrub.TableVectorizer, TextEncoder, StringEncoderâ€¦\n\nBuild a scikit-learn pipeline\n\ntabular_learner, sklearn.pipeline.make_pipeline â€¦\n\n???\nProfit ðŸ“ˆ"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#skrub-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#skrub-expressions",
    "title": "SODA Kickoff 2025",
    "section": "skrub expressions",
    "text": "skrub expressions\nWhen a normal pipe is not enoughâ€¦\n\nExpressions come to the rescue ðŸš’:\n\n\nKeep track of train, validation and test splits to avoid data leakage\nSimplify hyperparameter tuning and reporting\nHandle complex pipelines that involve multiple tables and custom\nPersist all objects for reproducibility"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#starting-with-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#starting-with-expressions",
    "title": "SODA Kickoff 2025",
    "section": "Starting with expressions",
    "text": "Starting with expressions\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = skrub.X(data.baskets[[\"ID\"]]) # mark as \"X\"\ny = skrub.y(data.baskets[\"fraud_flag\"]) # mark as \"y\"\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX, y, products represent inputs to the pipeline\nskrub keeps track of splits"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-and-inspect-complex-pipelines",
    "href": "pages/slides/soda-kickoff/slides.html#build-and-inspect-complex-pipelines",
    "title": "SODA Kickoff 2025",
    "section": "Build and inspect complex pipelines",
    "text": "Build and inspect complex pipelines\npred.skb.full_report()\n\nreport"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-in-scikit-learn",
    "href": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-in-scikit-learn",
    "title": "SODA Kickoff 2025",
    "section": "Hyperparameter tuning in scikit-learn",
    "text": "Hyperparameter tuning in scikit-learn\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-with-skrub-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-with-skrub-expressions",
    "title": "SODA Kickoff 2025",
    "section": "Hyperparameter tuning with skrub expressions",
    "text": "Hyperparameter tuning with skrub expressions\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)\nregressor.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#observe-the-impact-of-the-hyperparameters",
    "href": "pages/slides/soda-kickoff/slides.html#observe-the-impact-of-the-hyperparameters",
    "title": "SODA Kickoff 2025",
    "section": "Observe the impact of the hyperparameters",
    "text": "Observe the impact of the hyperparameters\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#tldw",
    "href": "pages/slides/soda-kickoff/slides.html#tldw",
    "title": "SODA Kickoff 2025",
    "section": "tl;dw",
    "text": "tl;dw\nskrub provides\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering\nsoonâ„¢ï¸, complex pipelines, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#getting-involved",
    "href": "pages/slides/soda-kickoff/slides.html#getting-involved",
    "title": "SODA Kickoff 2025",
    "section": "Getting involved",
    "text": "Getting involved\n\nSkrub website (QR code below!)\nSkrub materials website\nGit repository\nDiscord server\nBluesky"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#plan-for-the-presentation",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#plan-for-the-presentation",
    "title": "A Skrub use case in academia",
    "section": "Plan for the presentation",
    "text": "Plan for the presentation\n\nContext and explanation of the problem\nThe Retrieve, Merge, Predict pipeline\nHow is this relevant?"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-1",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-2",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-2",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-3",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-3",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-definitions",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-definitions",
    "title": "A Skrub use case in academia",
    "section": "Some definitions",
    "text": "Some definitions\n\nBase table: the table we want to augment (â€œMoviesâ€, â€œHousingâ€â€¦)\nQuery column: a column that should be used as join key (â€œMovie titleâ€, â€œAddressâ€â€¦)\nData lake: an unstructured repository of many (thousands ofâ€¦) â€œcandidate tablesâ€\nCandidate table: a table that may be useful for augmenting the base table (â€œMovie directorsâ€â€¦)\nCandidate column: a column in a candidate table that could be joined on the query column (â€œTitleâ€ in a table about filmographies)\nAugmented table: the result of joining the base table and a candidate table\n\n\n\n\n\n\n\n\nWarning\n\n\nThis terminology is slightly different from that used in the Skrub documentation"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#jaccard-containment",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#jaccard-containment",
    "title": "A Skrub use case in academia",
    "section": "Jaccard Containment",
    "text": "Jaccard Containment\n\n\nJaccard Similarity: \\(\\frac{|Q \\cap X|}{|Q \\cup X|}\\)\nJaccard Containment: \\(\\frac{|Q \\cap X|}{|Q|}\\)\n\n\nJaccard containment is a â€œnormalizedâ€ intersection:\n\n\n\n\n\n\nImportant\n\n\nWhat fraction of of query set Q is in candidate column X?\n\n\n\n\n\n\n\n\nhttps://ekzhu.com/datasketch/lshensemble.html"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#the-focus-of-the-study",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#the-focus-of-the-study",
    "title": "A Skrub use case in academia",
    "section": "The focus of the study:",
    "text": "The focus of the study:\n\nFind the best way to discover candidates.\nWork within a defined computational budget\nWork with exact joins between a base table and multiple join candidates.\nGuarantee that results are reproducible."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#we-do-not-consider",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#we-do-not-consider",
    "title": "A Skrub use case in academia",
    "section": "We do not consider:",
    "text": "We do not consider:\n\nEntity matching or fuzzy joins (e.g., matching â€œNYTâ€ and â€œThe New York Timesâ€).\nDiscovering the query column.\nMulti-key joins"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#working-with-data-lakes-is-hard",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#working-with-data-lakes-is-hard",
    "title": "A Skrub use case in academia",
    "section": "Working with data lakes is hard",
    "text": "Working with data lakes is hard\n\nSome CSVs donâ€™t use commas\nSome CSVs have no (known) schema\nSome CSVs arenâ€™t CSVs, theyâ€™re actually JSON files in disguise\nSome JSONs arenâ€™t JSONs, theyâ€™re actually strings in disguise\n\n\nIf you donâ€™t know the tables, everyone is sus"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#pipeline-schema",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#pipeline-schema",
    "title": "A Skrub use case in academia",
    "section": "Pipeline schema",
    "text": "Pipeline schema"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-retrieval",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-retrieval",
    "title": "A Skrub use case in academia",
    "section": "Candidate retrieval",
    "text": "Candidate retrieval\n\nExact Matching: measure the exact Jaccard containment (JC) for each column in the data lake.\nMinHash: estimate the Jaccard containment, query to get columns with a JC larger than a threshold.\nHybrid MinHash: query with MinHash, then measure the exact JC for the retrieved candidates.\nStarmie : use a language model to query candidate columns."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-selection",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-selection",
    "title": "A Skrub use case in academia",
    "section": "Candidate selection",
    "text": "Candidate selection\n\nHighest Containment Join: Rank candidates by Jaccard Containment.\nFull Join: Join all candidates.\nBest Single Join: Train a model on each candidate, select the best.\nStepwise Greedy Join: Like Best Single Join, but keep all good candidates."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation",
    "title": "A Skrub use case in academia",
    "section": "Aggregation",
    "text": "Aggregation"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation-1",
    "title": "A Skrub use case in academia",
    "section": "Aggregation",
    "text": "Aggregation\n\nAny: take one value at random from each group\nMean: for each group, take the mean of numerical values and mode of categorical values\nDeep Feature Synthesis (DFS): greedily generate new features (count, mean, sumâ€¦) to already present features."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#prediction",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#prediction",
    "title": "A Skrub use case in academia",
    "section": "Prediction",
    "text": "Prediction\n\nRidgeCV: linear baseline ðŸ“ˆ\nCatBoost: GDBT ðŸŒ²\nResNet: Neural Networks ðŸ§ \nRealMLP: Neural Networks ðŸ§ "
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-experimental-results",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-experimental-results",
    "title": "A Skrub use case in academia",
    "section": "Some experimental results",
    "text": "Some experimental results"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#total-compute-time",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#total-compute-time",
    "title": "A Skrub use case in academia",
    "section": "Total compute time",
    "text": "Total compute time\n\n\n\nML Model\nPlatform\nTotal compute time\n\n\n\n\nRidgeCV\nCPU\n4y 3m 10d 7h\n\n\nCatBoost\nCPU\n1y 3m 29d 21h\n\n\nResNet\nGPU\n5y 6m 23d 0h\n\n\nRealMLP\nGPU\n10y 7m 23d 3h\n\n\nTotal\nBoth\n21y 9m 26d 8h"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#features-of-research-code",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#features-of-research-code",
    "title": "A Skrub use case in academia",
    "section": "â€œFeaturesâ€ of research code",
    "text": "â€œFeaturesâ€ of research code\nResearch codeâ€¦\n\nIs mostly custom-made for a specific experiment\nFeatures little to no testing\nOften is poorly documented, or not at all\nInvolves a lot of technical debt"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrub-to-the-rescue",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrub-to-the-rescue",
    "title": "A Skrub use case in academia",
    "section": "Skrub to the rescue",
    "text": "Skrub to the rescue\n\nWell tested code\nGood documentation\nFeatures cover much of the pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#using-skrub-features-in-the-pipeline",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#using-skrub-features-in-the-pipeline",
    "title": "A Skrub use case in academia",
    "section": "Using Skrub features in the pipeline",
    "text": "Using Skrub features in the pipeline\n\nThe Discover object can replace (part of) the retrieval step.\nAll the code for joining can be replaced by the AggJoiner or MultiAggJoiner.\nThe MultiAggJoiner is an additional baseline.\nThe TableVectorizer can handle automated preprocessing of the tables.\nJoined candidates can be examined quickly using the TableReport."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline",
    "title": "A Skrub use case in academia",
    "section": "â€œSkrubifiedâ€ pipeline",
    "text": "â€œSkrubifiedâ€ pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline-1",
    "title": "A Skrub use case in academia",
    "section": "â€œSkrubifiedâ€ pipeline",
    "text": "â€œSkrubifiedâ€ pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-with-multiaggjoiner",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-with-multiaggjoiner",
    "title": "A Skrub use case in academia",
    "section": "Example with MultiAggJoiner",
    "text": "Example with MultiAggJoiner\n\n\nmerged = source_table.clone()\nhashes = []\nfor hash_, mdata in tqdm(\n    index_cand.items(),\n    total=len(index_cand),\n    leave=False,\n    desc=\"Full Join\",\n    position=2,\n):\n    cnd_md = mdata.candidate_metadata\n    hashes.append(cnd_md[\"hash\"])\n    candidate_table = pl.read_parquet(cnd_md[\"full_path\"])\n\n    left_on = mdata.left_on\n    right_on = mdata.right_on\n\n    aggr_right = aggregate_table(\n        candidate_table, right_on, aggregation_method=aggregation\n    )\n\n    merged = execute_join(\n        merged,\n        aggr_right,\n        left_on=left_on,\n        right_on=right_on,\n        how=\"left\",\n        suffix=\"_\" + hash_[:10],\n    )\n\n# MOCK-UP\nfrom skrub import MultiAggJoiner\njoiner = MultiAggJoiner(candidate_tables, keys=candidate_keys)\nmerged = joiner.fit_transform(source_table)"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#repositories",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#repositories",
    "title": "A Skrub use case in academia",
    "section": "Repositories",
    "text": "Repositories\n\nRetrieve, Merge, Predict website\nRetrieve, Merge, Predict repository"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#acknowledgements",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#acknowledgements",
    "title": "A Skrub use case in academia",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\n\n\n\n\n\nAuthors:\n\nRiccardo Cappuzzo (SODA, Dataiku)\nAimee Coelho (Dataiku)\nFelix Lefebvre (SODA)\nPaolo Papotti (Eurecom)\nGael Varoquaux (SODA)"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#conclusions-and-summary",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#conclusions-and-summary",
    "title": "A Skrub use case in academia",
    "section": "Conclusions and summary",
    "text": "Conclusions and summary\n\n\n\n\n\nTree-based models are more effective and more resilient than the alternatives\nGood table retrieval affects the whole pipeline\nSimple methods produce results comparable or even better than more complex methods\n\n\n\n\nSkrub provides well-tested, well-documented code\nSkrub objects provide features that cover most of the pipeline\nIn return, the pipeline helped deciding on relevant features."
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#skrub-compatibility",
    "href": "pages/slides/p16-day-2025/slides.html#skrub-compatibility",
    "title": "P16 Day 2025",
    "section": "Skrub compatibility",
    "text": "Skrub compatibility\n\nSkrub is fully compatible with pandas and polars\nSkrub transformers are fully compatible with scikit-learn"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#an-example-pipeline",
    "href": "pages/slides/p16-day-2025/slides.html#an-example-pipeline",
    "title": "P16 Day 2025",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPreprocess the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/p16-day-2025/slides.html#exploring-the-data-with-skrub",
    "title": "P16 Day 2025",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nTableReport Preview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\n\nMore examples"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#data-cleaning-with-pandaspolars-setup",
    "href": "pages/slides/p16-day-2025/slides.html#data-cleaning-with-pandaspolars-setup",
    "title": "P16 Day 2025",
    "section": "Data cleaning with pandas/polars: setup",
    "text": "Data cleaning with pandas/polars: setup\n\nPandasPolars\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pd = pd.DataFrame(data)\ndisplay(df_pd)\n\n\n\n\n\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\n\n\n0\n2\nx\nfoo\nNaN\n\n01 Jan 2023\n\n\n1\n3\nx\nbar\nNaN\n\n02 Jan 2023\n\n\n2\n2\nx\nbaz\nNaN\n\n03 Jan 2023\n\n\n\n\n\n\n\n\n\n\nimport polars as pl\nimport numpy as np\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pl = pl.DataFrame(data)\ndisplay(df_pl)\n\n\nshape: (3, 6)\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\ni64\nstr\nstr\nf64\nstr\nstr\n\n\n\n\n2\n\"x\"\n\"foo\"\nNaN\n\"\"\n\"01 Jan 2023\"\n\n\n3\n\"x\"\n\"bar\"\nNaN\n\"\"\n\"02 Jan 2023\"\n\n\n2\n\"x\"\n\"baz\"\nNaN\n\"\"\n\"03 Jan 2023\""
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "href": "pages/slides/p16-day-2025/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "title": "P16 Day 2025",
    "section": "Nulls, datetimes, constant columns with pandas/polars",
    "text": "Nulls, datetimes, constant columns with pandas/polars\n\nPandasPolars\n\n\n\n# Parse the datetime strings with a specific format\ndf_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d %b %Y')\n\n# Drop columns with only a single unique value\ndf_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_pd_cleaned = drop_empty_columns(df_pd_cleaned)\n\n\n\n\n# Parse the datetime strings with a specific format\ndf_pl = df_pl.with_columns([\n    pl.col(\"Date\").str.strptime(pl.Date, \"%d %b %Y\", strict=False).alias(\"Date\")\n])\n\n# Drop columns with only a single unique value\ndf_pl_cleaned = df_pl.select([\n    col for col in df_pl.columns if df_pl[col].n_unique() &gt; 1\n])\n\n# Import selectors for dtype selection\nimport polars.selectors as cs\n\n# Drop columns with only missing values or only empty strings\ndef drop_empty_columns(df):\n    all_nan = df.select(\n        [\n            col for col in df.select(cs.numeric()).columns if \n            df [col].is_nan().all()\n        ]\n    ).columns\n    \n    all_empty = df.select(\n        [\n            col for col in df.select(cs.string()).columns if \n            (df[col].str.strip_chars().str.len_chars()==0).all()\n        ]\n    ).columns\n\n    to_drop = all_nan + all_empty\n\n    return df.drop(to_drop)\n\ndf_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#data-cleaning-with-skrub.cleaner",
    "href": "pages/slides/p16-day-2025/slides.html#data-cleaning-with-skrub.cleaner",
    "title": "P16 Day 2025",
    "section": "Data cleaning with skrub.Cleaner",
    "text": "Data cleaning with skrub.Cleaner\n\nPandasPolars\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pd)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nInt\nStr\nDate\n\n\n\n\n0\n2\nfoo\n2023-01-01\n\n\n1\n3\nbar\n2023-01-02\n\n\n2\n2\nbaz\n2023-01-03\n\n\n\n\n\n\n\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pl)\ndisplay(df_cleaned)\n\n\nshape: (3, 3)\n\n\n\nInt\nStr\nDate\n\n\ni64\nstr\ndate\n\n\n\n\n2\n\"foo\"\n2023-01-01\n\n\n3\n\"bar\"\n2023-01-02\n\n\n2\n\"baz\"\n2023-01-03"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#encoding-datetime-features-with-skrub.datetimeencoder",
    "href": "pages/slides/p16-day-2025/slides.html#encoding-datetime-features-with-skrub.datetimeencoder",
    "title": "P16 Day 2025",
    "section": "Encoding datetime features with skrub.DatetimeEncoder",
    "text": "Encoding datetime features with skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nX_date = ToDatetime().fit_transform(df[\"date\"])\nde = DatetimeEncoder(resolution=\"second\")\n# de = DatetimeEncoder(periodic_encoding=\"spline\")\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\nshape: (3, 7)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ date_year â”† date_month â”† date_day â”† date_hour â”† date_minute â”† date_second â”† date_total_seconds â”‚\nâ”‚ ---       â”† ---        â”† ---      â”† ---       â”† ---         â”† ---         â”† ---                â”‚\nâ”‚ f32       â”† f32        â”† f32      â”† f32       â”† f32         â”† f32         â”† f32                â”‚\nâ•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 2023.0    â”† 1.0        â”† 1.0      â”† 12.0      â”† 34.0        â”† 56.0        â”† 1.6726e9           â”‚\nâ”‚ 2023.0    â”† 2.0        â”† 15.0     â”† 8.0       â”† 45.0        â”† 23.0        â”† 1.6765e9           â”‚\nâ”‚ 2023.0    â”† 3.0        â”† 20.0     â”† 18.0      â”† 12.0        â”† 45.0        â”† 1.6793e9           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "href": "pages/slides/p16-day-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "title": "P16 Day 2025",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler\n\n\n\n\n\n\n\n\n\n\nSkrub wants to solve ML problems based partly on solid engineering and partly on statistical notions. The SquashingScaler is based on the second part, and is taken from a recent paper that evaluates different techniques for improving the performance of NNs."
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "href": "pages/slides/p16-day-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "title": "P16 Day 2025",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#encoding-categorical-stringtext-features",
    "href": "pages/slides/p16-day-2025/slides.html#encoding-categorical-stringtext-features",
    "title": "P16 Day 2025",
    "section": "Encoding categorical (string/text) features",
    "text": "Encoding categorical (string/text) features\nCategorical features have a â€œcardinalityâ€: the number of unique values\n\nLow cardinality: OneHotEncoder\nHigh cardinality (&gt;40 unique values): skrub.StringEncoder\nText: skrub.TextEncoder and pretrained models from HuggingFace Hub"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/p16-day-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "P16 Day 2025",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer\n\nfrom skrub import TableVectorizer\n\ntable_vec = TableVectorizer()\ndf_encoded = table_vec.fit_transform(df)\n\n\n\nApply the Cleaner to all columns\nSplit columns by dtype and # of unique values\nEncode each column separately"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#encoding-all-the-features-tablevectorizer-1",
    "href": "pages/slides/p16-day-2025/slides.html#encoding-all-the-features-tablevectorizer-1",
    "title": "P16 Day 2025",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/p16-day-2025/slides.html#build-a-predictive-pipeline",
    "title": "P16 Day 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/p16-day-2025/slides.html#build-a-predictive-pipeline-1",
    "title": "P16 Day 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "href": "pages/slides/p16-day-2025/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "title": "P16 Day 2025",
    "section": "Build a predictive pipeline with tabular_pipeline",
    "text": "Build a predictive pipeline with tabular_pipeline\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_pipeline(Ridge())"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/p16-day-2025/slides.html#we-now-have-a-pipeline",
    "title": "P16 Day 2025",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\nExplore the data\n\nTableReport\n\nPre-process the data\n\nCleaner, ToDatetime â€¦\n\nPerform feature engineering\n\nTableVectorizer, SquashingScaler, TextEncoder, StringEncoderâ€¦\n\nBuild a scikit-learn pipeline\n\ntabular_pipeline\n\n???\nProfit ðŸ“ˆ"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#what-if",
    "href": "pages/slides/p16-day-2025/slides.html#what-if",
    "title": "P16 Day 2025",
    "section": "What ifâ€¦",
    "text": "What ifâ€¦\n\nYour data is spread over multiple tables?\nYou want to avoid data leakage?\nYou want to tune more than just the hyperparameters of your model?\nYou want to guarantee that your pipeline is replayed exactly on new data?\n\n\nWhen a normal pipeline is not enoughâ€¦\n\n\nâ€¦ the skrub DataOps come to the rescue ðŸš’"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#dataops",
    "href": "pages/slides/p16-day-2025/slides.html#dataops",
    "title": "P16 Day 2025",
    "section": "DataOpsâ€¦",
    "text": "DataOpsâ€¦\n\nExtend the scikit-learn machinery to complex multi-table operations, and take care of data leakage\nTrack all operations with a computational graph (a Data Ops plan)\nAre transparent and give direct access to the underlying object\nAllow tuning any operation in the Data Ops plan\nGuarantee that all operations are reproducible\nCan be persisted and shared easily"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/p16-day-2025/slides.html#how-do-dataops-work-though",
    "title": "P16 Day 2025",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#starting-with-the-dataops",
    "href": "pages/slides/p16-day-2025/slides.html#starting-with-the-dataops",
    "title": "P16 Day 2025",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\nimport skrub\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\n\nbaskets and products represent inputs to the pipeline.\nSkrub tracks X and y so that training and test splits are never mixed."
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#applying-a-transformer",
    "href": "pages/slides/p16-day-2025/slides.html#applying-a-transformer",
    "title": "P16 Day 2025",
    "section": "Applying a transformer",
    "text": "Applying a transformer\n\nfrom skrub import selectors as s\n\nvectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder()\n)\nvectorized_products = products.skb.apply(\n    vectorizer, cols=s.all() - \"basket_ID\"\n)"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#executing-dataframe-operations",
    "href": "pages/slides/p16-day-2025/slides.html#executing-dataframe-operations",
    "title": "P16 Day 2025",
    "section": "Executing dataframe operations",
    "text": "Executing dataframe operations\n\naggregated_products = vectorized_products.groupby(\n    \"basket_ID\"\n).agg(\"mean\").reset_index()\n\nfeatures = X.merge(\n    aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n)\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#applying-a-ml-model",
    "href": "pages/slides/p16-day-2025/slides.html#applying-a-ml-model",
    "title": "P16 Day 2025",
    "section": "Applying a ML model",
    "text": "Applying a ML model\n\nfrom sklearn.ensemble import ExtraTreesClassifier  \npredictions = features.skb.apply(\n    ExtraTreesClassifier(n_jobs=-1), y=y\n)"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#inspecting-the-data-ops-plan",
    "href": "pages/slides/p16-day-2025/slides.html#inspecting-the-data-ops-plan",
    "title": "P16 Day 2025",
    "section": "Inspecting the Data Ops plan",
    "text": "Inspecting the Data Ops plan\npredictions.skb.full_report()\n\nExecution report\nEach node:\n\nShows a preview of the data resulting from the operation\nReports the location in the code where the code is defined\nShows the run time of the node"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/p16-day-2025/slides.html#exporting-the-plan-in-a-learner",
    "title": "P16 Day 2025",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe Learner is a stand-alone object that works like a scikit-learn estimator that takes a dictionary as input rather than just X and y.\n\n\nlearner = predictions.skb.make_learner(fitted=True)\n\n\n\nThen, the learner can be pickled â€¦\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\nâ€¦ loaded and applied to new data:\nwith open(\"learner.bin\", \"rb\") as fp:\n    loaded_learner = pickle.load(fp)\ndata = skrub.datasets.fetch_credit_fraud(split=\"test\")\nnew_baskets = data.baskets\nnew_products = data.products\nloaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})\n\n\narray([0, 0, 0, ..., 0, 0, 0], shape=(31549,))"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#hyperparameter-tuning-in-a-data-ops-plan",
    "href": "pages/slides/p16-day-2025/slides.html#hyperparameter-tuning-in-a-data-ops-plan",
    "title": "P16 Day 2025",
    "section": "Hyperparameter tuning in a Data Ops plan",
    "text": "Hyperparameter tuning in a Data Ops plan\nSkrub implements four choose_* functions:\n\nchoose_from: select from the given list of options\nchoose_int: select an integer within a range\nchoose_float: select a float within a range\nchoose_bool: select a bool\noptional: chooses whether to execute the given operation"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/p16-day-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "P16 Day 2025",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestRegressor()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestRegressor()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#tuning-with-data-ops-is-simple",
    "href": "pages/slides/p16-day-2025/slides.html#tuning-with-data-ops-is-simple",
    "title": "P16 Day 2025",
    "section": "Tuning with Data Ops is simple!",
    "text": "Tuning with Data Ops is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestRegressor(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#run-hyperparameter-search",
    "href": "pages/slides/p16-day-2025/slides.html#run-hyperparameter-search",
    "title": "P16 Day 2025",
    "section": "Run hyperparameter search",
    "text": "Run hyperparameter search\n# fit the search \nsearch = regressor.skb.make_randomized_search(\n    scoring=\"roc_auc\", fitted=True, cv=5\n)\n\n# save the best learner\nbest_learner = search.best_learner_"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#a-parallel-coordinate-plot-to-explore-hyperparameters",
    "href": "pages/slides/p16-day-2025/slides.html#a-parallel-coordinate-plot-to-explore-hyperparameters",
    "title": "P16 Day 2025",
    "section": "A parallel coordinate plot to explore hyperparameters",
    "text": "A parallel coordinate plot to explore hyperparameters\nsearch = pred.skb.get_randomized_search(fitted=True)\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#more-information-about-the-data-ops",
    "href": "pages/slides/p16-day-2025/slides.html#more-information-about-the-data-ops",
    "title": "P16 Day 2025",
    "section": "More information about the Data Ops",
    "text": "More information about the Data Ops\n\nSkrub example gallery\nSkrub user guide\nTutorial on timeseries forecasting at Euroscipy 2025\nKaggle notebook on the Titanic survival challenge"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#getting-involved",
    "href": "pages/slides/p16-day-2025/slides.html#getting-involved",
    "title": "P16 Day 2025",
    "section": "Getting involved",
    "text": "Getting involved\nDo you want to learn more?\n\nSkrub website\nSkrub materials website\nDiscord server\n\nFollow skrub on:\n\nBluesky\nLinkedIn\n\nStar skrub on GitHub, or contribute directly:\n\nGitHub repository"
  },
  {
    "objectID": "pages/slides/p16-day-2025/slides.html#tldw-skrub",
    "href": "pages/slides/p16-day-2025/slides.html#tldw-skrub",
    "title": "P16 Day 2025",
    "section": "tl;dw: skrub",
    "text": "tl;dw: skrub\n\ninteractive data exploration: TableReport\nautomated pre-processing of pandas and polars dataframes: Cleaner\npowerful feature engineering: TableVectorizer, tabular_pipeline\ncolumn- and dataframe-level operations: ApplyToCols, selectors\nDataOps, plans, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#whoami",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#whoami",
    "title": "Women in Machine Learning & Data Science",
    "section": "whoami",
    "text": "whoami\n\nI am a research engineer at Inria as part of the P16 project, and I am the lead developer of skrub \nIâ€™m Italian, but I donâ€™t drink coffee, wine, and I like pizza with fries \nI did my PhD in CÃ´te dâ€™Azur, but I moved to Paris because it was too sunny and I donâ€™t like the sea"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#who-are-you",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#who-are-you",
    "title": "Women in Machine Learning & Data Science",
    "section": "Who are you?",
    "text": "Who are you?\n\nWho is familiar with Pandas or Polars?\nWho has worked with scikit-learn?\nWho has already made contributions in open source?\nWho has heard of skrub before today?"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#roadmap-for-the-presentation",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#roadmap-for-the-presentation",
    "title": "Women in Machine Learning & Data Science",
    "section": "Roadmap for the presentation",
    "text": "Roadmap for the presentation\n\nWhat is skrub\nContributing to skrub: subjects\nContributing to skrub: setting up the environment\n\nQR code for the presentation at the end!"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#skrub-compatibility",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#skrub-compatibility",
    "title": "Women in Machine Learning & Data Science",
    "section": "Skrub compatibility",
    "text": "Skrub compatibility\n\nSkrub is mostly written in Python, but it inlcudes some Javascript\nSkrub is fully compatible with pandas and polars\n\nAny feature needs to be supported by both libraries\n\nSkrub transformers are fully compatible with scikit-learn\n\nTransformers need to satisfy some requirements"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#first-an-example-pipeline",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#first-an-example-pipeline",
    "title": "Women in Machine Learning & Data Science",
    "section": "First, an example pipeline",
    "text": "First, an example pipeline\n\nGather some data\nExplore the data\nPreprocess the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#skrub.tablereport",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#skrub.tablereport",
    "title": "Women in Machine Learning & Data Science",
    "section": "skrub.TableReport",
    "text": "skrub.TableReport\nfrom skrub import TableReport\nTableReport(employee_salaries)\nTableReport Preview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#skrub.tablereport-1",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#skrub.tablereport-1",
    "title": "Women in Machine Learning & Data Science",
    "section": "skrub.TableReport",
    "text": "skrub.TableReport\n\nThe report uses uses Jinjia templates and Javascript for interactivity\nThe backend is in Python\nSpace is limited, need to maximize information density.\nLight &gt; feature-rich (no plotly)"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#data-cleaning-with-pandaspolars-setup",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#data-cleaning-with-pandaspolars-setup",
    "title": "Women in Machine Learning & Data Science",
    "section": "Data cleaning with pandas/polars: setup",
    "text": "Data cleaning with pandas/polars: setup\n\nPandasPolars\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pd = pd.DataFrame(data)\ndisplay(df_pd)\n\n\n\n\n\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\n\n\n0\n2\nx\nfoo\nNaN\n\n01 Jan 2023\n\n\n1\n3\nx\nbar\nNaN\n\n02 Jan 2023\n\n\n2\n2\nx\nbaz\nNaN\n\n03 Jan 2023\n\n\n\n\n\n\n\n\n\n\nimport polars as pl\nimport numpy as np\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pl = pl.DataFrame(data)\ndisplay(df_pl)\n\n\nshape: (3, 6)\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\ni64\nstr\nstr\nf64\nstr\nstr\n\n\n\n\n2\n\"x\"\n\"foo\"\nNaN\n\"\"\n\"01 Jan 2023\"\n\n\n3\n\"x\"\n\"bar\"\nNaN\n\"\"\n\"02 Jan 2023\"\n\n\n2\n\"x\"\n\"baz\"\nNaN\n\"\"\n\"03 Jan 2023\""
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "title": "Women in Machine Learning & Data Science",
    "section": "Nulls, datetimes, constant columns with pandas/polars",
    "text": "Nulls, datetimes, constant columns with pandas/polars\n\nPandasPolars\n\n\n\n# Parse the datetime strings with a specific format\ndf_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d %b %Y')\n\n# Drop columns with only a single unique value\ndf_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_pd_cleaned = drop_empty_columns(df_pd_cleaned)\n\n\n\n\n# Parse the datetime strings with a specific format\ndf_pl = df_pl.with_columns([\n    pl.col(\"Date\").str.strptime(pl.Date, \"%d %b %Y\", strict=False).alias(\"Date\")\n])\n\n# Drop columns with only a single unique value\ndf_pl_cleaned = df_pl.select([\n    col for col in df_pl.columns if df_pl[col].n_unique() &gt; 1\n])\n\n# Import selectors for dtype selection\nimport polars.selectors as cs\n\n# Drop columns with only missing values or only empty strings\ndef drop_empty_columns(df):\n    all_nan = df.select(\n        [\n            col for col in df.select(cs.numeric()).columns if \n            df [col].is_nan().all()\n        ]\n    ).columns\n    \n    all_empty = df.select(\n        [\n            col for col in df.select(cs.string()).columns if \n            (df[col].str.strip_chars().str.len_chars()==0).all()\n        ]\n    ).columns\n\n    to_drop = all_nan + all_empty\n\n    return df.drop(to_drop)\n\ndf_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#skrub.cleaner",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#skrub.cleaner",
    "title": "Women in Machine Learning & Data Science",
    "section": "skrub.Cleaner",
    "text": "skrub.Cleaner\n\nPandasPolars\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pd)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nInt\nStr\nDate\n\n\n\n\n0\n2\nfoo\n2023-01-01\n\n\n1\n3\nbar\n2023-01-02\n\n\n2\n2\nbaz\n2023-01-03\n\n\n\n\n\n\n\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pl)\ndisplay(df_cleaned)\n\n\nshape: (3, 3)\n\n\n\nInt\nStr\nDate\n\n\ni64\nstr\ndate\n\n\n\n\n2\n\"foo\"\n2023-01-01\n\n\n3\n\"bar\"\n2023-01-02\n\n\n2\n\"baz\"\n2023-01-03"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#skrub.cleaner-1",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#skrub.cleaner-1",
    "title": "Women in Machine Learning & Data Science",
    "section": "skrub.Cleaner",
    "text": "skrub.Cleaner\n\nThe actual transformations are performed in part by skrub.DropUninformative.\nNew criteria for selecting columns should go in DropUninformative."
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#skrub.datetimeencoder",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#skrub.datetimeencoder",
    "title": "Women in Machine Learning & Data Science",
    "section": "skrub.DatetimeEncoder",
    "text": "skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nX_date = ToDatetime().fit_transform(df[\"date\"])\nde = DatetimeEncoder(resolution=\"second\")\n# de = DatetimeEncoder(periodic_encoding=\"spline\")\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\nshape: (3, 7)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ date_year â”† date_month â”† date_day â”† date_hour â”† date_minute â”† date_second â”† date_total_seconds â”‚\nâ”‚ ---       â”† ---        â”† ---      â”† ---       â”† ---         â”† ---         â”† ---                â”‚\nâ”‚ f32       â”† f32        â”† f32      â”† f32       â”† f32         â”† f32         â”† f32                â”‚\nâ•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 2023.0    â”† 1.0        â”† 1.0      â”† 12.0      â”† 34.0        â”† 56.0        â”† 1.6726e9           â”‚\nâ”‚ 2023.0    â”† 2.0        â”† 15.0     â”† 8.0       â”† 45.0        â”† 23.0        â”† 1.6765e9           â”‚\nâ”‚ 2023.0    â”† 3.0        â”† 20.0     â”† 18.0      â”† 12.0        â”† 45.0        â”† 1.6793e9           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#encoding-categorical-stringtext-features",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#encoding-categorical-stringtext-features",
    "title": "Women in Machine Learning & Data Science",
    "section": "Encoding categorical (string/text) features",
    "text": "Encoding categorical (string/text) features\nCategorical features have a â€œcardinalityâ€: the number of unique values\n\nLow cardinality: OneHotEncoder\nHigh cardinality (&gt;40 unique values): skrub.StringEncoder\nText: skrub.TextEncoder and pretrained models from HuggingFace Hub"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "Women in Machine Learning & Data Science",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer\n\nfrom skrub import TableVectorizer\n\ntable_vec = TableVectorizer()\ndf_encoded = table_vec.fit_transform(df)\n\n\n\nApply the Cleaner to all columns\nSplit columns by dtype and # of unique values\nEncode each column separately"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#encoding-all-the-features-tablevectorizer-1",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#encoding-all-the-features-tablevectorizer-1",
    "title": "Women in Machine Learning & Data Science",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "title": "Women in Machine Learning & Data Science",
    "section": "Build a predictive pipeline with tabular_pipeline",
    "text": "Build a predictive pipeline with tabular_pipeline\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_pipeline(Ridge())"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#dataops",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#dataops",
    "title": "Women in Machine Learning & Data Science",
    "section": "DataOpsâ€¦",
    "text": "DataOpsâ€¦\n\nExtend the scikit-learn machinery to complex multi-table operations, and take care of data leakage\nTrack all operations with a computational graph (a Data Ops plan)\nAre transparent and give direct access to the underlying object\nAllow tuning any operation in the Data Ops plan\nGuarantee that all operations are reproducible\nCan be persisted and shared easily"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#how-do-dataops-work-though",
    "title": "Women in Machine Learning & Data Science",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#starting-with-the-dataops",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#starting-with-the-dataops",
    "title": "Women in Machine Learning & Data Science",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\nimport skrub\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\n\nbaskets and products represent inputs to the pipeline.\nSkrub tracks X and y so that training and test splits are never mixed."
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#applying-a-transformer",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#applying-a-transformer",
    "title": "Women in Machine Learning & Data Science",
    "section": "Applying a transformer",
    "text": "Applying a transformer\n\nfrom skrub import selectors as s\n\nvectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder()\n)\nvectorized_products = products.skb.apply(\n    vectorizer, cols=s.all() - \"basket_ID\"\n)"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#executing-dataframe-operations",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#executing-dataframe-operations",
    "title": "Women in Machine Learning & Data Science",
    "section": "Executing dataframe operations",
    "text": "Executing dataframe operations\n\naggregated_products = vectorized_products.groupby(\n    \"basket_ID\"\n).agg(\"mean\").reset_index()\n\nfeatures = X.merge(\n    aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n)\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#applying-a-ml-model",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#applying-a-ml-model",
    "title": "Women in Machine Learning & Data Science",
    "section": "Applying a ML model",
    "text": "Applying a ML model\n\nfrom sklearn.ensemble import ExtraTreesClassifier  \npredictions = features.skb.apply(\n    ExtraTreesClassifier(n_jobs=-1), y=y\n)"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#inspecting-the-data-ops-plan",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#inspecting-the-data-ops-plan",
    "title": "Women in Machine Learning & Data Science",
    "section": "Inspecting the Data Ops plan",
    "text": "Inspecting the Data Ops plan\npredictions.skb.full_report()\n\nExecution report\nEach node:\n\nShows a preview of the data resulting from the operation\nReports the location in the code where the code is defined\nShows the run time of the node"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#exporting-the-plan-in-a-learner",
    "title": "Women in Machine Learning & Data Science",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe Learner is a stand-alone object that works like a scikit-learn estimator that takes a dictionary as input rather than just X and y.\n\n\nlearner = predictions.skb.make_learner(fitted=True)\n\n\n\nThen, the learner can be pickled â€¦\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\nâ€¦ loaded and applied to new data:\nwith open(\"learner.bin\", \"rb\") as fp:\n    loaded_learner = pickle.load(fp)\ndata = skrub.datasets.fetch_credit_fraud(split=\"test\")\nnew_baskets = data.baskets\nnew_products = data.products\nloaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})\n\n\narray([0, 0, 0, ..., 0, 0, 0], shape=(31549,))"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#hyperparameter-tuning-in-a-data-ops-plan",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#hyperparameter-tuning-in-a-data-ops-plan",
    "title": "Women in Machine Learning & Data Science",
    "section": "Hyperparameter tuning in a Data Ops plan",
    "text": "Hyperparameter tuning in a Data Ops plan\n\nchoose_from: select from the given list of options\nchoose_int: select an integer within a range\nchoose_float: select a float within a range\nchoose_bool: select a bool\noptional: chooses whether to execute the given operation"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "Women in Machine Learning & Data Science",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestRegressor()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestRegressor()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#tuning-with-data-ops-is-simple",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#tuning-with-data-ops-is-simple",
    "title": "Women in Machine Learning & Data Science",
    "section": "Tuning with Data Ops is simple!",
    "text": "Tuning with Data Ops is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestRegressor(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#exploring-the-hyperparameters",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#exploring-the-hyperparameters",
    "title": "Women in Machine Learning & Data Science",
    "section": "Exploring the hyperparameters",
    "text": "Exploring the hyperparameters\nsearch = pred.skb.get_randomized_search(fitted=True)\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#examples-and-guides",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#examples-and-guides",
    "title": "Women in Machine Learning & Data Science",
    "section": "Examples and guides",
    "text": "Examples and guides\n\nSkrub example gallery\nSkrub user guide\nTutorial on timeseries forecasting at Euroscipy 2025\nKaggle notebook on the Titanic survival challenge"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#getting-involved",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#getting-involved",
    "title": "Women in Machine Learning & Data Science",
    "section": "Getting involved",
    "text": "Getting involved\nDo you want to learn more?\n\nSkrub website\nSkrub materials website\nDiscord server\n\nFollow skrub on:\n\nBluesky\nLinkedIn\n\nStar skrub on GitHub, or contribute directly:\n\nGitHub repository"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#before-you-start-working-on-an-issue",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#before-you-start-working-on-an-issue",
    "title": "Women in Machine Learning & Data Science",
    "section": "Before you start working on an issue",
    "text": "Before you start working on an issue\n\n\n\n\n\n\nImportant\n\n\nWrite a comment on the issue so we know youâ€™re working on it.\n\n\n\nWe want to avoid having multiple people working on the same issue in separate PRs."
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#legend",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#legend",
    "title": "Women in Machine Learning & Data Science",
    "section": "Legend:",
    "text": "Legend:\n\nðŸ˜´ : easy issue\nðŸ : some complexity\nðŸ‘º : hard problem\nðŸƒâ€â™€ï¸ : quick to solve\nðŸ›Œ : likely will take a while\nðŸˆ : docs, need to deal with Sphinx"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#tablereport",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#tablereport",
    "title": "Women in Machine Learning & Data Science",
    "section": "TableReport",
    "text": "TableReport\n\n#1175 - Better control over the TableReportâ€™s progress display. ðŸ‘º ðŸ›Œ\n#1523 - Fix the behavior of the TableReport when max_plot_columns is set to None. ðŸ ðŸƒ\n#1178 - Shuffle the rows of the TableReport example in the home page. ðŸ˜´ ðŸƒ"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#new-transformers",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#new-transformers",
    "title": "Women in Machine Learning & Data Science",
    "section": "New transformers",
    "text": "New transformers\n\n#1001 - Add a DropSimilar transformer. ðŸ‘º ðŸ›Œ\n#710 - Add holidays as features. ðŸ‘º ðŸ›Œ\n#1677 - Make a public ToFloat.ðŸ ðŸ›Œ\n#1542 - Add a transformer that parses string columns that include units (kg, $ etc). ðŸ‘º ðŸ›Œ\n#1430 - Extend ToDatetime so that it can take a list of datetime formats. ðŸ ðŸ›Œ"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#bugfixes-and-maintenance",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#bugfixes-and-maintenance",
    "title": "Women in Machine Learning & Data Science",
    "section": "Bugfixes and maintenance",
    "text": "Bugfixes and maintenance\n\n#1675 - Improve error message when the TableReport receives a lazy Polars dataframe. ðŸ˜´ ðŸƒ\n#1665 - Remove black from the project. ðŸ˜´ ðŸƒ\n#1490 - Cleaner fails when there is an empty polars column name. ðŸ‘º ðŸƒ"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#documentation",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#documentation",
    "title": "Women in Machine Learning & Data Science",
    "section": "Documentation",
    "text": "Documentation\n\n#1476 - DOC: add an example dedicated to showing the features of the TableReport. ðŸ ðŸ›ŒðŸˆ\n#991 - Move the dev docs of the TableReport to the main documentation page. ðŸðŸˆ\n#1582 - Reorganize the â€œDevelopmentâ€ section in the top bar. ðŸ‘º ðŸ›ŒðŸˆ\n#1425 - Shorten the note on the single-column transformer. ðŸ˜´ ðŸƒ\n#1660 - Add different doc versions to the switcher. ðŸðŸˆ ðŸƒ\n#1616 - Change the numbering of examples. ðŸ˜´ðŸˆ ðŸƒ"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#examples",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#examples",
    "title": "Women in Machine Learning & Data Science",
    "section": "Examples",
    "text": "Examples\n\n#1629 - Add an example for the DatetimeEncoder. ðŸ‘º ðŸ›ŒðŸˆ\n#1234 - Shuffle the toxicity dataset in the example. ðŸ˜´ ðŸƒ\nAny example you can come up with!"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#setting-up-the-repository",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#setting-up-the-repository",
    "title": "Women in Machine Learning & Data Science",
    "section": "Setting up the repository",
    "text": "Setting up the repository\nFirst off, you need to fork the skrub repository: https://github.com/skrub-data/skrub/fork\nThen, clone the repo on your local machine\ngit clone https://github.com/&lt;YOUR_USERNAME&gt;/skrub\ncd skrub\nAdd the upstream remote to pull the latest version of skrub:\ngit remote add upstream https://github.com/skrub-data/skrub.git\nYou can check that the remote has been added with git remote -v."
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#setting-up-the-environment",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#setting-up-the-environment",
    "title": "Women in Machine Learning & Data Science",
    "section": "Setting up the environment",
    "text": "Setting up the environment\nDepends on the tools you use!\nFrom inside the skrub directory you just cloned:\n\nvenvuvcondapixi\n\n\n\nCreate the venv (in the current dir):\n\npython -m venv dev-skrub\n\nActivate the venv:\n\nsource dev-skrub/bin/activate\n\nInstall skrub and dependencies:\n\npip install -e \".[dev]\"\n\n\n\nCreate the venv (in the current dir):\n\nuv venv dev-skrub \n\nActivate the venv:\n\nsource activate \n\nInstall skrub and dev dependencies:\n\nuv pip install -e \".[dev]\"\n\n\n\nCreate the conda environment:\n\nconda create -n dev-skrub\n\nActivate the enviornment\n\nconda activate dev-skrub\n\nInstall skrub and dependencies:\n\npip install -e \".[dev]\"\n\n\n\nInstall pixi: https://pixi.sh/latest/installation/\nGo to the skrub folder\nInstall an environment:\n\npixi install dev\n# activate dev from IDE\n\nRun a command in a specific environment:\n\npixi run -e ci-py309-min-deps COMMAND\n\nSpawn a shell with the given env:\n\npixi shell -e ci-latest-optional-deps\n\n\n\n\nMy recommendation: use pixi!"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#note-on-the-pixi.lock-file",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#note-on-the-pixi.lock-file",
    "title": "Women in Machine Learning & Data Science",
    "section": "Note on the pixi.lock file",
    "text": "Note on the pixi.lock file\n\n\n\n\n\n\nImportant\n\n\nIf you use pixi, it may happen that the pixi.lock file will be updated as you run commands.\nRevert changes to this file before adding files and pushing upstream."
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#running-tests",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#running-tests",
    "title": "Women in Machine Learning & Data Science",
    "section": "Running tests",
    "text": "Running tests\n\nUsing environmentsWith pixi\n\n\nFrom inside the root skrub folder, and after activating the environment\npytest --pyargs skrub\n\n\nRun all tests (you will be prompted to choose an env):\npixi run test\nRun tests in a specific env\npixi run -e dev test"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#running-tests-1",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#running-tests-1",
    "title": "Women in Machine Learning & Data Science",
    "section": "Running tests",
    "text": "Running tests\nTests are stored in skrub/tests, or in a tests subfolder.\nIt is possible to run specific tests by providing a path. To test TableVectorizer:\npytest -vsl skrub/tests/test_table_vectorizer.py \n-vsl prints out more information compared to the default."
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#working-on-the-documentation",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#working-on-the-documentation",
    "title": "Women in Machine Learning & Data Science",
    "section": "Working on the documentation",
    "text": "Working on the documentation\nDocs are written in RST and use the Sphinx library for rendering, cross-references and everything else.\n\nFrom environmentWith pixi\n\n\nBuild the doc from the doc folder using:\n# Build the full documentation, including examples\nmake html\n\n# Build documentation without running examples (faster)\nmake html-noplot\n\n# Clean previously built documentation\nmake clean\n\n\nFrom the skrub root folder (where pyproject.toml is):\n# Build the full documentation, including examples\npixi run build-doc\n\n# Build documentation without running examples (faster)\npixi run build-doc-quick\n\n# Clean previously built documentation\npixi run clean-doc\n\n\n\nAfter rendering the docs, open the doc/_build/html/index.html file with a browser."
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#writing-an-example",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#writing-an-example",
    "title": "Women in Machine Learning & Data Science",
    "section": "Writing an example",
    "text": "Writing an example\nFull guide: https://skrub-data.org/stable/tutorial_example.html"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#opening-a-pr-and-contributing-upstream",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#opening-a-pr-and-contributing-upstream",
    "title": "Women in Machine Learning & Data Science",
    "section": "Opening a PR and contributing upstream",
    "text": "Opening a PR and contributing upstream\nMore detail is available on the main website: https://skrub-data.org/stable/CONTRIBUTING.html\nStart by creating a branch:\n# fetch latest updates and start from the current head\ngit fetch upstream\ngit checkout -b my-branch-name-eg-fix-issue-123\nMake some changes, then:\ngit add ./the/file-i-changed\ngit commit -m \"my message\"\ngit push --set-upstream origin my-branch-name-eg-fix-issue-123\nAt this point, visit the GitHub PR page and open a PR from there: https://github.com/skrub-data/skrub/pulls"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#code-formatting-and-pre-commit",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#code-formatting-and-pre-commit",
    "title": "Women in Machine Learning & Data Science",
    "section": "Code formatting and pre-commit",
    "text": "Code formatting and pre-commit\nFormatting is enforced though pre-commit checks.\n\nFrom environmentPixi\n\n\n\nMake sure pre-commit is installed in the environment and the folder\n\npre-commit install\n\nIf missing, install it with pip\n\npip install pre-commit\nThen, add modified files and run pre-commit on them.\ngit add YOUR_FILE\npre-commit\n# if the file has been formatted, add again\ngit add YOUR_FILE\n# commit the changes\ngit commit -m \"MY COMMIT MESSAGE\"\n# pre-commit will run again automatically\n# push the  commit\ngit push\n\n\nIf youâ€™re working entirely with pixi:\ngit add YOUR_FILE\npixi run lint\n# if the file has been formatted, add again\ngit add YOUR_FILE\n# commit the changes\ngit commit -m \"MY COMMIT MESSAGE\"\n# pre-commit will run again automatically\n# push the  commit\ngit push"
  },
  {
    "objectID": "pages/slides/sprint-wimlds-2025/slides.html#where-to-find-this-presentation",
    "href": "pages/slides/sprint-wimlds-2025/slides.html#where-to-find-this-presentation",
    "title": "Women in Machine Learning & Data Science",
    "section": "Where to find this presentation",
    "text": "Where to find this presentation\n\n\nLink to these slides \n\nGitHub project"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#whoami",
    "href": "pages/slides/talk-framatome/slides.html#whoami",
    "title": "Skrub @ Framatome",
    "section": "whoami",
    "text": "whoami\n\nI am a research engineer at Inria as part of the SODA team and the P16 project, and I am the lead developer of skrub \nI hold a PhD in Computer Science and I have been working mostly on tabular data processing, cleaning and retrieval."
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#skrub-compatibility",
    "href": "pages/slides/talk-framatome/slides.html#skrub-compatibility",
    "title": "Skrub @ Framatome",
    "section": "Skrub compatibility",
    "text": "Skrub compatibility\n\nSkrub is fully compatible with pandas and polars\nSkrub transformers are fully compatible with scikit-learn"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#an-example-pipeline",
    "href": "pages/slides/talk-framatome/slides.html#an-example-pipeline",
    "title": "Skrub @ Framatome",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPreprocess the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#exploring-the-data",
    "href": "pages/slides/talk-framatome/slides.html#exploring-the-data",
    "title": "Skrub @ Framatome",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#exploring-the-data-1",
    "href": "pages/slides/talk-framatome/slides.html#exploring-the-data-1",
    "title": "Skrub @ Framatome",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/talk-framatome/slides.html#exploring-the-data-with-skrub",
    "title": "Skrub @ Framatome",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nTableReport Preview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\n\nMore examples"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#data-cleaning-with-pandaspolars-setup",
    "href": "pages/slides/talk-framatome/slides.html#data-cleaning-with-pandaspolars-setup",
    "title": "Skrub @ Framatome",
    "section": "Data cleaning with pandas/polars: setup",
    "text": "Data cleaning with pandas/polars: setup\n\nPandasPolars\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pd = pd.DataFrame(data)\ndisplay(df_pd)\n\n\n\n\n\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\n\n\n0\n2\nx\nfoo\nNaN\n\n01 Jan 2023\n\n\n1\n3\nx\nbar\nNaN\n\n02 Jan 2023\n\n\n2\n2\nx\nbaz\nNaN\n\n03 Jan 2023\n\n\n\n\n\n\n\n\n\n\nimport polars as pl\nimport numpy as np\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pl = pl.DataFrame(data)\ndisplay(df_pl)\n\n\nshape: (3, 6)\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\ni64\nstr\nstr\nf64\nstr\nstr\n\n\n\n\n2\n\"x\"\n\"foo\"\nNaN\n\"\"\n\"01 Jan 2023\"\n\n\n3\n\"x\"\n\"bar\"\nNaN\n\"\"\n\"02 Jan 2023\"\n\n\n2\n\"x\"\n\"baz\"\nNaN\n\"\"\n\"03 Jan 2023\""
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "href": "pages/slides/talk-framatome/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "title": "Skrub @ Framatome",
    "section": "Nulls, datetimes, constant columns with pandas/polars",
    "text": "Nulls, datetimes, constant columns with pandas/polars\n\nPandasPolars\n\n\n\n# Parse the datetime strings with a specific format\ndf_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d %b %Y')\n\n# Drop columns with only a single unique value\ndf_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_pd_cleaned = drop_empty_columns(df_pd_cleaned)\n\n\n\n\n# Parse the datetime strings with a specific format\ndf_pl = df_pl.with_columns([\n    pl.col(\"Date\").str.strptime(pl.Date, \"%d %b %Y\", strict=False).alias(\"Date\")\n])\n\n# Drop columns with only a single unique value\ndf_pl_cleaned = df_pl.select([\n    col for col in df_pl.columns if df_pl[col].n_unique() &gt; 1\n])\n\n# Import selectors for dtype selection\nimport polars.selectors as cs\n\n# Drop columns with only missing values or only empty strings\ndef drop_empty_columns(df):\n    all_nan = df.select(\n        [\n            col for col in df.select(cs.numeric()).columns if \n            df [col].is_nan().all()\n        ]\n    ).columns\n    \n    all_empty = df.select(\n        [\n            col for col in df.select(cs.string()).columns if \n            (df[col].str.strip_chars().str.len_chars()==0).all()\n        ]\n    ).columns\n\n    to_drop = all_nan + all_empty\n\n    return df.drop(to_drop)\n\ndf_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#data-cleaning-with-skrub.cleaner",
    "href": "pages/slides/talk-framatome/slides.html#data-cleaning-with-skrub.cleaner",
    "title": "Skrub @ Framatome",
    "section": "Data cleaning with skrub.Cleaner",
    "text": "Data cleaning with skrub.Cleaner\n\nPandasPolars\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pd)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nInt\nStr\nDate\n\n\n\n\n0\n2\nfoo\n2023-01-01\n\n\n1\n3\nbar\n2023-01-02\n\n\n2\n2\nbaz\n2023-01-03\n\n\n\n\n\n\n\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pl)\ndisplay(df_cleaned)\n\n\nshape: (3, 3)\n\n\n\nInt\nStr\nDate\n\n\ni64\nstr\ndate\n\n\n\n\n2\n\"foo\"\n2023-01-01\n\n\n3\n\"bar\"\n2023-01-02\n\n\n2\n\"baz\"\n2023-01-03"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#encoding-datetime-features-with-pandaspolars",
    "href": "pages/slides/talk-framatome/slides.html#encoding-datetime-features-with-pandaspolars",
    "title": "Skrub @ Framatome",
    "section": "Encoding datetime features with pandas/polars",
    "text": "Encoding datetime features with pandas/polars\n\nPandasPolars\n\n\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pd = pd.DataFrame(data)\ndatetime_column = \"date\"\ndf_pd[datetime_column] = pd.to_datetime(df_pd[datetime_column], errors='coerce')\n\ndf_pd['year'] = df_pd[datetime_column].dt.year\ndf_pd['month'] = df_pd[datetime_column].dt.month\ndf_pd['day'] = df_pd[datetime_column].dt.day\ndf_pd['hour'] = df_pd[datetime_column].dt.hour\ndf_pd['minute'] = df_pd[datetime_column].dt.minute\ndf_pd['second'] = df_pd[datetime_column].dt.second\n\n\n\n\nimport polars as pl\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pl = pl.DataFrame(data)\ndf_pl = df_pl.with_columns(date=pl.col(\"date\").str.to_datetime())\n\ndf_pl = df_pl.with_columns(\n    year=pl.col(\"date\").dt.year(),\n    month=pl.col(\"date\").dt.month(),\n    day=pl.col(\"date\").dt.day(),\n    hour=pl.col(\"date\").dt.hour(),\n    minute=pl.col(\"date\").dt.minute(),\n    second=pl.col(\"date\").dt.second(),\n)"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#encoding-datetime-features-with-skrub.datetimeencoder",
    "href": "pages/slides/talk-framatome/slides.html#encoding-datetime-features-with-skrub.datetimeencoder",
    "title": "Skrub @ Framatome",
    "section": "Encoding datetime features with skrub.DatetimeEncoder",
    "text": "Encoding datetime features with skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nX_date = ToDatetime().fit_transform(df[\"date\"])\nde = DatetimeEncoder(resolution=\"second\")\n# de = DatetimeEncoder(periodic_encoding=\"spline\")\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\nshape: (3, 7)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ date_year â”† date_month â”† date_day â”† date_hour â”† date_minute â”† date_second â”† date_total_seconds â”‚\nâ”‚ ---       â”† ---        â”† ---      â”† ---       â”† ---         â”† ---         â”† ---                â”‚\nâ”‚ f32       â”† f32        â”† f32      â”† f32       â”† f32         â”† f32         â”† f32                â”‚\nâ•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 2023.0    â”† 1.0        â”† 1.0      â”† 12.0      â”† 34.0        â”† 56.0        â”† 1.6726e9           â”‚\nâ”‚ 2023.0    â”† 2.0        â”† 15.0     â”† 8.0       â”† 45.0        â”† 23.0        â”† 1.6765e9           â”‚\nâ”‚ 2023.0    â”† 3.0        â”† 20.0     â”† 18.0      â”† 12.0        â”† 45.0        â”† 1.6793e9           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "href": "pages/slides/talk-framatome/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "title": "Skrub @ Framatome",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler\n\n\n\n\n\n\n\n\n\n\nSkrub wants to solve ML problems based partly on solid engineering and partly on statistical notions. The SquashingScaler is based on the second part, and is taken from a recent paper that evaluates different techniques for improving the performance of NNs."
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "href": "pages/slides/talk-framatome/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "title": "Skrub @ Framatome",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#encoding-categorical-stringtext-features",
    "href": "pages/slides/talk-framatome/slides.html#encoding-categorical-stringtext-features",
    "title": "Skrub @ Framatome",
    "section": "Encoding categorical (string/text) features",
    "text": "Encoding categorical (string/text) features\nCategorical features have a â€œcardinalityâ€: the number of unique values\n\nLow cardinality: OneHotEncoder\nHigh cardinality (&gt;40 unique values): skrub.StringEncoder\nText: skrub.TextEncoder and pretrained models from HuggingFace Hub"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/talk-framatome/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "Skrub @ Framatome",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer\n\nfrom skrub import TableVectorizer\n\ntable_vec = TableVectorizer()\ndf_encoded = table_vec.fit_transform(df)\n\n\n\nApply the Cleaner to all columns\nSplit columns by dtype and # of unique values\nEncode each column separately"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#encoding-all-the-features-tablevectorizer-1",
    "href": "pages/slides/talk-framatome/slides.html#encoding-all-the-features-tablevectorizer-1",
    "title": "Skrub @ Framatome",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#column-transformations-with-applytocols",
    "href": "pages/slides/talk-framatome/slides.html#column-transformations-with-applytocols",
    "title": "Skrub @ Framatome",
    "section": "Column transformations with ApplyToCols",
    "text": "Column transformations with ApplyToCols"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/talk-framatome/slides.html#build-a-predictive-pipeline",
    "title": "Skrub @ Framatome",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/talk-framatome/slides.html#build-a-predictive-pipeline-1",
    "title": "Skrub @ Framatome",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "href": "pages/slides/talk-framatome/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "title": "Skrub @ Framatome",
    "section": "Build a predictive pipeline with tabular_pipeline",
    "text": "Build a predictive pipeline with tabular_pipeline\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_pipeline(Ridge())"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/talk-framatome/slides.html#we-now-have-a-pipeline",
    "title": "Skrub @ Framatome",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\nExplore the data\n\nTableReport\n\nPre-process the data\n\nCleaner, ToDatetime â€¦\n\nPerform feature engineering\n\nTableVectorizer, SquashingScaler, TextEncoder, StringEncoderâ€¦\n\nBuild a scikit-learn pipeline\n\ntabular_pipeline\n\n???\nProfit ðŸ“ˆ"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#what-if",
    "href": "pages/slides/talk-framatome/slides.html#what-if",
    "title": "Skrub @ Framatome",
    "section": "What ifâ€¦",
    "text": "What ifâ€¦\n\nYour data is spread over multiple tables?\nYou want to avoid data leakage?\nYou want to tune more than just the hyperparameters of your model?\nYou want to guarantee that your pipeline is replayed exactly on new data?\n\n\nWhen a normal pipeline is not enoughâ€¦\n\n\nâ€¦ the skrub DataOps come to the rescue ðŸš’"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#dataops",
    "href": "pages/slides/talk-framatome/slides.html#dataops",
    "title": "Skrub @ Framatome",
    "section": "DataOpsâ€¦",
    "text": "DataOpsâ€¦\n\nExtend the scikit-learn machinery to complex multi-table operations\nTake care of data leakage\nTrack all operations with a computational graph (a Data Ops plan)\nAllow tuning any operation in the Data Ops plan\nCan be persisted and shared easily"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/talk-framatome/slides.html#how-do-dataops-work-though",
    "title": "Skrub @ Framatome",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#starting-with-the-dataops",
    "href": "pages/slides/talk-framatome/slides.html#starting-with-the-dataops",
    "title": "Skrub @ Framatome",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\nimport skrub\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\n\nbaskets and products represent inputs to the pipeline.\nSkrub tracks X and y so that training and test splits are never mixed."
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#applying-a-transformer",
    "href": "pages/slides/talk-framatome/slides.html#applying-a-transformer",
    "title": "Skrub @ Framatome",
    "section": "Applying a transformer",
    "text": "Applying a transformer\n\nfrom skrub import selectors as s\n\nvectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder()\n)\nvectorized_products = products.skb.apply(\n    vectorizer, cols=s.all() - \"basket_ID\"\n)"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#executing-dataframe-operations",
    "href": "pages/slides/talk-framatome/slides.html#executing-dataframe-operations",
    "title": "Skrub @ Framatome",
    "section": "Executing dataframe operations",
    "text": "Executing dataframe operations\n\naggregated_products = vectorized_products.groupby(\n    \"basket_ID\"\n).agg(\"mean\").reset_index()\n\nfeatures = X.merge(\n    aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n)\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#applying-a-ml-model",
    "href": "pages/slides/talk-framatome/slides.html#applying-a-ml-model",
    "title": "Skrub @ Framatome",
    "section": "Applying a ML model",
    "text": "Applying a ML model\n\nfrom sklearn.ensemble import ExtraTreesClassifier  \npredictions = features.skb.apply(\n    ExtraTreesClassifier(n_jobs=-1), y=y\n)"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/talk-framatome/slides.html#exporting-the-plan-in-a-learner",
    "title": "Skrub @ Framatome",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe Learner is a stand-alone object that works like a scikit-learn estimator that takes a dictionary as input rather than just X and y.\n\n\nlearner = predictions.skb.make_learner(fitted=True)\n\n\n\nThen, the learner can be pickled â€¦\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\nâ€¦ loaded and applied to new data:\nwith open(\"learner.bin\", \"rb\") as fp:\n    loaded_learner = pickle.load(fp)\ndata = skrub.datasets.fetch_credit_fraud(split=\"test\")\nnew_baskets = data.baskets\nnew_products = data.products\nloaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})\n\n\narray([0, 0, 0, ..., 0, 0, 0], shape=(31549,))"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#inspecting-the-data-ops-plan",
    "href": "pages/slides/talk-framatome/slides.html#inspecting-the-data-ops-plan",
    "title": "Skrub @ Framatome",
    "section": "Inspecting the Data Ops plan",
    "text": "Inspecting the Data Ops plan\npredictions.skb.full_report()\n\nExecution report\nEach node:\n\nShows a preview of the data resulting from the operation\nReports the location in the code where the code is defined\nShows the run time of the node"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/talk-framatome/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "Skrub @ Framatome",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestRegressor()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestRegressor()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#tuning-with-data-ops-is-simple",
    "href": "pages/slides/talk-framatome/slides.html#tuning-with-data-ops-is-simple",
    "title": "Skrub @ Framatome",
    "section": "Tuning with Data Ops is simple!",
    "text": "Tuning with Data Ops is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestRegressor(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#run-hyperparameter-search",
    "href": "pages/slides/talk-framatome/slides.html#run-hyperparameter-search",
    "title": "Skrub @ Framatome",
    "section": "Run hyperparameter search",
    "text": "Run hyperparameter search\n# fit the search \nsearch = regressor.skb.make_randomized_search(\n    scoring=\"roc_auc\", fitted=True, cv=5\n)\n\n# save the best learner\nbest_learner = search.best_learner_"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#a-parallel-coordinate-plot-to-explore-hyperparameters",
    "href": "pages/slides/talk-framatome/slides.html#a-parallel-coordinate-plot-to-explore-hyperparameters",
    "title": "Skrub @ Framatome",
    "section": "A parallel coordinate plot to explore hyperparameters",
    "text": "A parallel coordinate plot to explore hyperparameters\nsearch = pred.skb.get_randomized_search(fitted=True)\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#more-information-about-the-data-ops",
    "href": "pages/slides/talk-framatome/slides.html#more-information-about-the-data-ops",
    "title": "Skrub @ Framatome",
    "section": "More information about the Data Ops",
    "text": "More information about the Data Ops\n\nSkrub example gallery\nSkrub user guide\nTutorial on timeseries forecasting at Euroscipy 2025\nKaggle notebook on the Titanic survival challenge"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#getting-involved",
    "href": "pages/slides/talk-framatome/slides.html#getting-involved",
    "title": "Skrub @ Framatome",
    "section": "Getting involved",
    "text": "Getting involved\nDo you want to learn more?\n\nSkrub website\nSkrub materials website\nDiscord server\n\nFollow skrub on:\n\nBluesky\nLinkedIn\n\nStar skrub on GitHub, or contribute directly:\n\nGitHub repository"
  },
  {
    "objectID": "pages/slides/talk-framatome/slides.html#tldw-skrub",
    "href": "pages/slides/talk-framatome/slides.html#tldw-skrub",
    "title": "Skrub @ Framatome",
    "section": "tl;dw: skrub",
    "text": "tl;dw: skrub\n\ninteractive data exploration: TableReport\nautomated pre-processing of pandas and polars dataframes: Cleaner\npowerful feature engineering: TableVectorizer, tabular_pipeline\ncolumn- and dataframe-level operations: ApplyToCols, selectors\nDataOps, plans, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/listings.html",
    "href": "pages/listings.html",
    "title": "Latest",
    "section": "",
    "text": "Mar 26, 2025\n\n\nRiccardo Cappuzzo\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/listings.html#blog-posts",
    "href": "pages/listings.html#blog-posts",
    "title": "Latest",
    "section": "",
    "text": "Mar 26, 2025\n\n\nRiccardo Cappuzzo\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/listings.html#talks",
    "href": "pages/listings.html#talks",
    "title": "Latest",
    "section": "Talks",
    "text": "Talks"
  },
  {
    "objectID": "pages/notebooks/index_notebooks.html",
    "href": "pages/notebooks/index_notebooks.html",
    "title": "Skrub tutorials and long-form posts",
    "section": "",
    "text": "An introductory notebook for skrub\nComparing different categorical encoders.\nEuroSciPy 2025 Tutorial"
  }
]