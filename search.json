[
  {
    "objectID": "pages/notebooks/categorical-encoders/categorical-encoders.html",
    "href": "pages/notebooks/categorical-encoders/categorical-encoders.html",
    "title": "What‚Äôs the best way to encode categorical features? A use case with Skrub encoders",
    "section": "",
    "text": "Encoding categorical values (such as names, addresses, but also textual data) is a very common problem when it comes to prepare tabular data for training ML models, and Skrub provides four encoders to this end:\n\nskrub.MinhashEncoder, a simple encoder based on hashing categories.\nskrub.GapEncoder, which encoders strings based on latent categories estimated from the data.\nskrub.TextEncoder, a language model-based encoder that uses pre-trained language models to produce vectors for each string.\nskrub.StringEncoder, an encoder that vectorizes data with tf-idf and then applies SVD to reduce the number of features.\n\nThe objective of this post is to test the performance of each encoder on a few datasets in order to find out which methods should be considered in various circumstances.\n\nPreparing the datasets\nWe begin by importing and preparing the datasets that should be used for the experiments:\nfrom skrub.datasets import fetch_toxicity, fetch_movielens, fetch_employee_salaries, fetch_open_payments\n\ndatasets = {}\n## Open Payments (Classification)\ndataset = fetch_open_payments()\nX, y = dataset.X, dataset.y\ny = y.map({\"disallowed\": 0, \"allowed\": 1})\ndatasets[\"Open Payments\"] = (X,y,\"classification\")\n\n## Toxicity (Classification)\ndataset = fetch_toxicity()\nX, y = dataset.X, dataset.y\nX[\"is_toxic\"] = y\ny = X.pop(\"is_toxic\").map({\"Toxic\": 1, \"Not Toxic\": 0})\ndatasets[\"Toxicity\"] = (X,y, \"classification\")\n\n## Movielens (Regression)\ndataset = fetch_movielens()\nX, y = pl.from_pandas(dataset.movies), pl.from_pandas(dataset.ratings)\nX = (\n    X.join(y, on=\"movieId\")\n    .group_by(\"movieId\", \"title\", \"genres\")\n    .agg(target=pl.mean(\"rating\"))\n)\ny = X[\"target\"].to_numpy()\nX = X.drop(\"target\")\ndatasets[\"Movielens\"]=(X,y, \"regression\")\n\n## Employee salaries (Regression)\ndataset = fetch_employee_salaries()\nX = pl.from_pandas(dataset.employee_salaries)\ny = X[\"current_annual_salary\"]\nX = X.drop(\"current_annual_salary\")\n\ndatasets[\"Employee salaries\"]=(X,y, \"regression\")\n\n\nSetting up the experiments\nWe can test each method by building a scikit-learn pipeline for each categorical encoder, using the default HistGradientBoostingClassifier and HistGradientBoostingRegressor as prediction model.\nThem, we use the cross_validate function to track the fit and score time, as well as the prediction performance of each pipeline over different splits. For simplicity, we are not performing hyperparameter optimization for either the categorical encoder or the learner.\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import (\n    HistGradientBoostingClassifier,\n    HistGradientBoostingRegressor,\n)\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import make_pipeline\nimport polars as pl\nfrom skrub import (\n    GapEncoder,\n    MinHashEncoder,\n    StringEncoder,\n    TableVectorizer,\n    TextEncoder,\n)\n\ndef run_experiments(X, y, task, dataset_name):\n    if task == \"regression\":\n        model = HistGradientBoostingRegressor()\n        scoring = \"r2\"\n    else:\n        model = HistGradientBoostingClassifier()\n        scoring = \"roc_auc\"\n\n    results = []\n\n    # For each encoder, create a new pipeline\n    gap_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=GapEncoder(n_components=30)), model\n    )\n    minhash_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=MinHashEncoder(n_components=30)), model\n    )\n    text_encoder = TextEncoder(\n        \"sentence-transformers/paraphrase-albert-small-v2\",\n        device=\"cpu\",\n    )\n    text_encoder_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=text_encoder),\n        model,\n    )\n    string_encoder = StringEncoder(ngram_range=(3, 4), analyzer=\"char_wb\")\n    string_encoder_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=string_encoder),\n        model,\n    )\n\n    pipes = [\n        (\"GapEncoder\", gap_pipe),\n        (\"MinHashEncoder\", minhash_pipe),\n        (\"TextEncoder\", text_encoder_pipe),\n        (\"StringEncoder\", string_encoder_pipe),\n    ]\n\n    for name, p in pipes:\n        cross_validate_results = cross_validate(p, X, y, scoring=scoring)\n        results.append(\n            pl.DataFrame(add_results(name, dataset_name, cross_validate_results))\n        )\n    df_results = pl.concat(results).with_columns(task=pl.lit(task))\n    return df_results\n\n\nRunning the experiments and saving the results\nFinally, I ran the crossvalidation step for each encoder on all dataset and I recorded the files in a csv file. This step took quite some time, and was done offline in a separater script.\nall_results = []\nfor dataset_name,v in datasets.items():\n    X, y, task = v\n    results = run_experiments(X, y, task, dataset_name) \n    all_results.append(results)\ndf_all_results = pl.concat(all_results)\ndf_all_results.write_csv(\"results-encoder_benchmark.csv\")\n\n\nPlotting the results\nNow, we can load the results and start plotting the results. We first split the results in two subtables based on the specific task (either regression or classification), to avoid mixing metrics.\n\nimport polars as pl\nimport matplotlib.pyplot as plt\ndf = pl.read_csv(\"results-encoder_benchmark.csv\")\ndf_regression = df.filter(task=\"regression\")\ndf_classification = df.filter(task=\"classification\")\n\nTo see the tradeoff between fit time and prediction performance, we use a scatterplot with error bars to find the average performance and run time for each method.\nThen, we can plot the prediction performance as a function of the run time.\n\ndef plot_scatter_errorbar(df, ylabel, sharey=False, suptitle=\"\"):\n    # Fixing the colors for each cluster of points and the error bars\n    tab10_colors = plt.get_cmap('tab10').colors\n    colors = dict(zip(df[\"estimator\"].unique().sort().to_list(),tab10_colors[:4]))\n    fig, axs = plt.subplots(1,2, sharey=sharey, layout=\"constrained\", figsize=(8,3))\n    # Each dataset gets a subplot\n    for idx, (dataset, g) in enumerate(df.group_by(\"dataset\")):\n        ax=axs[idx]\n        # Each estimator is plotted separately as a cluster of points\n        for estimator, gdf in g.group_by(\"estimator\"):\n            estim = estimator[0]\n            color = colors[estim]\n            x = gdf[\"fit_time\"].to_numpy()\n            y = gdf[\"test_score\"].to_numpy()\n            label = estim if idx == 0 else \"_\" + estim\n            ax.scatter(x=x, y=y, label=label, color=color)\n            \n            # find the mean and the error bars \n            xerr_mean = gdf[\"fit_time\"].mean()\n            yerr_mean = gdf[\"test_score\"].mean()\n            x_err = gdf[\"fit_time\"].std()\n            y_err = gdf[\"test_score\"].std()\n            # plot the error bars\n            ax.errorbar(xerr_mean, yerr_mean, xerr=x_err, fmt=\"none\", color=color)\n            ax.errorbar(xerr_mean, yerr_mean, yerr=y_err, fmt=\"none\", color=color)\n            \n        ax.set_title(dataset[0])\n        ax.set_xlabel(\"Fit time (s)\")\n        ax.set_ylabel(ylabel)\n        ax.set_xscale(\"log\")\n    fig.suptitle(suptitle)\n    fig.legend(loc=\"lower center\", ncols=2)\n\n\nplot_scatter_errorbar(df_classification, \"ROC-AUC\", sharey=True, suptitle=\"Classification\")\nplot_scatter_errorbar(df_regression, \"R2 score\", sharey=False, suptitle=\"Regression\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prediction performance is fairly consistent across methods, although this depends on the table under observation. MinhashEncoder and StringEncoder are consistently faster than the alternatives.\nTextEncoder is always much slower than the other methods, however it must be noted that this example was run on a CPU, rather than a much faster GPU.\nTo have a better idea of why some methods may outperform others, we should take a look at the actual tables. We can do so very easily thanks to the skrub TableReport object.\n\nfrom skrub import TableReport\nfrom skrub.datasets import fetch_toxicity, fetch_movielens, fetch_employee_salaries, fetch_open_payments\n\n\n# OPEN PAYMENTS\ndataset = fetch_open_payments()\nX, y = dataset.X, dataset.y\nTableReport(X)\n\nProcessing column   5 / 5\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# TOXICITY\ndataset = fetch_toxicity()\nX, y = dataset.X, dataset.y\nTableReport(X)\n\nProcessing column   1 / 1\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# MOVIELENS\ndataset = fetch_movielens()\nX, y = pl.from_pandas(dataset.movies), pl.from_pandas(dataset.ratings)\nX = (\n    X.join(y, on=\"movieId\")\n    .group_by(\"movieId\", \"title\", \"genres\")\n    .agg(target=pl.mean(\"rating\"))\n)\nX = X.drop(\"target\")\n\nTableReport(X)\n\nProcessing column   3 / 3\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# EMPLOYEE SALARIES\ndataset = fetch_employee_salaries()\nX = pl.from_pandas(dataset.employee_salaries)\nX = X.drop(\"current_annual_salary\")\nTableReport(X)\n\nProcessing column   8 / 8\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nAll datasets include high cardinality features which must be encoded using one of the skrub Encoders. The Toxicity dataset is different from the others in that it involves free-flowing text as tweets, while all other tables include a (possibly) large number of unique categories.\nThis explains why the TextEncoder is so much better than the other encoders on Toxicity, while its performance on the other datasets is more in line with the others.\nOn the other hand, the StringEncodershows a strong performance in all cases, while being top-2 on average for the fit time.\n\n\nIn summary\nThe skrub TableVectorizer transforms categorical features into numbers so that ML models can make better use of the information they contain. The StringEncoder can be considered the best all-rounder, being fast to train in most cases, while maintaining strong performance in general. The TextEncoder shines when textual data is available as it can make full use of the pre-trained language models it relies on. The MinHashEncoder and the GapEncoder are more specialized models that may work better than the alternatives in specific circumstances."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html",
    "title": "Skrub",
    "section": "",
    "text": "skrub-data.org\nLess wrangling, more machine learning"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#skrub-helps-at-several-stages-of-a-tabular-learning-project",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#skrub-helps-at-several-stages-of-a-tabular-learning-project",
    "title": "Skrub",
    "section": "Skrub helps at several stages of a tabular learning project",
    "text": "Skrub helps at several stages of a tabular learning project\n\nWhat‚Äôs in the data? (EDA)\nCan we learn anything? (baselines)\nHow do I represent the data? (feature extraction)\nHow do I bring it all together? (building a pipeline)"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tablereport-interactive-display-of-a-dataframe",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tablereport-interactive-display-of-a-dataframe",
    "title": "Skrub",
    "section": "TableReport: interactive display of a dataframe",
    "text": "TableReport: interactive display of a dataframe\n\nskrub.TableReport(employees, verbose=0)\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWe can tell skrub to patch the default display of polars and pandas dataframes.\n\nskrub.patch_display(verbose=0)"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tabular_learner-a-pre-made-robust-baseline",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tabular_learner-a-pre-made-robust-baseline",
    "title": "Skrub",
    "section": "tabular_learner: a pre-made robust baseline",
    "text": "tabular_learner: a pre-made robust baseline\n\nlearner = skrub.tabular_learner(\"regressor\")\nlearner\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())]) tablevectorizer: TableVectorizerTableVectorizer(low_cardinality=ToCategorical()) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder() low_cardinalityToCategoricalToCategorical() high_cardinalityStringEncoderStringEncoder() HistGradientBoostingRegressor?Documentation for HistGradientBoostingRegressorHistGradientBoostingRegressor() \n\n\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(learner, X, y, scoring=\"r2\")\n\narray([0.90121408, 0.8776637 , 0.91159274, 0.92363764, 0.92534873])\n\n\nThe tabular_learner adapts to the supervised estimator we choose\n\nfrom sklearn.linear_model import Ridge\n\nlearner = skrub.tabular_learner(Ridge())\nlearner\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())]) tablevectorizer: TableVectorizerTableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline')) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder(periodic_encoding='spline') low_cardinalityOneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False) high_cardinalityStringEncoderStringEncoder() SimpleImputer?Documentation for SimpleImputerSimpleImputer(add_indicator=True) StandardScaler?Documentation for StandardScalerStandardScaler() Ridge?Documentation for RidgeRidge() \n\n\n\ncross_val_score(learner, X, y, scoring=\"r2\")\n\narray([0.77747104, 0.74636601, 0.78608641, 0.77574873, 0.79073481])"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tablevectorizer-apply-an-appropriate-transformer-to-each-column",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tablevectorizer-apply-an-appropriate-transformer-to-each-column",
    "title": "Skrub",
    "section": "TableVectorizer: apply an appropriate transformer to each column",
    "text": "TableVectorizer: apply an appropriate transformer to each column\n\nvectorizer = skrub.TableVectorizer()\ntransformed = vectorizer.fit_transform(X)\n\nThe TableVectorizer identifies several kinds of columns:\n\ncategorical, low cardinality\ncategorical, high cardinality\ndatetime\nnumeric\n‚Ä¶ we may add more\n\n\nfrom pprint import pprint\n\npprint(vectorizer.column_to_kind_)\n\n{'assignment_category': 'low_cardinality',\n 'date_first_hired': 'datetime',\n 'department': 'low_cardinality',\n 'department_name': 'low_cardinality',\n 'division': 'high_cardinality',\n 'employee_position_title': 'high_cardinality',\n 'gender': 'low_cardinality',\n 'year_first_hired': 'numeric'}\n\n\nFor each kind, it applies an appropriate transformer\n\nvectorizer.transformers_[\"department\"]  # low-cardinality categorical\n\nOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneHotEncoder?Documentation for OneHotEncoderiFittedOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False) \n\n\n\nvectorizer.transformers_[\"employee_position_title\"]  # high-cardinality categorical\n\nStringEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StringEncoderiFittedStringEncoder() \n\n\n\nvectorizer.transformers_[\"date_first_hired\"]  # datetime\n\nDatetimeEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DatetimeEncoderiFittedDatetimeEncoder() \n\n\n‚Ä¶ and those transformers turn the input into numeric features that can be used for ML\n\ntransformed[vectorizer.input_to_outputs_[\"date_first_hired\"]]\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nFor high-cardinality categorical columns the default GapEncoder identifies sparse topics (more later).\n\ntransformed[vectorizer.input_to_outputs_[\"employee_position_title\"]]\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe transformer used for each column kind can be easily configured."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#preprocessing-in-the-tablevectorizer",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#preprocessing-in-the-tablevectorizer",
    "title": "Skrub",
    "section": "Preprocessing in the TableVectorizer",
    "text": "Preprocessing in the TableVectorizer\nThe TableVectorizer actually performs a lot of preprocessing before applying the final transformers, such as:\n\nensuring consistent column names\ndetecting missing values such as \"N/A\"\ndropping empty columns\nhandling pandas dtypes ‚Äì float64, nan vs Float64, NA\nparsing numbers\nparsing dates, ensuring consistent dtype and timezone\nconverting numbers to float32 for faster computation & less memory downstream\n‚Ä¶\n\n\npprint(vectorizer.all_processing_steps_[\"date_first_hired\"])\n\n[CleanNullStrings(),\n DropUninformative(),\n ToDatetime(),\n DatetimeEncoder(),\n {'date_first_hired_day': ToFloat32(), 'date_first_hired_month': ToFloat32(), ...}]"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#extracting-good-features",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#extracting-good-features",
    "title": "Skrub",
    "section": "Extracting good features",
    "text": "Extracting good features\nSkrub offers several encoders to extract features from different columns. In particular from categorical columns.\n\nGapEncoder\nCategories are somewhere between text and an enumeration‚Ä¶ The GapEncoder is somewhere between a topic model and a one-hot encoder!\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\ngap = skrub.GapEncoder()\npos_title = X[\"employee_position_title\"]\nloadings = gap.fit_transform(pos_title).set_index(pos_title.values).head()\n\nloadings.columns = [c.split(\": \")[1] for c in loadings.columns]\nsns.heatmap(loadings)\n_ = plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\")\n\n\n\n\n\n\n\n\n\n\nTextEncoder\nExtract embeddings from a text column using any model from the HuggingFace Hub.\n\nimport pandas as pd\n\nX = pd.Series([\"airport\", \"flight\", \"plane\", \"pineapple\", \"fruit\"])\nencoder = skrub.TextEncoder(model_name=\"all-MiniLM-L6-v2\", n_components=None)\nembeddings = encoder.fit_transform(X).set_index(X.values)\n\nsns.heatmap(embeddings @ embeddings.T)\n\n\n\n\n\n\n\n\n\n\nMinHashEncoder\nA fast, stateless way of encoding strings that works especially well with models based on decision trees (gradient boosting, random forest)."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#more-interactive-and-expressive-pipelines",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#more-interactive-and-expressive-pipelines",
    "title": "Skrub",
    "section": "More interactive and expressive pipelines",
    "text": "More interactive and expressive pipelines\nTo go further than what can be done with scikit-learn Pipelines and the skrub transformers shown above, we are developing new utilities to easily define and inspect flexible pipelines that can process several dataframes.\nA prototype will be shown in a separate notebook."
  },
  {
    "objectID": "pages/slides/index_slides.html",
    "href": "pages/slides/index_slides.html",
    "title": "Skrub talks",
    "section": "",
    "text": "This page is used to track all the talks about skrub, together with some useful additional material."
  },
  {
    "objectID": "pages/slides/index_slides.html#all-talks",
    "href": "pages/slides/index_slides.html#all-talks",
    "title": "Skrub talks",
    "section": "All talks",
    "text": "All talks"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#whoami",
    "href": "pages/slides/pydata-2025/slides.html#whoami",
    "title": "Skrub",
    "section": "whoami",
    "text": "whoami\n\nI am a research engineer at Inria as part of the P16 project, and I am the lead developer of skrub \nI‚Äôm Italian, but I don‚Äôt drink coffee, wine, and I like pizza with fries \nI did my PhD in C√¥te d‚ÄôAzur, and I moved away because it was too sunny and I don‚Äôt like the sea"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#a-teaser-for-later",
    "href": "pages/slides/pydata-2025/slides.html#a-teaser-for-later",
    "title": "Skrub",
    "section": "A teaser for later‚Ä¶",
    "text": "A teaser for later‚Ä¶\nInspect all the steps of your pipeline: Execution report"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#a-teaser-for-later-1",
    "href": "pages/slides/pydata-2025/slides.html#a-teaser-for-later-1",
    "title": "Skrub",
    "section": "A teaser for later‚Ä¶",
    "text": "A teaser for later‚Ä¶\nExplore your hyperparameter search space"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#skrub-compatibility",
    "href": "pages/slides/pydata-2025/slides.html#skrub-compatibility",
    "title": "Skrub",
    "section": "skrub compatibility",
    "text": "skrub compatibility\n\nSkrub is fully compatible with pandas and polars\nSkrub transformers are fully compatible with scikit-learn"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#an-example-pipeline",
    "href": "pages/slides/pydata-2025/slides.html#an-example-pipeline",
    "title": "Skrub",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPreprocess the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#exploring-the-data",
    "href": "pages/slides/pydata-2025/slides.html#exploring-the-data",
    "title": "Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#exploring-the-data-1",
    "href": "pages/slides/pydata-2025/slides.html#exploring-the-data-1",
    "title": "Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/pydata-2025/slides.html#exploring-the-data-with-skrub",
    "title": "Skrub",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nTableReport Preview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\n\nMore examples"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#data-cleaning-with-pandaspolars-setup",
    "href": "pages/slides/pydata-2025/slides.html#data-cleaning-with-pandaspolars-setup",
    "title": "Skrub",
    "section": "Data cleaning with pandas/polars: setup",
    "text": "Data cleaning with pandas/polars: setup\n\nPandasPolars\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pd = pd.DataFrame(data)\ndisplay(df_pd)\n\n\n\n\n\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\n\n\n0\n2\nx\nfoo\nNaN\n\n01 Jan 2023\n\n\n1\n3\nx\nbar\nNaN\n\n02 Jan 2023\n\n\n2\n2\nx\nbaz\nNaN\n\n03 Jan 2023\n\n\n\n\n\n\n\n\n\n\nimport polars as pl\nimport numpy as np\ndata = {\n    \"Int\": [2, 3, 2],  # Multiple unique values\n    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n}\n\ndf_pl = pl.DataFrame(data)\ndisplay(df_pl)\n\n\nshape: (3, 6)\n\n\n\nInt\nConst str\nStr\nAll nan\nAll empty\nDate\n\n\ni64\nstr\nstr\nf64\nstr\nstr\n\n\n\n\n2\n\"x\"\n\"foo\"\nNaN\n\"\"\n\"01 Jan 2023\"\n\n\n3\n\"x\"\n\"bar\"\nNaN\n\"\"\n\"02 Jan 2023\"\n\n\n2\n\"x\"\n\"baz\"\nNaN\n\"\"\n\"03 Jan 2023\""
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "href": "pages/slides/pydata-2025/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "title": "Skrub",
    "section": "Nulls, datetimes, constant columns with pandas/polars",
    "text": "Nulls, datetimes, constant columns with pandas/polars\n\nPandasPolars\n\n\n\n# Parse the datetime strings with a specific format\ndf_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d %b %Y')\n\n# Drop columns with only a single unique value\ndf_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_pd_cleaned = drop_empty_columns(df_pd_cleaned)\n\n\n\n\n# Parse the datetime strings with a specific format\ndf_pl = df_pl.with_columns([\n    pl.col(\"Date\").str.strptime(pl.Date, \"%d %b %Y\", strict=False).alias(\"Date\")\n])\n\n# Drop columns with only a single unique value\ndf_pl_cleaned = df_pl.select([\n    col for col in df_pl.columns if df_pl[col].n_unique() &gt; 1\n])\n\n# Import selectors for dtype selection\nimport polars.selectors as cs\n\n# Drop columns with only missing values or only empty strings\ndef drop_empty_columns(df):\n    all_nan = df.select(\n        [\n            col for col in df.select(cs.numeric()).columns if \n            df [col].is_nan().all()\n        ]\n    ).columns\n    \n    all_empty = df.select(\n        [\n            col for col in df.select(cs.string()).columns if \n            (df[col].str.strip_chars().str.len_chars()==0).all()\n        ]\n    ).columns\n\n    to_drop = all_nan + all_empty\n\n    return df.drop(to_drop)\n\ndf_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#data-cleaning-with-skrub.cleaner",
    "href": "pages/slides/pydata-2025/slides.html#data-cleaning-with-skrub.cleaner",
    "title": "Skrub",
    "section": "Data cleaning with skrub.Cleaner",
    "text": "Data cleaning with skrub.Cleaner\n\nPandasPolars\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pd)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nInt\nStr\nDate\n\n\n\n\n0\n2\nfoo\n2023-01-01\n\n\n1\n3\nbar\n2023-01-02\n\n\n2\n2\nbaz\n2023-01-03\n\n\n\n\n\n\n\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\ndf_cleaned = cleaner.fit_transform(df_pl)\ndisplay(df_cleaned)\n\n\nshape: (3, 3)\n\n\n\nInt\nStr\nDate\n\n\ni64\nstr\ndate\n\n\n\n\n2\n\"foo\"\n2023-01-01\n\n\n3\n\"bar\"\n2023-01-02\n\n\n2\n\"baz\"\n2023-01-03"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-datetime-features-with-pandaspolars",
    "href": "pages/slides/pydata-2025/slides.html#encoding-datetime-features-with-pandaspolars",
    "title": "Skrub",
    "section": "Encoding datetime features with pandas/polars",
    "text": "Encoding datetime features with pandas/polars\n\nPandasPolars\n\n\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pd = pd.DataFrame(data)\ndatetime_column = \"date\"\ndf_pd[datetime_column] = pd.to_datetime(df_pd[datetime_column], errors='coerce')\n\ndf_pd['year'] = df_pd[datetime_column].dt.year\ndf_pd['month'] = df_pd[datetime_column].dt.month\ndf_pd['day'] = df_pd[datetime_column].dt.day\ndf_pd['hour'] = df_pd[datetime_column].dt.hour\ndf_pd['minute'] = df_pd[datetime_column].dt.minute\ndf_pd['second'] = df_pd[datetime_column].dt.second\n\n\n\n\nimport polars as pl\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pl = pl.DataFrame(data)\ndf_pl = df_pl.with_columns(date=pl.col(\"date\").str.to_datetime())\n\ndf_pl = df_pl.with_columns(\n    year=pl.col(\"date\").dt.year(),\n    month=pl.col(\"date\").dt.month(),\n    day=pl.col(\"date\").dt.day(),\n    hour=pl.col(\"date\").dt.hour(),\n    minute=pl.col(\"date\").dt.minute(),\n    second=pl.col(\"date\").dt.second(),\n)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-datetime-features-with-skrub.datetimeencoder",
    "href": "pages/slides/pydata-2025/slides.html#encoding-datetime-features-with-skrub.datetimeencoder",
    "title": "Skrub",
    "section": "Encoding datetime features with skrub.DatetimeEncoder",
    "text": "Encoding datetime features with skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nX_date = ToDatetime().fit_transform(df[\"date\"])\nde = DatetimeEncoder(resolution=\"second\")\n# de = DatetimeEncoder(periodic_encoding=\"spline\")\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\nshape: (3, 7)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ date_year ‚îÜ date_month ‚îÜ date_day ‚îÜ date_hour ‚îÜ date_minute ‚îÜ date_second ‚îÜ date_total_seconds ‚îÇ\n‚îÇ ---       ‚îÜ ---        ‚îÜ ---      ‚îÜ ---       ‚îÜ ---         ‚îÜ ---         ‚îÜ ---                ‚îÇ\n‚îÇ f32       ‚îÜ f32        ‚îÜ f32      ‚îÜ f32       ‚îÜ f32         ‚îÜ f32         ‚îÜ f32                ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 2023.0    ‚îÜ 1.0        ‚îÜ 1.0      ‚îÜ 12.0      ‚îÜ 34.0        ‚îÜ 56.0        ‚îÜ 1.6726e9           ‚îÇ\n‚îÇ 2023.0    ‚îÜ 2.0        ‚îÜ 15.0     ‚îÜ 8.0       ‚îÜ 45.0        ‚îÜ 23.0        ‚îÜ 1.6765e9           ‚îÇ\n‚îÇ 2023.0    ‚îÜ 3.0        ‚îÜ 20.0     ‚îÜ 18.0      ‚îÜ 12.0        ‚îÜ 45.0        ‚îÜ 1.6793e9           ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#what-periodic-features-look-like",
    "href": "pages/slides/pydata-2025/slides.html#what-periodic-features-look-like",
    "title": "Skrub",
    "section": "What periodic features look like",
    "text": "What periodic features look like"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "href": "pages/slides/pydata-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "title": "Skrub",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler\n\n\nSkrub wants to solve ML problems based partly on solid engineering and partly on statistical notions. The SquashingScaler is based on the second part, and is taken from a recent paper that evaluates different techniques for improving the performance of NNs."
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "href": "pages/slides/pydata-2025/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "title": "Skrub",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-categorical-stringtext-features",
    "href": "pages/slides/pydata-2025/slides.html#encoding-categorical-stringtext-features",
    "title": "Skrub",
    "section": "Encoding categorical (string/text) features",
    "text": "Encoding categorical (string/text) features\nCategorical features have a ‚Äúcardinality‚Äù: the number of unique values\n\nLow cardinality: OneHotEncoder\nHigh cardinality (&gt;40 unique values): skrub.StringEncoder\nText: skrub.TextEncoder and pretrained models from HuggingFace Hub"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/pydata-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "Skrub",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer\nfrom skrub import TableVectorizer, TextEncoder\n\ntable_vec = TableVectorizer()\ndf_encoded = table_vec.fit_transform(df)\n\n\nApply the Cleaner to all columns\nSplit columns by dtype and # of unique values\nEncode each column separately"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#encoding-all-the-features-tablevectorizer-1",
    "href": "pages/slides/pydata-2025/slides.html#encoding-all-the-features-tablevectorizer-1",
    "title": "Skrub",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline-1",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "href": "pages/slides/pydata-2025/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "title": "Skrub",
    "section": "Build a predictive pipeline with tabular_pipeline",
    "text": "Build a predictive pipeline with tabular_pipeline\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_pipeline(Ridge())"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/pydata-2025/slides.html#we-now-have-a-pipeline",
    "title": "Skrub",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\nExplore the data\n\nTableReport\n\nPre-process the data\n\nCleaner, ToDatetime ‚Ä¶\n\nPerform feature engineering\n\nTableVectorizer,SquashingScaler, TextEncoder, StringEncoder‚Ä¶\n\nBuild a scikit-learn pipeline\n\ntabular_pipeline ‚Ä¶\n\n???\nProfit üìà"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#what-if",
    "href": "pages/slides/pydata-2025/slides.html#what-if",
    "title": "Skrub",
    "section": "What if‚Ä¶",
    "text": "What if‚Ä¶\n\nYour data is spread over multiple tables?\nYou want to avoid data leakage?\nYou want to tune more than just the hyperparameters of your model?\nYou want to guarantee that your pipeline is replayed exactly on new data?"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#section-2",
    "href": "pages/slides/pydata-2025/slides.html#section-2",
    "title": "Skrub",
    "section": "",
    "text": "When a normal pipe is not enough‚Ä¶\n\n‚Ä¶ the skrub DataOps come to the rescue üöí"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#dataops",
    "href": "pages/slides/pydata-2025/slides.html#dataops",
    "title": "Skrub",
    "section": "DataOps‚Ä¶",
    "text": "DataOps‚Ä¶\n\nExtend the scikit-learn machinery to complex multi-table operations, and take care of data leakage\nTrack all operations with a computational graph (a Data Ops plan)\nAre transparent and give direct access to the underlying object\nAllow tuning any operation in the Data Ops plan\nGuarantee that all operations are reproducible\nCan be persisted and shared easily"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/pydata-2025/slides.html#how-do-dataops-work-though",
    "title": "Skrub",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#starting-with-the-dataops",
    "href": "pages/slides/pydata-2025/slides.html#starting-with-the-dataops",
    "title": "Skrub",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\nimport skrub\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\n\nbaskets and products represent inputs to the pipeline.\nSkrub tracks X and y so that training and test splits are never mixed."
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#applying-a-transformer",
    "href": "pages/slides/pydata-2025/slides.html#applying-a-transformer",
    "title": "Skrub",
    "section": "Applying a transformer",
    "text": "Applying a transformer\n\nfrom skrub import selectors as s\n\nvectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder()\n)\nvectorized_products = products.skb.apply(\n    vectorizer, cols=s.all() - \"basket_ID\"\n)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#executing-dataframe-operations",
    "href": "pages/slides/pydata-2025/slides.html#executing-dataframe-operations",
    "title": "Skrub",
    "section": "Executing dataframe operations",
    "text": "Executing dataframe operations\n\naggregated_products = vectorized_products.groupby(\n    \"basket_ID\"\n).agg(\"mean\").reset_index()\n\nfeatures = X.merge(\n    aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n)\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#applying-a-ml-model",
    "href": "pages/slides/pydata-2025/slides.html#applying-a-ml-model",
    "title": "Skrub",
    "section": "Applying a ML model",
    "text": "Applying a ML model\n\nfrom sklearn.ensemble import ExtraTreesClassifier  \npredictions = features.skb.apply(\n    ExtraTreesClassifier(n_jobs=-1), y=y\n)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#inspecting-the-data-ops-plan",
    "href": "pages/slides/pydata-2025/slides.html#inspecting-the-data-ops-plan",
    "title": "Skrub",
    "section": "Inspecting the Data Ops plan",
    "text": "Inspecting the Data Ops plan\npredictions.skb.full_report()\n\nExecution report\nEach node:\n\nShows a preview of the data resulting from the operation\nReports the location in the code where the code is defined\nShows the run time of the node\n\nThe plan is exported as HTML + JS (no need for kernels)."
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/pydata-2025/slides.html#exporting-the-plan-in-a-learner",
    "title": "Skrub",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe Learner is a stand-alone object that works like a scikit-learn estimator that takes a dictionary as input rather than just X and y.\n\n\nlearner = predictions.skb.make_learner(fitted=True)\n\n\n\nThen, the learner can be pickled ‚Ä¶\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\n‚Ä¶ loaded and applied to new data:\nwith open(\"learner.bin\", \"rb\") as fp:\n    loaded_learner = pickle.load(fp)\ndata = skrub.datasets.fetch_credit_fraud(split=\"test\")\nnew_baskets = data.baskets\nnew_products = data.products\nloaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})\n\n\narray([0, 0, 0, ..., 0, 0, 0], shape=(31549,))"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#hyperparameter-tuning-in-a-data-plan",
    "href": "pages/slides/pydata-2025/slides.html#hyperparameter-tuning-in-a-data-plan",
    "title": "Skrub",
    "section": "Hyperparameter tuning in a Data Plan",
    "text": "Hyperparameter tuning in a Data Plan\nSkrub implements four choose_* functions:\n\nchoose_from: select from the given list of options\nchoose_int: select an integer within a range\nchoose_float: select a float within a range\nchoose_bool: select a bool\noptional: chooses whether to execute the given operation"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/pydata-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "Skrub",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#tuning-with-dataops-is-simple",
    "href": "pages/slides/pydata-2025/slides.html#tuning-with-dataops-is-simple",
    "title": "Skrub",
    "section": "Tuning with DataOps is simple!",
    "text": "Tuning with DataOps is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#run-hyperparameter-search",
    "href": "pages/slides/pydata-2025/slides.html#run-hyperparameter-search",
    "title": "Skrub",
    "section": "Run hyperparameter search",
    "text": "Run hyperparameter search\n# fit the search \nsearch = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True, cv=5)\n\n# save the best learner\nbest_learner = search.best_learner_"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#tuning-with-dataops-is-not-limited-to-estimators",
    "href": "pages/slides/pydata-2025/slides.html#tuning-with-dataops-is-not-limited-to-estimators",
    "title": "Skrub",
    "section": "Tuning with DataOps is not limited to estimators",
    "text": "Tuning with DataOps is not limited to estimators\n\nPandasPolars\n\n\n\ndf = pd.DataFrame(\n    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n)\n\ndf_do = skrub.var(\"grades\", df)\n\nagg_grades = df_do.groupby(\"subject\").agg(skrub.choose_from([\"count\", \"mean\"]))\nagg_grades.skb.describe_param_grid()\n\n\"- choose_from(['count', 'mean']): ['count', 'mean']\\n\"\n\n\n\n\n\ndf = pl.DataFrame(\n    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n)\n\ndf_do = skrub.var(\"grades\", df)\n\nagg_grades = df_do.group_by(\"subject\").agg(\n    skrub.choose_from([pl.mean(\"grade\"), pl.count(\"grade\")])\n)\nagg_grades.skb.describe_param_grid()\n\n'- choose_from([&lt;Expr [\\'col(\"grade\").mean()\\'] at 0x71FCE93BFD10&gt;, &lt;Expr [\\'col(\"grade\").count()\\'] at 0x71FCE9407210&gt;]): [&lt;Expr [\\'col(\"grade\").mean()\\'] at 0x71FCE93BFD10&gt;, &lt;Expr [\\'col(\"grade\").count()\\'] at 0x71FCE9407210&gt;]\\n'"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#a-parallel-coordinate-plot-to-explore-hyperparameters",
    "href": "pages/slides/pydata-2025/slides.html#a-parallel-coordinate-plot-to-explore-hyperparameters",
    "title": "Skrub",
    "section": "A parallel coordinate plot to explore hyperparameters",
    "text": "A parallel coordinate plot to explore hyperparameters\nsearch = pred.skb.get_randomized_search(fitted=True)\nsearch.plot_parallel_coord()\n\n\n                            \n                                            \n\n\nsource"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#more-information-about-the-data-ops",
    "href": "pages/slides/pydata-2025/slides.html#more-information-about-the-data-ops",
    "title": "Skrub",
    "section": "More information about the Data Ops",
    "text": "More information about the Data Ops\n\nSkrub example gallery\nSkrub user guide\nTutorial on timeseries forecasting at Euroscipy 2025\nKaggle notebook on the Titanic survival challenge"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#getting-involved",
    "href": "pages/slides/pydata-2025/slides.html#getting-involved",
    "title": "Skrub",
    "section": "Getting involved",
    "text": "Getting involved\nDo you want to learn more?\n\nSkrub website\nSkrub materials website\nDiscord server\n\nFollow skrub on:\n\nBluesky\nLinkedIn\n\nStar skrub on GitHub, or contribute directly:\n\nGitHub repository"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#sprint-on-thursday",
    "href": "pages/slides/pydata-2025/slides.html#sprint-on-thursday",
    "title": "Skrub",
    "section": "Sprint on Thursday!!!",
    "text": "Sprint on Thursday!!!"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#were-hiring",
    "href": "pages/slides/pydata-2025/slides.html#were-hiring",
    "title": "Skrub",
    "section": "We‚Äôre hiring!!",
    "text": "We‚Äôre hiring!!\nCome talk to me or go to the P16 booth"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#tldw-skrub",
    "href": "pages/slides/pydata-2025/slides.html#tldw-skrub",
    "title": "Skrub",
    "section": "tl;dw: skrub",
    "text": "tl;dw: skrub\n\ninteractive data exploration: TableReport\nautomated pre-processing of pandas and polars dataframes: Cleaner\npowerful feature engineering: TableVectorizer, tabular_pipeline\ncolumn- and dataframe-level operations: ApplyToCols, selectors\nDataOps, plans, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#fine-grained-column-transformations-with-applytocols",
    "href": "pages/slides/pydata-2025/slides.html#fine-grained-column-transformations-with-applytocols",
    "title": "Skrub",
    "section": "Fine-grained column transformations with ApplyToCols",
    "text": "Fine-grained column transformations with ApplyToCols\n\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])"
  },
  {
    "objectID": "pages/slides/pydata-2025/slides.html#fine-grained-column-transformations-with-applytocols-1",
    "href": "pages/slides/pydata-2025/slides.html#fine-grained-column-transformations-with-applytocols-1",
    "title": "Skrub",
    "section": "Fine-grained column transformations with ApplyToCols",
    "text": "Fine-grained column transformations with ApplyToCols\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ntext_bar\ntext_baz\ntext_foo\nnumber\n\n\n\n\n0\n0.0\n0.0\n1.0\n-1.224745\n\n\n1\n1.0\n0.0\n0.0\n0.000000\n\n\n2\n0.0\n1.0\n0.0\n1.224745"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#plan-for-the-presentation",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#plan-for-the-presentation",
    "title": "A Skrub use case in academia",
    "section": "Plan for the presentation",
    "text": "Plan for the presentation\n\nContext and explanation of the problem\nThe Retrieve, Merge, Predict pipeline\nHow is this relevant?"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-1",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-2",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-2",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-3",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-3",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-definitions",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-definitions",
    "title": "A Skrub use case in academia",
    "section": "Some definitions",
    "text": "Some definitions\n\nBase table: the table we want to augment (‚ÄúMovies‚Äù, ‚ÄúHousing‚Äù‚Ä¶)\nQuery column: a column that should be used as join key (‚ÄúMovie title‚Äù, ‚ÄúAddress‚Äù‚Ä¶)\nData lake: an unstructured repository of many (thousands of‚Ä¶) ‚Äúcandidate tables‚Äù\nCandidate table: a table that may be useful for augmenting the base table (‚ÄúMovie directors‚Äù‚Ä¶)\nCandidate column: a column in a candidate table that could be joined on the query column (‚ÄúTitle‚Äù in a table about filmographies)\nAugmented table: the result of joining the base table and a candidate table\n\n\n\n\n\n\n\n\nWarning\n\n\nThis terminology is slightly different from that used in the Skrub documentation"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#jaccard-containment",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#jaccard-containment",
    "title": "A Skrub use case in academia",
    "section": "Jaccard Containment",
    "text": "Jaccard Containment\n\n\nJaccard Similarity: \\(\\frac{|Q \\cap X|}{|Q \\cup X|}\\)\nJaccard Containment: \\(\\frac{|Q \\cap X|}{|Q|}\\)\n\n\nJaccard containment is a ‚Äúnormalized‚Äù intersection:\n\n\n\n\n\n\nImportant\n\n\nWhat fraction of of query set Q is in candidate column X?\n\n\n\n\n\n\n\n\nhttps://ekzhu.com/datasketch/lshensemble.html"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#the-focus-of-the-study",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#the-focus-of-the-study",
    "title": "A Skrub use case in academia",
    "section": "The focus of the study:",
    "text": "The focus of the study:\n\nFind the best way to discover candidates.\nWork within a defined computational budget\nWork with exact joins between a base table and multiple join candidates.\nGuarantee that results are reproducible."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#we-do-not-consider",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#we-do-not-consider",
    "title": "A Skrub use case in academia",
    "section": "We do not consider:",
    "text": "We do not consider:\n\nEntity matching or fuzzy joins (e.g., matching ‚ÄúNYT‚Äù and ‚ÄúThe New York Times‚Äù).\nDiscovering the query column.\nMulti-key joins"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#working-with-data-lakes-is-hard",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#working-with-data-lakes-is-hard",
    "title": "A Skrub use case in academia",
    "section": "Working with data lakes is hard",
    "text": "Working with data lakes is hard\n\nSome CSVs don‚Äôt use commas\nSome CSVs have no (known) schema\nSome CSVs aren‚Äôt CSVs, they‚Äôre actually JSON files in disguise\nSome JSONs aren‚Äôt JSONs, they‚Äôre actually strings in disguise\n\n\nIf you don‚Äôt know the tables, everyone is sus"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#pipeline-schema",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#pipeline-schema",
    "title": "A Skrub use case in academia",
    "section": "Pipeline schema",
    "text": "Pipeline schema"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-retrieval",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-retrieval",
    "title": "A Skrub use case in academia",
    "section": "Candidate retrieval",
    "text": "Candidate retrieval\n\nExact Matching: measure the exact Jaccard containment (JC) for each column in the data lake.\nMinHash: estimate the Jaccard containment, query to get columns with a JC larger than a threshold.\nHybrid MinHash: query with MinHash, then measure the exact JC for the retrieved candidates.\nStarmie : use a language model to query candidate columns."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-selection",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-selection",
    "title": "A Skrub use case in academia",
    "section": "Candidate selection",
    "text": "Candidate selection\n\nHighest Containment Join: Rank candidates by Jaccard Containment.\nFull Join: Join all candidates.\nBest Single Join: Train a model on each candidate, select the best.\nStepwise Greedy Join: Like Best Single Join, but keep all good candidates."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation",
    "title": "A Skrub use case in academia",
    "section": "Aggregation",
    "text": "Aggregation"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation-1",
    "title": "A Skrub use case in academia",
    "section": "Aggregation",
    "text": "Aggregation\n\nAny: take one value at random from each group\nMean: for each group, take the mean of numerical values and mode of categorical values\nDeep Feature Synthesis (DFS): greedily generate new features (count, mean, sum‚Ä¶) to already present features."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#prediction",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#prediction",
    "title": "A Skrub use case in academia",
    "section": "Prediction",
    "text": "Prediction\n\nRidgeCV: linear baseline üìà\nCatBoost: GDBT üå≤\nResNet: Neural Networks üß†\nRealMLP: Neural Networks üß†"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-experimental-results",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-experimental-results",
    "title": "A Skrub use case in academia",
    "section": "Some experimental results",
    "text": "Some experimental results"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#total-compute-time",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#total-compute-time",
    "title": "A Skrub use case in academia",
    "section": "Total compute time",
    "text": "Total compute time\n\n\n\nML Model\nPlatform\nTotal compute time\n\n\n\n\nRidgeCV\nCPU\n4y 3m 10d 7h\n\n\nCatBoost\nCPU\n1y 3m 29d 21h\n\n\nResNet\nGPU\n5y 6m 23d 0h\n\n\nRealMLP\nGPU\n10y 7m 23d 3h\n\n\nTotal\nBoth\n21y 9m 26d 8h"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#features-of-research-code",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#features-of-research-code",
    "title": "A Skrub use case in academia",
    "section": "‚ÄúFeatures‚Äù of research code",
    "text": "‚ÄúFeatures‚Äù of research code\nResearch code‚Ä¶\n\nIs mostly custom-made for a specific experiment\nFeatures little to no testing\nOften is poorly documented, or not at all\nInvolves a lot of technical debt"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrub-to-the-rescue",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrub-to-the-rescue",
    "title": "A Skrub use case in academia",
    "section": "Skrub to the rescue",
    "text": "Skrub to the rescue\n\nWell tested code\nGood documentation\nFeatures cover much of the pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#using-skrub-features-in-the-pipeline",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#using-skrub-features-in-the-pipeline",
    "title": "A Skrub use case in academia",
    "section": "Using Skrub features in the pipeline",
    "text": "Using Skrub features in the pipeline\n\nThe Discover object can replace (part of) the retrieval step.\nAll the code for joining can be replaced by the AggJoiner or MultiAggJoiner.\nThe MultiAggJoiner is an additional baseline.\nThe TableVectorizer can handle automated preprocessing of the tables.\nJoined candidates can be examined quickly using the TableReport."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline",
    "title": "A Skrub use case in academia",
    "section": "‚ÄúSkrubified‚Äù pipeline",
    "text": "‚ÄúSkrubified‚Äù pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline-1",
    "title": "A Skrub use case in academia",
    "section": "‚ÄúSkrubified‚Äù pipeline",
    "text": "‚ÄúSkrubified‚Äù pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-with-multiaggjoiner",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-with-multiaggjoiner",
    "title": "A Skrub use case in academia",
    "section": "Example with MultiAggJoiner",
    "text": "Example with MultiAggJoiner\n\n\nmerged = source_table.clone()\nhashes = []\nfor hash_, mdata in tqdm(\n    index_cand.items(),\n    total=len(index_cand),\n    leave=False,\n    desc=\"Full Join\",\n    position=2,\n):\n    cnd_md = mdata.candidate_metadata\n    hashes.append(cnd_md[\"hash\"])\n    candidate_table = pl.read_parquet(cnd_md[\"full_path\"])\n\n    left_on = mdata.left_on\n    right_on = mdata.right_on\n\n    aggr_right = aggregate_table(\n        candidate_table, right_on, aggregation_method=aggregation\n    )\n\n    merged = execute_join(\n        merged,\n        aggr_right,\n        left_on=left_on,\n        right_on=right_on,\n        how=\"left\",\n        suffix=\"_\" + hash_[:10],\n    )\n\n# MOCK-UP\nfrom skrub import MultiAggJoiner\njoiner = MultiAggJoiner(candidate_tables, keys=candidate_keys)\nmerged = joiner.fit_transform(source_table)"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#repositories",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#repositories",
    "title": "A Skrub use case in academia",
    "section": "Repositories",
    "text": "Repositories\n\nRetrieve, Merge, Predict website\nRetrieve, Merge, Predict repository"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#acknowledgements",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#acknowledgements",
    "title": "A Skrub use case in academia",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\n\n\n\n\n\nAuthors:\n\nRiccardo Cappuzzo (SODA, Dataiku)\nAimee Coelho (Dataiku)\nFelix Lefebvre (SODA)\nPaolo Papotti (Eurecom)\nGael Varoquaux (SODA)"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#conclusions-and-summary",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#conclusions-and-summary",
    "title": "A Skrub use case in academia",
    "section": "Conclusions and summary",
    "text": "Conclusions and summary\n\n\n\n\n\nTree-based models are more effective and more resilient than the alternatives\nGood table retrieval affects the whole pipeline\nSimple methods produce results comparable or even better than more complex methods\n\n\n\n\nSkrub provides well-tested, well-documented code\nSkrub objects provide features that cover most of the pipeline\nIn return, the pipeline helped deciding on relevant features."
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#fun-facts",
    "href": "pages/slides/soda-kickoff/slides.html#fun-facts",
    "title": "SODA Kickoff 2025",
    "section": "Fun facts",
    "text": "Fun facts\n\nI‚Äôm Italian, but I don‚Äôt drink coffee, wine, and I like pizza with fries\nI did my PhD in C√¥te d‚ÄôAzur, and I moved away because it was too sunny and I don‚Äôt like the sea"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#fun-facts-1",
    "href": "pages/slides/soda-kickoff/slides.html#fun-facts-1",
    "title": "SODA Kickoff 2025",
    "section": "Fun facts",
    "text": "Fun facts\n\nI‚Äôm mildly obsessed with matplotlib"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#an-example-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#an-example-pipeline",
    "title": "SODA Kickoff 2025",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPre-process the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data",
    "title": "SODA Kickoff 2025",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data-1",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data-1",
    "title": "SODA Kickoff 2025",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data-with-skrub",
    "title": "SODA Kickoff 2025",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas",
    "href": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas",
    "title": "SODA Kickoff 2025",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'A': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'C': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'E': [np.nan, np.nan, np.nan],  # All missing values \n    'F': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\ndf = pd.DataFrame(data)\ndisplay(df)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nDate\n\n\n\n\n0\n1\n2\nx\n4\nNaN\n\n01/01/2023\n\n\n1\n1\n3\nx\n5\nNaN\n\n02/01/2023\n\n\n2\n1\n2\nx\n6\nNaN\n\n03/01/2023"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas-1",
    "href": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas-1",
    "title": "SODA Kickoff 2025",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\n# Parse the datetime strings with a specific format\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n\n# Drop columns with only a single unique value\ndf_cleaned = df.loc[:, df.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_cleaned = drop_empty_columns(df_cleaned)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#lightweight-data-cleaning-cleaner",
    "href": "pages/slides/soda-kickoff/slides.html#lightweight-data-cleaning-cleaner",
    "title": "SODA Kickoff 2025",
    "section": "Lightweight data cleaning: Cleaner",
    "text": "Lightweight data cleaning: Cleaner\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas",
    "title": "SODA Kickoff 2025",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf = pd.DataFrame(data)\ndf_expanded = df.copy()\ndatetime_column = \"date\"\ndf_expanded[datetime_column] = pd.to_datetime(df_expanded[datetime_column], errors='coerce')\n\ndf_expanded['year'] = df_expanded[datetime_column].dt.year\ndf_expanded['month'] = df_expanded[datetime_column].dt.month\ndf_expanded['day'] = df_expanded[datetime_column].dt.day\ndf_expanded['hour'] = df_expanded[datetime_column].dt.hour\ndf_expanded['minute'] = df_expanded[datetime_column].dt.minute\ndf_expanded['second'] = df_expanded[datetime_column].dt.second"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas-1",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas-1",
    "title": "SODA Kickoff 2025",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\ndf_expanded['hour_sin'] = np.sin(2 * np.pi * df_expanded['hour'] / 24)\ndf_expanded['hour_cos'] = np.cos(2 * np.pi * df_expanded['hour'] / 24)\n\ndf_expanded['month_sin'] = np.sin(2 * np.pi * df_expanded['month'] / 12)\ndf_expanded['month_cos'] = np.cos(2 * np.pi * df_expanded['month'] / 12)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nDataFrame with expanded datetime columns:\")\nprint(df_expanded)\n\nOriginal DataFrame:\n                  date  value\n0  2023-01-01 12:34:56     10\n1  2023-02-15 08:45:23     20\n2  2023-03-20 18:12:45     30\n\nDataFrame with expanded datetime columns:\n                 date  value  year  month  day  hour  minute  second  \\\n0 2023-01-01 12:34:56     10  2023      1    1    12      34      56   \n1 2023-02-15 08:45:23     20  2023      2   15     8      45      23   \n2 2023-03-20 18:12:45     30  2023      3   20    18      12      45   \n\n       hour_sin      hour_cos  month_sin     month_cos  \n0  1.224647e-16 -1.000000e+00   0.500000  8.660254e-01  \n1  8.660254e-01 -5.000000e-01   0.866025  5.000000e-01  \n2 -1.000000e+00 -1.836970e-16   1.000000  6.123234e-17"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "title": "SODA Kickoff 2025",
    "section": "Encoding datetime features skrub.DatetimeEncoder",
    "text": "Encoding datetime features skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\nX_date = ToDatetime().fit_transform(df[\"date\"])\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\n   date_year  date_total_seconds  date_month_circular_0  \\\n0     2023.0        1.672577e+09               0.500000   \n1     2023.0        1.676451e+09               0.866025   \n2     2023.0        1.679336e+09               1.000000   \n\n   date_month_circular_1  date_day_circular_0  date_day_circular_1  \\\n0           8.660254e-01         2.079117e-01             0.978148   \n1           5.000000e-01         1.224647e-16            -1.000000   \n2           6.123234e-17        -8.660254e-01            -0.500000   \n\n   date_hour_circular_0  date_hour_circular_1  \n0          1.224647e-16         -1.000000e+00  \n1          8.660254e-01         -5.000000e-01  \n2         -1.000000e+00         -1.836970e-16  \n\n\n}"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "SODA Kickoff 2025",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline",
    "title": "SODA Kickoff 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-1",
    "title": "SODA Kickoff 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-2",
    "title": "SODA Kickoff 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "title": "SODA Kickoff 2025",
    "section": "Build a predictive pipeline with tabular_learner",
    "text": "Build a predictive pipeline with tabular_learner\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_learner(Ridge())\nmodel\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('tablevectorizer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_threshold¬†\n40\n\n\n\nlow_cardinality¬†\nOneHotEncoder..._output=False)\n\n\n\nhigh_cardinality¬†\nStringEncoder()\n\n\n\nnumeric¬†\nPassThrough()\n\n\n\ndatetime¬†\nDatetimeEncod...ding='spline')\n\n\n\nspecific_transformers¬†\n()\n\n\n\ndrop_null_fraction¬†\n1.0\n\n\n\ndrop_if_constant¬†\nFalse\n\n\n\ndrop_if_unique¬†\nFalse\n\n\n\ndatetime_format¬†\nNone\n\n\n\nn_jobs¬†\nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolution¬†\n'hour'\n\n\n\nadd_weekday¬†\nFalse\n\n\n\nadd_total_seconds¬†\nTrue\n\n\n\nadd_day_of_year¬†\nFalse\n\n\n\nperiodic_encoding¬†\n'spline'\n\n\n\n\n            \n        \n    low_cardinalityOneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories¬†\n'auto'\n\n\n\ndrop¬†\n'if_binary'\n\n\n\nsparse_output¬†\nFalse\n\n\n\ndtype¬†\n'float32'\n\n\n\nhandle_unknown¬†\n'ignore'\n\n\n\nmin_frequency¬†\nNone\n\n\n\nmax_categories¬†\nNone\n\n\n\nfeature_name_combiner¬†\n'concat'\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_components¬†\n30\n\n\n\nvectorizer¬†\n'tfidf'\n\n\n\nngram_range¬†\n(3, ...)\n\n\n\nanalyzer¬†\n'char_wb'\n\n\n\nstop_words¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values¬†\nnan\n\n\n\nstrategy¬†\n'mean'\n\n\n\nfill_value¬†\nNone\n\n\n\ncopy¬†\nTrue\n\n\n\nadd_indicator¬†\nTrue\n\n\n\nkeep_empty_features¬†\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    Ridge?Documentation for Ridge\n        \n            \n                Parameters\n                \n\n\n\n\nalpha¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\ncopy_X¬†\nTrue\n\n\n\nmax_iter¬†\nNone\n\n\n\ntol¬†\n0.0001\n\n\n\nsolver¬†\n'auto'\n\n\n\npositive¬†\nFalse\n\n\n\nrandom_state¬†\nNone"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#we-now-have-a-pipeline",
    "title": "SODA Kickoff 2025",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\n\nskrub.datasets, or user data\n\nExplore the data\n\nskrub.TableReport\n\nPre-process the data\n\nskrub.TableVectorizer, Cleaner, DatetimeEncoder ‚Ä¶\n\nPerform feature engineering\n\nskrub.TableVectorizer, TextEncoder, StringEncoder‚Ä¶\n\nBuild a scikit-learn pipeline\n\ntabular_learner, sklearn.pipeline.make_pipeline ‚Ä¶\n\n???\nProfit üìà"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#skrub-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#skrub-expressions",
    "title": "SODA Kickoff 2025",
    "section": "skrub expressions",
    "text": "skrub expressions\nWhen a normal pipe is not enough‚Ä¶\n\nExpressions come to the rescue üöí:\n\n\nKeep track of train, validation and test splits to avoid data leakage\nSimplify hyperparameter tuning and reporting\nHandle complex pipelines that involve multiple tables and custom\nPersist all objects for reproducibility"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#starting-with-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#starting-with-expressions",
    "title": "SODA Kickoff 2025",
    "section": "Starting with expressions",
    "text": "Starting with expressions\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = skrub.X(data.baskets[[\"ID\"]]) # mark as \"X\"\ny = skrub.y(data.baskets[\"fraud_flag\"]) # mark as \"y\"\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX, y, products represent inputs to the pipeline\nskrub keeps track of splits"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-and-inspect-complex-pipelines",
    "href": "pages/slides/soda-kickoff/slides.html#build-and-inspect-complex-pipelines",
    "title": "SODA Kickoff 2025",
    "section": "Build and inspect complex pipelines",
    "text": "Build and inspect complex pipelines\npred.skb.full_report()\n\nreport"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-in-scikit-learn",
    "href": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-in-scikit-learn",
    "title": "SODA Kickoff 2025",
    "section": "Hyperparameter tuning in scikit-learn",
    "text": "Hyperparameter tuning in scikit-learn\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-with-skrub-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-with-skrub-expressions",
    "title": "SODA Kickoff 2025",
    "section": "Hyperparameter tuning with skrub expressions",
    "text": "Hyperparameter tuning with skrub expressions\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)\nregressor.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#observe-the-impact-of-the-hyperparameters",
    "href": "pages/slides/soda-kickoff/slides.html#observe-the-impact-of-the-hyperparameters",
    "title": "SODA Kickoff 2025",
    "section": "Observe the impact of the hyperparameters",
    "text": "Observe the impact of the hyperparameters\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#tldw",
    "href": "pages/slides/soda-kickoff/slides.html#tldw",
    "title": "SODA Kickoff 2025",
    "section": "tl;dw",
    "text": "tl;dw\nskrub provides\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering\nsoon‚Ñ¢Ô∏è, complex pipelines, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#getting-involved",
    "href": "pages/slides/soda-kickoff/slides.html#getting-involved",
    "title": "SODA Kickoff 2025",
    "section": "Getting involved",
    "text": "Getting involved\n\nSkrub website (QR code below!)\nSkrub materials website\nGit repository\nDiscord server\nBluesky"
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object",
    "href": "pages/slides/discover/discover.html#the-discover-object",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object"
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object-1",
    "href": "pages/slides/discover/discover.html#the-discover-object-1",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object"
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object-2",
    "href": "pages/slides/discover/discover.html#the-discover-object-2",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object\n\nGive a high level overview of the content of a data lake\nBuild an (approximate) schema of the data\nSuggest tables that are relevant to what the user provides"
  },
  {
    "objectID": "pages/slides/discover/discover.html#planned-features",
    "href": "pages/slides/discover/discover.html#planned-features",
    "title": "Discover object",
    "section": "Planned features",
    "text": "Planned features\n\nIf no query is provided:\n\n\nGiven a collection of tables, profile them and produce aggregated statistics.\nDtypes, null values, shape of the tables.\n\n\nIf a query table is provided\n\n\nMeasure various pairwise metrics between columns in the query table, and the columns in the collection of tables.\nRank the columns based on the metrics to find those that are most relevant.\nJaccard containment will be the first metric.\nStatistics remain available to perform feature selection."
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover\n\npath_to_tables = \"./many_tables/\"\ndiscover = Discover(path_to_tables)\n\ndataframe_stats = discover.fit_transform()"
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code-1",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code-1",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover\nimport pandas as pd\n\npath_to_tables = \"./many_tables/\"\nquery_table = pd.read_csv(\"this_table.csv\")\n\ndiscover = Discover(path_to_tables)\n\nranking_by_column = discover.fit_transform(query_table)"
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code-2",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code-2",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover, MultiAggJoiner\nimport pandas as pd\n\npath_to_tables = \"./many_tables/\"\nquery_table = pd.read_csv(\"this_table.csv\")\n\ndiscover = Discover(path_to_tables)\n\nranking_by_column = discover.fit_transform(query_table)\n\njoiner = MultiAggJoiner(ranking_by_column)\njoined_table = joiner.fit_transform(query_table)"
  },
  {
    "objectID": "pages/slides/discover/discover.html#interface-with-the-data",
    "href": "pages/slides/discover/discover.html#interface-with-the-data",
    "title": "Discover object",
    "section": "Interface with the data",
    "text": "Interface with the data\n\nThe initial implementation will read from a path/glob\nLater version will target SQL databases\nWhat other technologies should we consider?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skrub learning materials index",
    "section": "",
    "text": "Star\n\n\n\n\n\n\nNote\n\n\n\nPyData 2025 Slides:\nFind the slides for the PyData 2025 talk here.\n\n\nThis is the Skrub learning materials website: this is where you‚Äôll be able to find all the material that is used to teach and present Skrub to audiences.\nSlides for presentations, talks and lectures are in slides section. Notebooks and blog posts are instead located in the notebook section.\nNote that the material that is gathered here may become obsolete depending on the development of Skrub.\nFor up-to-date information on Skrub and its API refer to the documentation, while\nadditional examples are available in the main gallery. Material on the main website is constantly tested with the latest version of Skrub."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#plan-for-the-presentation",
    "href": "pages/slides/skrub-intro/skrub-intro.html#plan-for-the-presentation",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Plan for the presentation",
    "text": "Plan for the presentation\n\nIntroducing skrub\n\nExample use case\nDetailed explanation of the features\nGetting involved"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#in-the-beginning",
    "href": "pages/slides/skrub-intro/skrub-intro.html#in-the-beginning",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "In the beginning‚Ä¶",
    "text": "In the beginning‚Ä¶\nSkrub stems from the development of dirty_cat, a package that provided support for handling dirty columns and perform fuzzy joins across tables.\n\nIt has since evolved into a package that provides:\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#skrubs-vision",
    "href": "pages/slides/skrub-intro/skrub-intro.html#skrubs-vision",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Skrub‚Äôs vision",
    "text": "Skrub‚Äôs vision\nThe goal of skrub is to facilitate building and deploying machine-learning models on pandas and polars dataframes (later, SQL databases‚Ä¶)\n\n\n\nSkrub is high-level, with a philosophy and an API matching that of scikit-learn. It strives to bridge the worlds of databases and machine-learning, enabling imperfect assembly and representations of the data when it is noisy."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#an-example-use-case",
    "href": "pages/slides/skrub-intro/skrub-intro.html#an-example-use-case",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "An example use case",
    "text": "An example use case\n\nGather some data\n\nEmployee salaries, census, customer churn‚Ä¶\n\nExplore the data\n\nNull values, dtypes, correlated features‚Ä¶\n\nPre-process the data\nBuild a scikit-learn estimator\n???\nProfit üìà"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data",
    "href": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nimport skrub\nimport pandas as pd\nfrom skrub.datasets import fetch_employee_salaries\n\ndataset = fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\nemployees.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data-interactively",
    "href": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data-interactively",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Exploring the data‚Ä¶ interactively!",
    "text": "Exploring the data‚Ä¶ interactively!\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\n\nObtain high-level statistics about the data (number of uniques, missing values‚Ä¶)\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\nMore examples here"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-2",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nmodel = make_pipeline(StandardScaler(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-3",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(SimpleImputer(), StandardScaler(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-4",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-5",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-5",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-6",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-6",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['year_first_hired']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['gender', 'department',\n                                                   'department_name',\n                                                   'division',\n                                                   'assignment_category',\n                                                   'employee_position_title',\n                                                   'date_first_hired'])])),\n                ('simpleimputer', SimpleImputer()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('columntransformer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers¬†\n[('standardscaler', ...), ('onehotencoder', ...)]\n\n\n\nremainder¬†\n'drop'\n\n\n\nsparse_threshold¬†\n0.3\n\n\n\nn_jobs¬†\nNone\n\n\n\ntransformer_weights¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\nverbose_feature_names_out¬†\nTrue\n\n\n\nforce_int_remainder_cols¬†\n'deprecated'\n\n\n\n\n            \n        \n    standardscaler['year_first_hired']StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    onehotencoder['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title', 'date_first_hired']OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories¬†\n'auto'\n\n\n\ndrop¬†\nNone\n\n\n\nsparse_output¬†\nTrue\n\n\n\ndtype¬†\n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown¬†\n'ignore'\n\n\n\nmin_frequency¬†\nNone\n\n\n\nmax_categories¬†\nNone\n\n\n\nfeature_name_combiner¬†\n'concat'\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values¬†\nnan\n\n\n\nstrategy¬†\n'mean'\n\n\n\nfill_value¬†\nNone\n\n\n\ncopy¬†\nTrue\n\n\n\nadd_indicator¬†\nFalse\n\n\n\nkeep_empty_features¬†\nFalse\n\n\n\n\n            \n        \n    Ridge?Documentation for Ridge\n        \n            \n                Parameters\n                \n\n\n\n\nalpha¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\ncopy_X¬†\nTrue\n\n\n\nmax_iter¬†\nNone\n\n\n\ntol¬†\n0.0001\n\n\n\nsolver¬†\n'auto'\n\n\n\npositive¬†\nFalse\n\n\n\nrandom_state¬†\nNone"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Enter: tabular_learner",
    "text": "Enter: tabular_learner\nimport skrub\nfrom sklearn.linear_model import Ridge\ntl = skrub.tabular_learner(Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Enter: tabular_learner",
    "text": "Enter: tabular_learner\n\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('tablevectorizer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_threshold¬†\n40\n\n\n\nlow_cardinality¬†\nOneHotEncoder..._output=False)\n\n\n\nhigh_cardinality¬†\nStringEncoder()\n\n\n\nnumeric¬†\nPassThrough()\n\n\n\ndatetime¬†\nDatetimeEncod...ding='spline')\n\n\n\nspecific_transformers¬†\n()\n\n\n\ndrop_null_fraction¬†\n1.0\n\n\n\ndrop_if_constant¬†\nFalse\n\n\n\ndrop_if_unique¬†\nFalse\n\n\n\ndatetime_format¬†\nNone\n\n\n\nn_jobs¬†\nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolution¬†\n'hour'\n\n\n\nadd_weekday¬†\nFalse\n\n\n\nadd_total_seconds¬†\nTrue\n\n\n\nadd_day_of_year¬†\nFalse\n\n\n\nperiodic_encoding¬†\n'spline'\n\n\n\n\n            \n        \n    low_cardinalityOneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories¬†\n'auto'\n\n\n\ndrop¬†\n'if_binary'\n\n\n\nsparse_output¬†\nFalse\n\n\n\ndtype¬†\n'float32'\n\n\n\nhandle_unknown¬†\n'ignore'\n\n\n\nmin_frequency¬†\nNone\n\n\n\nmax_categories¬†\nNone\n\n\n\nfeature_name_combiner¬†\n'concat'\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_components¬†\n30\n\n\n\nvectorizer¬†\n'tfidf'\n\n\n\nngram_range¬†\n(3, ...)\n\n\n\nanalyzer¬†\n'char_wb'\n\n\n\nstop_words¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values¬†\nnan\n\n\n\nstrategy¬†\n'mean'\n\n\n\nfill_value¬†\nNone\n\n\n\ncopy¬†\nTrue\n\n\n\nadd_indicator¬†\nTrue\n\n\n\nkeep_empty_features¬†\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    Ridge?Documentation for Ridge\n        \n            \n                Parameters\n                \n\n\n\n\nalpha¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\ncopy_X¬†\nTrue\n\n\n\nmax_iter¬†\nNone\n\n\n\ntol¬†\n0.0001\n\n\n\nsolver¬†\n'auto'\n\n\n\npositive¬†\nFalse\n\n\n\nrandom_state¬†\nNone"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#a-robust-baseline-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#a-robust-baseline-tabular_learner",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "A robust baseline: tabular_learner",
    "text": "A robust baseline: tabular_learner\nGiven a scikit-learn estimator, tabular_learner:\n\nextracts numerical features\nimputes missing values with SimpleImputer (optional)\nscales the data with StandardScaler (optional)\n\n\nYou can also write ‚Äútabular_learner(\"regressor\")‚Äù:\n\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('tablevectorizer', ...), ('histgradientboostingregressor', ...)]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_threshold¬†\n40\n\n\n\nlow_cardinality¬†\nToCategorical()\n\n\n\nhigh_cardinality¬†\nStringEncoder()\n\n\n\nnumeric¬†\nPassThrough()\n\n\n\ndatetime¬†\nDatetimeEncoder()\n\n\n\nspecific_transformers¬†\n()\n\n\n\ndrop_null_fraction¬†\n1.0\n\n\n\ndrop_if_constant¬†\nFalse\n\n\n\ndrop_if_unique¬†\nFalse\n\n\n\ndatetime_format¬†\nNone\n\n\n\nn_jobs¬†\nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolution¬†\n'hour'\n\n\n\nadd_weekday¬†\nFalse\n\n\n\nadd_total_seconds¬†\nTrue\n\n\n\nadd_day_of_year¬†\nFalse\n\n\n\nperiodic_encoding¬†\nNone\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_components¬†\n30\n\n\n\nvectorizer¬†\n'tfidf'\n\n\n\nngram_range¬†\n(3, ...)\n\n\n\nanalyzer¬†\n'char_wb'\n\n\n\nstop_words¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\n\n            \n        \n    HistGradientBoostingRegressor?Documentation for HistGradientBoostingRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nloss¬†\n'squared_error'\n\n\n\nquantile¬†\nNone\n\n\n\nlearning_rate¬†\n0.1\n\n\n\nmax_iter¬†\n100\n\n\n\nmax_leaf_nodes¬†\n31\n\n\n\nmax_depth¬†\nNone\n\n\n\nmin_samples_leaf¬†\n20\n\n\n\nl2_regularization¬†\n0.0\n\n\n\nmax_features¬†\n1.0\n\n\n\nmax_bins¬†\n255\n\n\n\ncategorical_features¬†\n'from_dtype'\n\n\n\nmonotonic_cst¬†\nNone\n\n\n\ninteraction_cst¬†\nNone\n\n\n\nwarm_start¬†\nFalse\n\n\n\nearly_stopping¬†\n'auto'\n\n\n\nscoring¬†\n'loss'\n\n\n\nvalidation_fraction¬†\n0.1\n\n\n\nn_iter_no_change¬†\n10\n\n\n\ntol¬†\n1e-07\n\n\n\nverbose¬†\n0\n\n\n\nrandom_state¬†\nNone"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#unmasking-the-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#unmasking-the-tabular_learner",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Unmasking the tabular_learner",
    "text": "Unmasking the tabular_learner"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\n\nPre-process the data\nConvert complex data types (datetimes, text) into numerical features"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\nPre-process the data\n\nEnsure consistent column names\nDetect missing values such as ‚ÄúN/A‚Äù\nDrop empty columns\nCheck and convert dtypes to np.float32\nParse dates, ensuring consistent dtype and timezone\nIdentify which categorical features are low- and high-cardinality"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-2",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-3",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\nConvert complex data types (datetimes, text) into numerical features\n\nEncode dates with DateTimeEncoder\nEncode low-cardinality features (&lt;=30 cat.) with OneHotEncoder\nEncode high-cardinality features (&gt;30 cat.) with:\n\nGapEncoder: Relatively slow, easily interpretable, good quality embeddings. Target encoding and hashing.\nMinHashEncoder: Very fast, somewhat low quality embeddings. Hashing ngrams.\nTextEncoder: Very slow, relies on language models, best solution for text and when context is available.\nStringEncoder: Best trade-off between compute cost and embeddings quality. Tf-idf followed by SVD.\n\n\n\nHigh-cardinality encoders are robust in presence of typos and dirty data."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-4",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\n\nvectorizer = skrub.TableVectorizer()\ntransformed = vectorizer.fit_transform(employees)\nfrom pprint import pprint\n\npprint(vectorizer.column_to_kind_)\n\n{'assignment_category': 'low_cardinality',\n 'date_first_hired': 'datetime',\n 'department': 'low_cardinality',\n 'department_name': 'low_cardinality',\n 'division': 'high_cardinality',\n 'employee_position_title': 'high_cardinality',\n 'gender': 'low_cardinality',\n 'year_first_hired': 'numeric'}\n\n\n\npprint(vectorizer.all_processing_steps_[\"date_first_hired\"])\n\n[CleanNullStrings(),\n DropUninformative(),\n ToDatetime(),\n DatetimeEncoder(),\n {'date_first_hired_day': ToFloat32(), 'date_first_hired_month': ToFloat32(), ...}]"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nimport pandas as pd\n\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/skrub-data/datasets/master\"\n    \"/data/bike-sharing-dataset.csv\"\n)\n# Extract our input data (X) and the target column (y)\ny = data[\"cnt\"]\nX = data[[\"date\", \"holiday\", \"temp\", \"hum\", \"windspeed\", \"weathersit\"]]\n\nX\n\n\n\n\n\n\n\n\ndate\nholiday\ntemp\nhum\nwindspeed\nweathersit\n\n\n\n\n0\n2011-01-01 00:00:00\n0\n0.24\n0.81\n0.0000\n1\n\n\n1\n2011-01-01 01:00:00\n0\n0.22\n0.80\n0.0000\n1\n\n\n2\n2011-01-01 02:00:00\n0\n0.22\n0.80\n0.0000\n1\n\n\n3\n2011-01-01 03:00:00\n0\n0.24\n0.75\n0.0000\n1\n\n\n4\n2011-01-01 04:00:00\n0\n0.24\n0.75\n0.0000\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n17374\n2012-12-31 19:00:00\n0\n0.26\n0.60\n0.1642\n2\n\n\n17375\n2012-12-31 20:00:00\n0\n0.26\n0.60\n0.1642\n2\n\n\n17376\n2012-12-31 21:00:00\n0\n0.26\n0.60\n0.1642\n1\n\n\n17377\n2012-12-31 22:00:00\n0\n0.26\n0.56\n0.1343\n1\n\n\n17378\n2012-12-31 23:00:00\n0\n0.26\n0.65\n0.1343\n1\n\n\n\n\n17379 rows √ó 6 columns"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nfrom pprint import pprint\nfrom skrub import TableVectorizer, DatetimeEncoder\n\ntable_vec_weekday = TableVectorizer(datetime=DatetimeEncoder(add_weekday=True)).fit(X)\npprint(table_vec_weekday.get_feature_names_out())\n\narray(['date_year', 'date_month', 'date_day', 'date_hour',\n       'date_total_seconds', 'date_weekday', 'holiday', 'temp', 'hum',\n       'windspeed', 'weathersit'], dtype='&lt;U18')"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-2",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\npipeline_weekday = make_pipeline(table_vec_weekday, HistGradientBoostingRegressor())\n\ncross_val_score(\n    pipeline_weekday, X, y, scoring=\"neg_mean_squared_error\",\n    cv=TimeSeriesSplit(n_splits=5),\n)\n\narray([ -3694.45159469,  -3180.1148674 , -15183.44808403,  -4824.29173547,\n        -5391.06731737])"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-3",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-4",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\n\n\nTimeseries support in skrub is still in its early stages! Please stay tuned for new developments."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-2",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-3",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values",
    "href": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "AggJoiner: automatically aggregate values",
    "text": "AggJoiner: automatically aggregate values"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "AggJoiner: automatically aggregate values",
    "text": "AggJoiner: automatically aggregate values\n\nimport skrub\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    \"UID\": [28, 32, 28], \n    \"Basket ID\": [1100, 1300, 1400]})\ndf2 = pd.DataFrame({\n    \"Basket ID\": [1100, 1100, 1100, 1300, 1400], \n    \"Product ID\": [\"A521\", \"B695\", \"F221\", \"W214\", \"B695\",], \n    \"Price\": [25, 30, 10, 320, 30]})\n\njoiner = skrub.AggJoiner(df2, operations=\"sum\", key=\"Basket ID\", cols=\"Price\")\njoiner.fit_transform(df1)\n\n\n\n\n\n\n\n\nUID\nBasket ID\nPrice_sum\n\n\n\n\n0\n28\n1100\n65\n\n\n1\n32\n1300\n320\n\n\n2\n28\n1400\n30"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-infer-missing-values",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-infer-missing-values",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "InterpolationJoiner: infer missing values",
    "text": "InterpolationJoiner: infer missing values\n\nimport pandas as pd\n\nfrom skrub.datasets import fetch_flight_delays\n\ndataset = fetch_flight_delays()\nweather = dataset.weather\nweather = weather.sample(100_000, random_state=0, ignore_index=True)\nstations = dataset.stations\nweather = stations.merge(weather, on=\"ID\")[\n    [\"LATITUDE\", \"LONGITUDE\", \"YEAR/MONTH/DAY\", \"TMAX\", \"PRCP\", \"SNOW\"]\n]\nweather[\"YEAR/MONTH/DAY\"] = pd.to_datetime(weather[\"YEAR/MONTH/DAY\"])\n\n\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nYEAR/MONTH/DAY\nTMAX\nPRCP\nSNOW\n\n\n\n\n0\n25.333\n55.517\n2008-11-16\n297.0\nNaN\nNaN\n\n\n1\n25.333\n55.517\n2008-04-12\n333.0\nNaN\nNaN\n\n\n2\n25.255\n55.364\n2008-08-28\n430.0\n0.0\nNaN\n\n\n3\n25.255\n55.364\n2008-02-17\n264.0\n0.0\nNaN\n\n\n4\n25.255\n55.364\n2008-11-25\n291.0\nNaN\nNaN"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "InterpolationJoiner",
    "text": "InterpolationJoiner\n\nfrom skrub import InterpolationJoiner\njoiner = InterpolationJoiner(\n    aux_table,\n    key=[\"LATITUDE\", \"LONGITUDE\", \"YEAR/MONTH/DAY\"],\n    suffix=\"_predicted\",\n).fit(main_table)\njoin = joiner.transform(main_table)\njoin.head()\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nYEAR/MONTH/DAY\nTMAX\nPRCP\nSNOW\nTMAX_predicted\nPRCP_predicted\nSNOW_predicted\n\n\n\n\n0\n25.333\n55.517\n2008-11-16\n297.0\nNaN\nNaN\n253.131219\n10.099097\n-0.001023\n\n\n1\n25.333\n55.517\n2008-04-12\n333.0\nNaN\nNaN\n292.267381\n8.770696\n0.142712\n\n\n2\n25.255\n55.364\n2008-08-28\n430.0\n0.0\nNaN\n330.768383\n22.541307\n-0.053227\n\n\n3\n25.255\n55.364\n2008-02-17\n264.0\n0.0\nNaN\n274.812990\n12.980406\n0.239012\n\n\n4\n25.255\n55.364\n2008-11-25\n291.0\nNaN\nNaN\n260.411121\n6.621378\n-0.014548"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-1",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "InterpolationJoiner",
    "text": "InterpolationJoiner"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-4",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining\n\nJoiner: Perform fuzzy-joining: join columns that contain similar-looking values.\nAggJoiner Aggregate an auxiliary dataframe before joining it on a base dataframe, and create new features that aggregate (sum, mean, mode‚Ä¶) the values in the columns.\nMultiAggJoiner extends AggJoiner to a multi-table scenario.\nInterpolationJoiner Perform an equi-join and estimate what missing rows would contain if they existed in the table.\n\n\nAll Joiner objects are scikit-learn estimators, so they can be used in a Pipeline."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-deduplication",
    "href": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-deduplication",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Additional goodies: deduplication",
    "text": "Additional goodies: deduplication\ndeduplicate misspelled categories\n\nfrom skrub import deduplicate\npprint(duplicated)\ndeduplicate_correspondence = deduplicate(duplicated)\npprint(deduplicate_correspondence.to_dict())\n\n['ulack', 'black', 'black', 'xudte', 'white', 'white']\n{'black': 'black', 'ulack': 'black', 'white': 'white', 'xudte': 'white'}\n\n\nDoc"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-wikipedia-embeddings-as-features",
    "href": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-wikipedia-embeddings-as-features",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Additional goodies: Wikipedia embeddings as features",
    "text": "Additional goodies: Wikipedia embeddings as features\nKEN embeddings capture relational information about all entities in Wikipedia.\nfrom skrub.datasets import fetch_ken_embeddings\nembedding_games = fetch_ken_embeddings(\n    search_types=\"game\",\n    exclude=\"companies|developer\",\n    embedding_table_id=\"games\",\n)\n\nDoc"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#install-skrub",
    "href": "pages/slides/skrub-intro/skrub-intro.html#install-skrub",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Install skrub",
    "text": "Install skrub\n\nBase installation:\n# with pip\npip install skrub -U\n# with conda\nconda install -c conda-forge skrub\n\n\nFor deep learning features such as TextEncoder:\n# with pip\npip install skrub[transformers] -U\n# with conda\nconda install -c conda-forge skrub[transformers]\n\n\nDocumentation"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#join-the-community",
    "href": "pages/slides/skrub-intro/skrub-intro.html#join-the-community",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Join the community",
    "text": "Join the community\n\nSkrub website\nSkrub materials website\nGit repository\nDiscord server\nBluesky"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#contribute-to-skrub",
    "href": "pages/slides/skrub-intro/skrub-intro.html#contribute-to-skrub",
    "title": "Skrub workshop - Introducing Skrub",
    "section": "Contribute to skrub",
    "text": "Contribute to skrub\n\nOpen an issue on GitHub\nCheck out the documentation on how to contribute"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#a-teaser-for-later",
    "href": "pages/slides/talk-edf/slides.html#a-teaser-for-later",
    "title": "EDF Talk: Skrub",
    "section": "A teaser for later‚Ä¶",
    "text": "A teaser for later‚Ä¶\nInspect all the steps of your pipeline: Execution report"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#a-teaser-for-later-1",
    "href": "pages/slides/talk-edf/slides.html#a-teaser-for-later-1",
    "title": "EDF Talk: Skrub",
    "section": "A teaser for later‚Ä¶",
    "text": "A teaser for later‚Ä¶\nExplore your hyperparameter search space"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#skrub-compatibility",
    "href": "pages/slides/talk-edf/slides.html#skrub-compatibility",
    "title": "EDF Talk: Skrub",
    "section": "skrub compatibility",
    "text": "skrub compatibility\n\nskrub is fully compatible with pandas and polars\nskrub transformers are fully compatible with scikit-learn"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#an-example-pipeline",
    "href": "pages/slides/talk-edf/slides.html#an-example-pipeline",
    "title": "EDF Talk: Skrub",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPreprocess the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#exploring-the-data",
    "href": "pages/slides/talk-edf/slides.html#exploring-the-data",
    "title": "EDF Talk: Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#exploring-the-data-1",
    "href": "pages/slides/talk-edf/slides.html#exploring-the-data-1",
    "title": "EDF Talk: Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/talk-edf/slides.html#exploring-the-data-with-skrub",
    "title": "EDF Talk: Skrub",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\n\nMore examples"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#data-cleaning-with-pandaspolars-setup",
    "href": "pages/slides/talk-edf/slides.html#data-cleaning-with-pandaspolars-setup",
    "title": "EDF Talk: Skrub",
    "section": "Data cleaning with pandas/polars: setup",
    "text": "Data cleaning with pandas/polars: setup\n\nPandasPolars\n\n\n\nimport pandas as pd\nimport numpy as np\ndata = {\n    'Constant int': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'Constant str': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n    'All empty': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\n\ndf_pd = pd.DataFrame(data)\ndisplay(df_pd)\n\n\n\n\n\n\n\n\nConstant int\nB\nConstant str\nD\nAll nan\nAll empty\nDate\n\n\n\n\n0\n1\n2\nx\n4\nNaN\n\n01/01/2023\n\n\n1\n1\n3\nx\n5\nNaN\n\n02/01/2023\n\n\n2\n1\n2\nx\n6\nNaN\n\n03/01/2023\n\n\n\n\n\n\n\n\n\n\nimport polars as pl\nimport numpy as np\ndata = {\n    'Constant int': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'Constant str': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n    'All empty': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\n\ndf_pl = pl.DataFrame(data)\ndisplay(df_pl)\n\n\nshape: (3, 7)\n\n\n\nConstant int\nB\nConstant str\nD\nAll nan\nAll empty\nDate\n\n\ni64\ni64\nstr\ni64\nf64\nstr\nstr\n\n\n\n\n1\n2\n\"x\"\n4\nNaN\n\"\"\n\"01/01/2023\"\n\n\n1\n3\n\"x\"\n5\nNaN\n\"\"\n\"02/01/2023\"\n\n\n1\n2\n\"x\"\n6\nNaN\n\"\"\n\"03/01/2023\""
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "href": "pages/slides/talk-edf/slides.html#nulls-datetimes-constant-columns-with-pandaspolars",
    "title": "EDF Talk: Skrub",
    "section": "Nulls, datetimes, constant columns with pandas/polars",
    "text": "Nulls, datetimes, constant columns with pandas/polars\n\nPandasPolars\n\n\n\n# Parse the datetime strings with a specific format\ndf_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d/%m/%Y')\n\n# Drop columns with only a single unique value\ndf_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_pd_cleaned = drop_empty_columns(df_pd_cleaned)\n\n\n\n\n# Parse the datetime strings with a specific format\ndf_pl = df_pl.with_columns([\n    pl.col(\"Date\").str.strptime(pl.Date, \"%d/%m/%Y\", strict=False).alias(\"Date\")\n])\n\n# Drop columns with only a single unique value\ndf_pl_cleaned = df_pl.select([\n    col for col in df_pl.columns if df_pl[col].n_unique() &gt; 1\n])\n\n# Import selectors for dtype selection\nimport polars.selectors as cs\n\n# Drop columns with only missing values or only empty strings\ndef drop_empty_columns(df):\n    all_nan = df.select(\n        [\n            col for col in df.select(cs.numeric()).columns if \n            df [col].is_nan().all()\n        ]\n    ).columns\n    \n    all_empty = df.select(\n        [\n            col for col in df.select(cs.string()).columns if \n            (df[col].str.strip_chars().str.len_chars()==0).all()\n        ]\n    ).columns\n\n    to_drop = all_nan + all_empty\n\n    return df.drop(to_drop)\n\ndf_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#data-cleaning-with-skrub.cleaner",
    "href": "pages/slides/talk-edf/slides.html#data-cleaning-with-skrub.cleaner",
    "title": "EDF Talk: Skrub",
    "section": "Data cleaning with skrub.Cleaner",
    "text": "Data cleaning with skrub.Cleaner\n\nPandasPolars\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df_pd)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03\n\n\n\n\n\n\n\n\n\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df_pl)\ndisplay(df_cleaned)\n\n\nshape: (3, 3)\n\n\n\nB\nD\nDate\n\n\ni64\ni64\ndate\n\n\n\n\n2\n4\n2023-01-01\n\n\n3\n5\n2023-01-02\n\n\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#comparison",
    "href": "pages/slides/talk-edf/slides.html#comparison",
    "title": "EDF Talk: Skrub",
    "section": "Comparison",
    "text": "Comparison\n\nPandasPolars\n\n\n\n\n\nprint(df_pd_cleaned)\n\n   B  D       Date\n0  2  4 2023-01-01\n1  3  5 2023-01-02\n2  2  6 2023-01-03\n\n\n\n\nprint(df_cleaned)\n\n   B  D       Date\n0  2  4 2023-01-01\n1  3  5 2023-01-02\n2  2  6 2023-01-03\n\n\n\n\n\n\n\n\nprint(df_pl_cleaned)\n\nshape: (3, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ B   ‚îÜ D   ‚îÜ Date       ‚îÇ\n‚îÇ --- ‚îÜ --- ‚îÜ ---        ‚îÇ\n‚îÇ i64 ‚îÜ i64 ‚îÜ date       ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 2   ‚îÜ 4   ‚îÜ 2023-01-01 ‚îÇ\n‚îÇ 3   ‚îÜ 5   ‚îÜ 2023-01-02 ‚îÇ\n‚îÇ 2   ‚îÜ 6   ‚îÜ 2023-01-03 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nprint(df_cleaned)\n\nshape: (3, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ B   ‚îÜ D   ‚îÜ Date       ‚îÇ\n‚îÇ --- ‚îÜ --- ‚îÜ ---        ‚îÇ\n‚îÇ i64 ‚îÜ i64 ‚îÜ date       ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 2   ‚îÜ 4   ‚îÜ 2023-01-01 ‚îÇ\n‚îÇ 3   ‚îÜ 5   ‚îÜ 2023-01-02 ‚îÇ\n‚îÇ 2   ‚îÜ 6   ‚îÜ 2023-01-03 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-datetime-features-with-pandaspolars",
    "href": "pages/slides/talk-edf/slides.html#encoding-datetime-features-with-pandaspolars",
    "title": "EDF Talk: Skrub",
    "section": "Encoding datetime features with pandas/polars",
    "text": "Encoding datetime features with pandas/polars\n\nPandasPolars\n\n\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pd = pd.DataFrame(data)\ndatetime_column = \"date\"\ndf_pd[datetime_column] = pd.to_datetime(df_pd[datetime_column], errors='coerce')\n\ndf_pd['year'] = df_pd[datetime_column].dt.year\ndf_pd['month'] = df_pd[datetime_column].dt.month\ndf_pd['day'] = df_pd[datetime_column].dt.day\ndf_pd['hour'] = df_pd[datetime_column].dt.hour\ndf_pd['minute'] = df_pd[datetime_column].dt.minute\ndf_pd['second'] = df_pd[datetime_column].dt.second\n\n\n\n\nimport polars as pl\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf_pl = pl.DataFrame(data)\ndf_pl = df_pl.with_columns(date=pl.col(\"date\").str.to_datetime())\n\ndf_pl = df_pl.with_columns(\n    year=pl.col(\"date\").dt.year(),\n    month=pl.col(\"date\").dt.month(),\n    day=pl.col(\"date\").dt.day(),\n    hour=pl.col(\"date\").dt.hour(),\n    minute=pl.col(\"date\").dt.minute(),\n    second=pl.col(\"date\").dt.second(),\n)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#adding-periodic-features-with-pandaspolars",
    "href": "pages/slides/talk-edf/slides.html#adding-periodic-features-with-pandaspolars",
    "title": "EDF Talk: Skrub",
    "section": "Adding periodic features with pandas/polars",
    "text": "Adding periodic features with pandas/polars\n\nPandasPolars\n\n\n\ndf_pd['hour_sin'] = np.sin(2 * np.pi * df_pd['hour'] / 24)\ndf_pd['hour_cos'] = np.cos(2 * np.pi * df_pd['hour'] / 24)\n\ndf_pd['month_sin'] = np.sin(2 * np.pi * df_pd['month'] / 12)\ndf_pd['month_cos'] = np.cos(2 * np.pi * df_pd['month'] / 12)\n\n\n\n\ndf_pl = df_pl.with_columns(\n    hour_sin = np.sin(2 * np.pi * pl.col(\"hour\") / 24),\n    hour_cos = np.cos(2 * np.pi * pl.col(\"hour\") / 24),\n    \n    month_sin = np.sin(2 * np.pi * pl.col(\"month\") / 12),\n    month_cos = np.cos(2 * np.pi * pl.col(\"month\") / 12),\n)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "href": "pages/slides/talk-edf/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "title": "EDF Talk: Skrub",
    "section": "Encoding datetime features skrub.DatetimeEncoder",
    "text": "Encoding datetime features skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nX_date = ToDatetime().fit_transform(df[\"date\"])\nde = DatetimeEncoder(periodic_encoding=\"circular\")\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\nshape: (3, 8)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ date_year ‚îÜ date_total ‚îÜ date_month ‚îÜ date_month ‚îÜ date_day_ ‚îÜ date_day_ ‚îÜ date_hour ‚îÜ date_hour ‚îÇ\n‚îÇ ---       ‚îÜ _seconds   ‚îÜ _circular_ ‚îÜ _circular_ ‚îÜ circular_ ‚îÜ circular_ ‚îÜ _circular ‚îÜ _circular ‚îÇ\n‚îÇ f32       ‚îÜ ---        ‚îÜ 0          ‚îÜ 1          ‚îÜ 0         ‚îÜ 1         ‚îÜ _0        ‚îÜ _1        ‚îÇ\n‚îÇ           ‚îÜ f32        ‚îÜ ---        ‚îÜ ---        ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÇ\n‚îÇ           ‚îÜ            ‚îÜ f64        ‚îÜ f64        ‚îÜ f64       ‚îÜ f64       ‚îÜ f64       ‚îÜ f64       ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 2023.0    ‚îÜ 1.6726e9   ‚îÜ 0.5        ‚îÜ 0.866025   ‚îÜ 0.207912  ‚îÜ 0.978148  ‚îÜ 1.2246e-1 ‚îÜ -1.0      ‚îÇ\n‚îÇ           ‚îÜ            ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ           ‚îÜ 6         ‚îÜ           ‚îÇ\n‚îÇ 2023.0    ‚îÜ 1.6765e9   ‚îÜ 0.866025   ‚îÜ 0.5        ‚îÜ 1.2246e-1 ‚îÜ -1.0      ‚îÜ 0.866025  ‚îÜ -0.5      ‚îÇ\n‚îÇ           ‚îÜ            ‚îÜ            ‚îÜ            ‚îÜ 6         ‚îÜ           ‚îÜ           ‚îÜ           ‚îÇ\n‚îÇ 2023.0    ‚îÜ 1.6793e9   ‚îÜ 1.0        ‚îÜ 6.1232e-17 ‚îÜ -0.866025 ‚îÜ -0.5      ‚îÜ -1.0      ‚îÜ -1.8370e- ‚îÇ\n‚îÇ           ‚îÜ            ‚îÜ            ‚îÜ            ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ 16        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#what-periodic-features-look-like",
    "href": "pages/slides/talk-edf/slides.html#what-periodic-features-look-like",
    "title": "EDF Talk: Skrub",
    "section": "What periodic features look like",
    "text": "What periodic features look like"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "href": "pages/slides/talk-edf/slides.html#encoding-numerical-features-with-skrub.squashingscaler",
    "title": "EDF Talk: Skrub",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler\n\n\nSkrub wants to solve ML problems based partly on solid engineering and partly on statistical notions. The SquashingScaler is based on the second part, and is taken from a recent paper that evaluates different techniques for improving the performance of NNs."
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "href": "pages/slides/talk-edf/slides.html#encoding-numerical-features-with-skrub.squashingscaler-1",
    "title": "EDF Talk: Skrub",
    "section": "Encoding numerical features with skrub.SquashingScaler",
    "text": "Encoding numerical features with skrub.SquashingScaler"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-categorical-stringtext-features",
    "href": "pages/slides/talk-edf/slides.html#encoding-categorical-stringtext-features",
    "title": "EDF Talk: Skrub",
    "section": "Encoding categorical (string/text) features",
    "text": "Encoding categorical (string/text) features\nCategorical features have a ‚Äúcardinality‚Äù: the number of unique values\n\nLow cardinality features: OneHotEncoder\nHigh cardinality features (&gt;40 unique values): skrub.StringEncoder\nTextual features: skrub.TextEncoder and pretrained models from HuggingFace Hub"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/talk-edf/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "EDF Talk: Skrub",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer\nfrom skrub import TableVectorizer, TextEncoder\n\ntext = TextEncoder()\ntable_vec = TableVectorizer(high_cardinality=text)\ndf_encoded = table_vec.fit_transform(df)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#encoding-all-the-features-tablevectorizer-1",
    "href": "pages/slides/talk-edf/slides.html#encoding-all-the-features-tablevectorizer-1",
    "title": "EDF Talk: Skrub",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#fine-grained-column-transformations-with-applytocols",
    "href": "pages/slides/talk-edf/slides.html#fine-grained-column-transformations-with-applytocols",
    "title": "EDF Talk: Skrub",
    "section": "Fine-grained column transformations with ApplyToCols",
    "text": "Fine-grained column transformations with ApplyToCols\n\nimport pandas as pd\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ndf = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n\ncategorical_columns = selector(dtype_include=object)(df)\nnumerical_columns = selector(dtype_exclude=object)(df)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\ntransformed = ct.fit_transform(df)\ntransformed\n\narray([[-1.22474487,  0.        ,  0.        ,  1.        ],\n       [ 0.        ,  1.        ,  0.        ,  0.        ],\n       [ 1.22474487,  0.        ,  1.        ,  0.        ]])"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#fine-grained-column-transformations-with-applytocols-1",
    "href": "pages/slides/talk-edf/slides.html#fine-grained-column-transformations-with-applytocols-1",
    "title": "EDF Talk: Skrub",
    "section": "Fine-grained column transformations with ApplyToCols",
    "text": "Fine-grained column transformations with ApplyToCols\n\nimport skrub.selectors as s\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import ApplyToCols\n\nnumeric = ApplyToCols(StandardScaler(), cols=s.numeric())\nstring = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n\ntransformed = make_pipeline(numeric, string).fit_transform(df)\ntransformed\n\n\n\n\n\n\n\n\ntext_bar\ntext_baz\ntext_foo\nnumber\n\n\n\n\n0\n0.0\n0.0\n1.0\n-1.224745\n\n\n1\n1.0\n0.0\n0.0\n0.000000\n\n\n2\n0.0\n1.0\n0.0\n1.224745"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline",
    "title": "EDF Talk: Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-1",
    "title": "EDF Talk: Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-2",
    "title": "EDF Talk: Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "href": "pages/slides/talk-edf/slides.html#build-a-predictive-pipeline-with-tabular_pipeline",
    "title": "EDF Talk: Skrub",
    "section": "Build a predictive pipeline with tabular_pipeline",
    "text": "Build a predictive pipeline with tabular_pipeline\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_pipeline(Ridge())"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/talk-edf/slides.html#we-now-have-a-pipeline",
    "title": "EDF Talk: Skrub",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\nExplore the data\n\nskrub.TableReport\n\nPre-process the data\n\nCleaner, ToDatetime ‚Ä¶\n\nPerform feature engineering\n\nskrub.TableVectorizer,SquashingScaler, TextEncoder, StringEncoder‚Ä¶\n\nBuild a scikit-learn pipeline\n\ntabular_pipeline, sklearn.pipeline.make_pipeline ‚Ä¶\n\n???\nProfit üìà"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#what-if",
    "href": "pages/slides/talk-edf/slides.html#what-if",
    "title": "EDF Talk: Skrub",
    "section": "What if‚Ä¶",
    "text": "What if‚Ä¶\n\nYour data is spread over multiple tables?\nYou want to avoid data leakage?\nYou want to tune more than just the hyperparameters of your model?\nYou want to guarantee that your pipeline is replayed exactly on new data?"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#section-2",
    "href": "pages/slides/talk-edf/slides.html#section-2",
    "title": "EDF Talk: Skrub",
    "section": "",
    "text": "When a normal pipe is not enough‚Ä¶\n\n‚Ä¶ the skrub DataOps come to the rescue üöí"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#dataops",
    "href": "pages/slides/talk-edf/slides.html#dataops",
    "title": "EDF Talk: Skrub",
    "section": "DataOps‚Ä¶",
    "text": "DataOps‚Ä¶\n\nExtend the scikit-learn machinery to complex multi-table operations, and take care of data leakage\nTrack all operations with a computational graph (a Data Ops plan)\nAllow tuning any operation in the data plan\nCan be persisted and shared easily"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/talk-edf/slides.html#how-do-dataops-work-though",
    "title": "EDF Talk: Skrub",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#dataops-plans-learners-oh-my",
    "href": "pages/slides/talk-edf/slides.html#dataops-plans-learners-oh-my",
    "title": "EDF Talk: Skrub",
    "section": "DataOps, Plans, learners: oh my!",
    "text": "DataOps, Plans, learners: oh my!\n\nA DataOp (singular) wraps a single operation, and can be combined and concatenated with other DataOps.\nThe Data Ops Plan is a collective name for the directed computational graph that tracks a sequence and combination of DataOps.\nThe plan can be exported as a standalone object called learner. The learner works like a scikit-learn estimator that takes a dictionary of values rather than just X and y."
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#starting-with-the-dataops",
    "href": "pages/slides/talk-edf/slides.html#starting-with-the-dataops",
    "title": "EDF Talk: Skrub",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\nimport skrub\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\n\nX, y, products represent inputs to the pipeline.\nskrub splits X and y when training."
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#building-a-full-data-plan",
    "href": "pages/slides/talk-edf/slides.html#building-a-full-data-plan",
    "title": "EDF Talk: Skrub",
    "section": "Building a full data plan",
    "text": "Building a full data plan\n\nfrom skrub import selectors as s\n\nvectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder()\n)\nvectorized_products = products.skb.apply(vectorizer, cols=s.all() - \"basket_ID\")"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#building-a-full-data-plan-1",
    "href": "pages/slides/talk-edf/slides.html#building-a-full-data-plan-1",
    "title": "EDF Talk: Skrub",
    "section": "Building a full data plan",
    "text": "Building a full data plan\n\naggregated_products = vectorized_products.groupby(\n    \"basket_ID\"\n).agg(\"mean\").reset_index()\n\nfeatures = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\")\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#building-a-full-data-plan-2",
    "href": "pages/slides/talk-edf/slides.html#building-a-full-data-plan-2",
    "title": "EDF Talk: Skrub",
    "section": "Building a full data plan",
    "text": "Building a full data plan\n\nfrom sklearn.ensemble import ExtraTreesClassifier  \npredictions = features.skb.apply(\n    ExtraTreesClassifier(n_jobs=-1), y=y\n)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#inspecting-the-data-plan",
    "href": "pages/slides/talk-edf/slides.html#inspecting-the-data-plan",
    "title": "EDF Talk: Skrub",
    "section": "Inspecting the data plan",
    "text": "Inspecting the data plan\npredictions.skb.full_report()\n\nExecution report\nEach node:\n\nShows a preview of the data resulting from the operation\nReports the location in the code where the code is defined\nShows the run time of the node (in the next release)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/talk-edf/slides.html#exporting-the-plan-in-a-learner",
    "title": "EDF Talk: Skrub",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe data plan can be exported as a learner:\n\n# anywhere\nlearner = predictions.skb.make_learner(fitted=True)\n\n\nThen, the learner can be pickled ‚Ä¶\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\n‚Ä¶ loaded ‚Ä¶\nwith open(\"learner.bin\", \"rb\") as fp:\n    loaded_learner = pickle.load(fp)\n\n\n‚Ä¶ and applied to new data:\n\ndata = skrub.datasets.fetch_credit_fraud(split=\"test\")\nnew_baskets = data.baskets\nnew_products = data.products\nloaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})\n\narray([0, 0, 0, ..., 0, 0, 0], shape=(31549,))"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#hyperparameter-tuning-in-a-data-plan",
    "href": "pages/slides/talk-edf/slides.html#hyperparameter-tuning-in-a-data-plan",
    "title": "EDF Talk: Skrub",
    "section": "Hyperparameter tuning in a Data Plan",
    "text": "Hyperparameter tuning in a Data Plan\nskrub implements four choose_* functions:\n\nchoose_from: select from the given list of options\nchoose_int: select an integer within a range\nchoose_float: select a float within a range\nchoose_bool: select a bool\noptional: chooses whether to execute the given operation"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/talk-edf/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "EDF Talk: Skrub",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#tuning-with-dataops-is-simple",
    "href": "pages/slides/talk-edf/slides.html#tuning-with-dataops-is-simple",
    "title": "EDF Talk: Skrub",
    "section": "Tuning with DataOps is simple!",
    "text": "Tuning with DataOps is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)\nsearch = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True)"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#tuning-with-dataops-is-not-limited-to-estimators",
    "href": "pages/slides/talk-edf/slides.html#tuning-with-dataops-is-not-limited-to-estimators",
    "title": "EDF Talk: Skrub",
    "section": "Tuning with DataOps is not limited to estimators",
    "text": "Tuning with DataOps is not limited to estimators\n\nPandasPolars\n\n\n\ndf = pd.DataFrame(\n    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n)\n\ndf_do = skrub.var(\"grades\", df)\n\nagg_grades = df_do.groupby(\"subject\").agg(skrub.choose_from([\"count\", \"mean\"]))\nagg_grades.skb.describe_param_grid()\n\n\"- choose_from(['count', 'mean']): ['count', 'mean']\\n\"\n\n\n\n\n\ndf = pl.DataFrame(\n    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n)\n\ndf_do = skrub.var(\"grades\", df)\n\nagg_grades = df_do.group_by(\"subject\").agg(\n    skrub.choose_from([pl.mean(\"grade\"), pl.count(\"grade\")])\n)\nagg_grades.skb.describe_param_grid()\n\n'- choose_from([&lt;Expr [\\'col(\"grade\").mean()\\'] at 0x783825B3A3D0&gt;, &lt;Expr [\\'col(\"grade\").count()\\'] at 0x78382C9A0090&gt;]): [&lt;Expr [\\'col(\"grade\").mean()\\'] at 0x783825B3A3D0&gt;, &lt;Expr [\\'col(\"grade\").count()\\'] at 0x78382C9A0090&gt;]\\n'"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#run-hyperparameter-search",
    "href": "pages/slides/talk-edf/slides.html#run-hyperparameter-search",
    "title": "EDF Talk: Skrub",
    "section": "Run hyperparameter search",
    "text": "Run hyperparameter search\n# fit the search \nsearch = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True, cv=5)\n\n# save the best learner\nbest_learner = search.best_learner_"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#observe-the-impact-of-the-hyperparameters",
    "href": "pages/slides/talk-edf/slides.html#observe-the-impact-of-the-hyperparameters",
    "title": "EDF Talk: Skrub",
    "section": "Observe the impact of the hyperparameters",
    "text": "Observe the impact of the hyperparameters\nData Ops provide a built-in parallel coordinate plot.\nsearch = pred.skb.get_randomized_search(fitted=True)\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#more-information-about-the-data-ops",
    "href": "pages/slides/talk-edf/slides.html#more-information-about-the-data-ops",
    "title": "EDF Talk: Skrub",
    "section": "More information about the Data Ops",
    "text": "More information about the Data Ops\n\nSkrub example gallery\nTutorial on timeseries forecasting at Euroscipy 2025\nSkrub User guide\nA Kaggle notebook on addressing the Titanic survival challenge with Data Ops"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#getting-involved",
    "href": "pages/slides/talk-edf/slides.html#getting-involved",
    "title": "EDF Talk: Skrub",
    "section": "Getting involved",
    "text": "Getting involved\nDo you want to learn more?\n\nSkrub website\nSkrub materials website\nDiscord server\n\nFollow skrub on:\n\nBluesky\nLinkedIn\n\nStar skrub on GitHub, or contribute directly:\n\nGit repository"
  },
  {
    "objectID": "pages/slides/talk-edf/slides.html#tldw",
    "href": "pages/slides/talk-edf/slides.html#tldw",
    "title": "EDF Talk: Skrub",
    "section": "tl;dw",
    "text": "tl;dw\nskrub provides\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering\nDataOps, plans, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#building-complex-pipelines",
    "href": "pages/slides/expressions/expressions.html#building-complex-pipelines",
    "title": "Skrub learning materials",
    "section": "Building complex pipelines",
    "text": "Building complex pipelines\n\nOur learner contains several data-processing steps\n\njoining tables\nselecting columns\napplying machine-learning estimators\n\nSome steps have state that needs to be fitted\nOften several tables and aggregations are involved"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#example",
    "href": "pages/slides/expressions/expressions.html#example",
    "title": "Skrub learning materials",
    "section": "Example",
    "text": "Example\n\nWe have e-commerce check-out baskets\nEach containing one or more products\nPredict if the transaction is fraudulent\n\n\nDataset"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#a-first-attempt",
    "href": "pages/slides/expressions/expressions.html#a-first-attempt",
    "title": "Skrub learning materials",
    "section": "A first attempt ‚Ä¶",
    "text": "A first attempt ‚Ä¶\n\nScikit-learn assumes a single table X of the right shape"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#loading-data",
    "href": "pages/slides/expressions/expressions.html#loading-data",
    "title": "Skrub learning materials",
    "section": "Loading data",
    "text": "Loading data\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = data.baskets[[\"ID\"]]\ny = data.baskets[\"fraud_flag\"]\nproducts = data.products"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\n\nvectorized_products = product_vectorizer.fit_transform(data.products)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-1",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-1",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\n\nvectorized_products = product_vectorizer.fit_transform(data.products)\nü§î\n\nHow to store product_vectorizer?\nFitted on whole products table: data leakage\nCannot tune hyper-parameters\nTransforming only some columns is hard\n\nColumnTransformer üòüüò∞"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#joining-the-product-features",
    "href": "pages/slides/expressions/expressions.html#joining-the-product-features",
    "title": "Skrub learning materials",
    "section": "Joining the product features",
    "text": "Joining the product features\naggregated_products = (\n    vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\n)\nX = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\").drop(\n    columns=[\"ID\", \"basket_ID\"]\n)\nü§î\n\nHow to keep track of these transformations?\nCannot tune choices"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator",
    "href": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator",
    "title": "Skrub learning materials",
    "section": "Adding the supervised estimator",
    "text": "Adding the supervised estimator\nclassifier = HistGradientBoostingClassifier()\n\ncross_val_score(classifier, X, y, scoring=\"roc_auc\", n_jobs=5)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#skrub-to-the-rescue",
    "href": "pages/slides/expressions/expressions.html#skrub-to-the-rescue",
    "title": "Skrub learning materials",
    "section": "Skrub to the rescue",
    "text": "Skrub to the rescue\n\nBuild complex pipelines involving multiple tables"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#loading-data-1",
    "href": "pages/slides/expressions/expressions.html#loading-data-1",
    "title": "Skrub learning materials",
    "section": "Loading data",
    "text": "Loading data\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = skrub.X(data.baskets[[\"ID\"]])\ny = skrub.y(data.baskets[\"fraud_flag\"])\nproducts = skrub.var(\"products\", data.products)\n\nX, y, products represent inputs to the model\nOperations on those objects are evaluated lazily\n\nRecorded rather than evaluated immediately\nBut a preview is computed for interactive development\n\nThey forward all operations to the result of their evaluation\n\nFull API of the underlying object is available"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-2",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-2",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-3",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-3",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X\nproduct_vectorizer is added to the model"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-4",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-4",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X\nproduct_vectorizer is added to the model\nWe can select columns to transform"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-5",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-5",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(\n        n_components=skrub.choose_int(2, 20)\n    )\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can tune hyperparameters (more later)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#joining-the-product-features-1",
    "href": "pages/slides/expressions/expressions.html#joining-the-product-features-1",
    "title": "Skrub learning materials",
    "section": "Joining the product features",
    "text": "Joining the product features\naggregated_products = (\n    vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\n)\nX = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\").drop(\n    columns=[\"ID\", \"basket_ID\"]\n)\n\nTransformations added to the model\nCan tune choices\nWhile having access to all the dataframe‚Äôs functionality"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator-1",
    "href": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator-1",
    "title": "Skrub learning materials",
    "section": "Adding the supervised estimator",
    "text": "Adding the supervised estimator\nclassifier = HistGradientBoostingClassifier()\npred = X.skb.apply(classifier, y=y)\nEvaluation\npred.skb.cross_validate(scoring=\"roc_auc\", n_jobs=5)\nTraining & using a model\n\n\ntrain.py\n\nestimator = pred.skb.get_estimator(fitted=True)\nwith open(\"estimator.pickle\", \"wb\") as ostream:\n    pickle.dump(estimator, ostream)\n\n\n\npredict.py\n\nwith open(\"estimator.pickle\", \"rb\") as istream:\n    estimator = pickle.load(istream)\n\nestimator.predict({'X': unseen_baskets, 'products': unseen_products})"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#easy-inspection",
    "href": "pages/slides/expressions/expressions.html#easy-inspection",
    "title": "Skrub learning materials",
    "section": "Easy inspection",
    "text": "Easy inspection\npred.skb.full_report()\n\nreport"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\n\nAny choice in the pipeline can be tuned\nOptions are specified inline\nInspecting results is easy"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning-1",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning-1",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nWithout skrub: üò≠üò≠üò≠\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#section",
    "href": "pages/slides/expressions/expressions.html#section",
    "title": "Skrub learning materials",
    "section": "",
    "text": "NO!"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning-2",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning-2",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nWith skrub: replace any value with a range\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(\n        n_components=skrub.choose_int(2, 20)\n    )\n)\n\n# ...\n\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()\n\n\nparallel coordinates plot"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#fun-facts",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#fun-facts",
    "title": "Probabl Sprint - July 2025",
    "section": "Fun facts",
    "text": "Fun facts\n\nI‚Äôm Italian, but I don‚Äôt drink coffee, wine, and I like pizza with fries\nI did my PhD in C√¥te d‚ÄôAzur, and I moved away because it was too sunny and I don‚Äôt like the sea"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#an-example-pipeline",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#an-example-pipeline",
    "title": "Probabl Sprint - July 2025",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPre-process the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data",
    "title": "Probabl Sprint - July 2025",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#exploring-the-data-with-skrub",
    "title": "Probabl Sprint - July 2025",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#data-cleaning-with-pandas",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#data-cleaning-with-pandas",
    "title": "Probabl Sprint - July 2025",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'Constant int': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'Constant str': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n    'All empty': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\ndf = pd.DataFrame(data)\ndisplay(df)\n\n\n\n\n\n\n\n\nConstant int\nB\nConstant str\nD\nAll nan\nAll empty\nDate\n\n\n\n\n0\n1\n2\nx\n4\nNaN\n\n01/01/2023\n\n\n1\n1\n3\nx\n5\nNaN\n\n02/01/2023\n\n\n2\n1\n2\nx\n6\nNaN\n\n03/01/2023"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#data-cleaning-with-pandas-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#data-cleaning-with-pandas-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\n# Parse the datetime strings with a specific format\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n\n# Drop columns with only a single unique value\ndf_cleaned = df.loc[:, df.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_cleaned = drop_empty_columns(df_cleaned)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#lightweight-data-cleaning-cleaner",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#lightweight-data-cleaning-cleaner",
    "title": "Probabl Sprint - July 2025",
    "section": "Lightweight data cleaning: Cleaner",
    "text": "Lightweight data cleaning: Cleaner\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-with-pandas",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-with-pandas",
    "title": "Probabl Sprint - July 2025",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf = pd.DataFrame(data)\ndf_expanded = df.copy()\ndatetime_column = \"date\"\ndf_expanded[datetime_column] = pd.to_datetime(df_expanded[datetime_column], errors='coerce')\n\ndf_expanded['year'] = df_expanded[datetime_column].dt.year\ndf_expanded['month'] = df_expanded[datetime_column].dt.month\ndf_expanded['day'] = df_expanded[datetime_column].dt.day\ndf_expanded['hour'] = df_expanded[datetime_column].dt.hour\ndf_expanded['minute'] = df_expanded[datetime_column].dt.minute\ndf_expanded['second'] = df_expanded[datetime_column].dt.second"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-with-pandas-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-with-pandas-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\ndf_expanded['hour_sin'] = np.sin(2 * np.pi * df_expanded['hour'] / 24)\ndf_expanded['hour_cos'] = np.cos(2 * np.pi * df_expanded['hour'] / 24)\n\ndf_expanded['month_sin'] = np.sin(2 * np.pi * df_expanded['month'] / 12)\ndf_expanded['month_cos'] = np.cos(2 * np.pi * df_expanded['month'] / 12)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nDataFrame with expanded datetime columns:\")\nprint(df_expanded)\n\nOriginal DataFrame:\n                  date  value\n0  2023-01-01 12:34:56     10\n1  2023-02-15 08:45:23     20\n2  2023-03-20 18:12:45     30\n\nDataFrame with expanded datetime columns:\n                 date  value  year  month  day  hour  minute  second  \\\n0 2023-01-01 12:34:56     10  2023      1    1    12      34      56   \n1 2023-02-15 08:45:23     20  2023      2   15     8      45      23   \n2 2023-03-20 18:12:45     30  2023      3   20    18      12      45   \n\n       hour_sin      hour_cos  month_sin     month_cos  \n0  1.224647e-16 -1.000000e+00   0.500000  8.660254e-01  \n1  8.660254e-01 -5.000000e-01   0.866025  5.000000e-01  \n2 -1.000000e+00 -1.836970e-16   1.000000  6.123234e-17"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "title": "Probabl Sprint - July 2025",
    "section": "Encoding datetime features skrub.DatetimeEncoder",
    "text": "Encoding datetime features skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\nX_date = ToDatetime().fit_transform(df[\"date\"])\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\n   date_year  date_total_seconds  date_month_circular_0  \\\n0     2023.0        1.672577e+09               0.500000   \n1     2023.0        1.676451e+09               0.866025   \n2     2023.0        1.679336e+09               1.000000   \n\n   date_month_circular_1  date_day_circular_0  date_day_circular_1  \\\n0           8.660254e-01         2.079117e-01             0.978148   \n1           5.000000e-01         1.224647e-16            -1.000000   \n2           6.123234e-17        -8.660254e-01            -0.500000   \n\n   date_hour_circular_0  date_hour_circular_1  \n0          1.224647e-16         -1.000000e+00  \n1          8.660254e-01         -5.000000e-01  \n2         -1.000000e+00         -1.836970e-16"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#what-periodic-features-look-like",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#what-periodic-features-look-like",
    "title": "Probabl Sprint - July 2025",
    "section": "What periodic features look like",
    "text": "What periodic features look like"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#encoding-all-the-features-tablevectorizer",
    "title": "Probabl Sprint - July 2025",
    "section": "Encoding all the features: TableVectorizer",
    "text": "Encoding all the features: TableVectorizer"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline",
    "title": "Probabl Sprint - July 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-2",
    "title": "Probabl Sprint - July 2025",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "title": "Probabl Sprint - July 2025",
    "section": "Build a predictive pipeline with tabular_learner",
    "text": "Build a predictive pipeline with tabular_learner\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_learner(Ridge())\nmodel\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('tablevectorizer', ...), ('simpleimputer', ...), ...]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    tablevectorizer: TableVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\ncardinality_threshold¬†\n40\n\n\n\nlow_cardinality¬†\nOneHotEncoder..._output=False)\n\n\n\nhigh_cardinality¬†\nStringEncoder()\n\n\n\nnumeric¬†\nPassThrough()\n\n\n\ndatetime¬†\nDatetimeEncod...ding='spline')\n\n\n\nspecific_transformers¬†\n()\n\n\n\ndrop_null_fraction¬†\n1.0\n\n\n\ndrop_if_constant¬†\nFalse\n\n\n\ndrop_if_unique¬†\nFalse\n\n\n\ndatetime_format¬†\nNone\n\n\n\nn_jobs¬†\nNone\n\n\n\n\n            \n        \n    datetimeDatetimeEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nresolution¬†\n'hour'\n\n\n\nadd_weekday¬†\nFalse\n\n\n\nadd_total_seconds¬†\nTrue\n\n\n\nadd_day_of_year¬†\nFalse\n\n\n\nperiodic_encoding¬†\n'spline'\n\n\n\n\n            \n        \n    low_cardinalityOneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories¬†\n'auto'\n\n\n\ndrop¬†\n'if_binary'\n\n\n\nsparse_output¬†\nFalse\n\n\n\ndtype¬†\n'float32'\n\n\n\nhandle_unknown¬†\n'ignore'\n\n\n\nmin_frequency¬†\nNone\n\n\n\nmax_categories¬†\nNone\n\n\n\nfeature_name_combiner¬†\n'concat'\n\n\n\n\n            \n        \n    high_cardinalityStringEncoder\n        \n            \n                Parameters\n                \n\n\n\n\nn_components¬†\n30\n\n\n\nvectorizer¬†\n'tfidf'\n\n\n\nngram_range¬†\n(3, ...)\n\n\n\nanalyzer¬†\n'char_wb'\n\n\n\nstop_words¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values¬†\nnan\n\n\n\nstrategy¬†\n'mean'\n\n\n\nfill_value¬†\nNone\n\n\n\ncopy¬†\nTrue\n\n\n\nadd_indicator¬†\nTrue\n\n\n\nkeep_empty_features¬†\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    Ridge?Documentation for Ridge\n        \n            \n                Parameters\n                \n\n\n\n\nalpha¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\ncopy_X¬†\nTrue\n\n\n\nmax_iter¬†\nNone\n\n\n\ntol¬†\n0.0001\n\n\n\nsolver¬†\n'auto'\n\n\n\npositive¬†\nFalse\n\n\n\nrandom_state¬†\nNone"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#we-now-have-a-pipeline",
    "title": "Probabl Sprint - July 2025",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\n\nskrub.datasets, or user data\n\nExplore the data\n\nskrub.TableReport\n\nPre-process the data\n\nskrub.TableVectorizer, Cleaner, DatetimeEncoder ‚Ä¶\n\nPerform feature engineering\n\nskrub.TableVectorizer, TextEncoder, StringEncoder‚Ä¶\n\nBuild a scikit-learn pipeline\n\ntabular_learner, sklearn.pipeline.make_pipeline ‚Ä¶\n\n???\nProfit üìà"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#a-realistic-scenario",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#a-realistic-scenario",
    "title": "Probabl Sprint - July 2025",
    "section": "A realistic scenario",
    "text": "A realistic scenario\nA data scientist needs to train a ML model, but features are spread across multiple tables.\n\n\n\n\n\n\n\nWarning\n\n\nMany issues with this!\n\n\n\n\n\nscikit-learn pipelines support only a single feature matrix X\nDataframe operations cannot be tuned\nData leakage must be accounted for\nPersisting and reproducing operations is complex"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#skrub-dataops",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#skrub-dataops",
    "title": "Probabl Sprint - July 2025",
    "section": "skrub DataOps",
    "text": "skrub DataOps\nWhen a normal pipe is not enough‚Ä¶\n\n‚Ä¶ the skrub DataOps come to the rescue üöí"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#dataops",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#dataops",
    "title": "Probabl Sprint - July 2025",
    "section": "DataOps‚Ä¶",
    "text": "DataOps‚Ä¶\n\nExtend the scikit-learn machinery to complex multi-table operations\nTrack all operations with a computational graph (a data plan)\nAllow tuning any operation in the data plan\nCan be persisted and shared easily by generating a learner"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#dataops-data-plans-learners-oh-my",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#dataops-data-plans-learners-oh-my",
    "title": "Probabl Sprint - July 2025",
    "section": "DataOps, Data Plans, learners: oh my!",
    "text": "DataOps, Data Plans, learners: oh my!\n\nA DataOp (singular) wraps a single operation, and can be combined and concatenated with other DataOps.\nThe Data Plan is a collective name for a sequence and combination of DataOps.\nThe Data Plan can be exported as a standalone object called learner. The learner takes a dictionary of values rather than just X and y."
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#how-do-dataops-work-though",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#how-do-dataops-work-though",
    "title": "Probabl Sprint - July 2025",
    "section": "How do DataOps work, though?",
    "text": "How do DataOps work, though?\nDataOps wrap around user operations, where user operations are:\n\nany dataframe operation (e.g., merge, group by, aggregate etc.)\nscikit-learn estimators (a Random Forest, RidgeCV etc.)\ncustom user code (load data from a path, fetch from an URL etc.)\n\n\n\n\n\n\n\n\nImportant\n\n\nDataOps record user operations, so that they can later be replayed in the same order and with the same arguments on unseen data."
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#starting-with-the-dataops",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#starting-with-the-dataops",
    "title": "Probabl Sprint - July 2025",
    "section": "Starting with the DataOps",
    "text": "Starting with the DataOps\n\ndata = skrub.datasets.fetch_credit_fraud()\n\nbaskets = skrub.var(\"baskets\", data.baskets)\nX = baskets[[\"ID\"]].skb.mark_as_X()\ny = baskets[\"fraud_flag\"].skb.mark_as_y()\n\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\n\nX, y, products represent inputs to the pipeline.\nskrub splits X and y when training."
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#building-a-full-data-plan",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#building-a-full-data-plan",
    "title": "Probabl Sprint - July 2025",
    "section": "Building a full data plan",
    "text": "Building a full data plan\nfrom skrub import selectors as s\nfrom sklearn.ensemble import ExtraTreesClassifier  \n\nvectorizer = skrub.TableVectorizer(high_cardinality=skrub.StringEncoder(), n_jobs=-1)\nvectorized_products = products.skb.apply(vectorizer, cols=s.all() - \"basket_ID\")\naggregated_products = vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\nfeatures = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\")\nfeatures = features.drop(columns=[\"ID\", \"basket_ID\"])\npredictions = features.skb.apply(ExtraTreesClassifier(n_jobs=-1), y=y)"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#inspecting-the-data-plan",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#inspecting-the-data-plan",
    "title": "Probabl Sprint - July 2025",
    "section": "Inspecting the data plan",
    "text": "Inspecting the data plan\npredictions.skb.full_report()\n\nreport"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#exporting-the-plan-in-a-learner",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#exporting-the-plan-in-a-learner",
    "title": "Probabl Sprint - July 2025",
    "section": "Exporting the plan in a learner",
    "text": "Exporting the plan in a learner\nThe data plan can be exported as a learner:\n# anywhere\nlearner = predictions.skb.make_learner()\n# search is a HPO object\nbest_learner = search.skb.best_learner_\n\nThen, the learner can be pickled ‚Ä¶\nimport pickle\n\nwith open(\"learner.bin\", \"wb\") as fp:\n    pickle.dump(learner, fp)\n\n\n‚Ä¶ and loaded\nwith open(\"learner.bin\", \"rb\") as fp:\n    learner = pickle.load(fp)\n\nlearner.predict({\"baskets\": new_baskets, \"products\": new_products})"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#hyperparameter-tuning-in-a-data-plan",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#hyperparameter-tuning-in-a-data-plan",
    "title": "Probabl Sprint - July 2025",
    "section": "Hyperparameter tuning in a Data Plan",
    "text": "Hyperparameter tuning in a Data Plan\nskrub implements four choose_* functions:\n\nchoose_from: select from the given list of options\nchoose_int: select an integer within a range\nchoose_float: select a float within a range\nchoose_bool: select a bool\noptional: chooses between a value or DataOp and no op"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#hyperparameter-tuning-in-a-data-plan-1",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#hyperparameter-tuning-in-a-data-plan-1",
    "title": "Probabl Sprint - July 2025",
    "section": "Hyperparameter tuning in a Data Plan",
    "text": "Hyperparameter tuning in a Data Plan\nIt‚Äôs possible to nest these functions to create complex grids:\nX.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#tuning-in-scikit-learn-can-be-complex",
    "title": "Probabl Sprint - July 2025",
    "section": "Tuning in scikit-learn can be complex",
    "text": "Tuning in scikit-learn can be complex\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#tuning-with-dataops-is-simple",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#tuning-with-dataops-is-simple",
    "title": "Probabl Sprint - July 2025",
    "section": "Tuning with DataOps is simple!",
    "text": "Tuning with DataOps is simple!\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)\nregressor.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#observe-the-impact-of-the-hyperparameters",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#observe-the-impact-of-the-hyperparameters",
    "title": "Probabl Sprint - July 2025",
    "section": "Observe the impact of the hyperparameters",
    "text": "Observe the impact of the hyperparameters\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#tldw",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#tldw",
    "title": "Probabl Sprint - July 2025",
    "section": "tl;dw",
    "text": "tl;dw\nskrub provides\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering\nsoon‚Ñ¢Ô∏è, DataOps, data plans, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/probabl-sprint-july-2025/slides.html#getting-involved",
    "href": "pages/slides/probabl-sprint-july-2025/slides.html#getting-involved",
    "title": "Probabl Sprint - July 2025",
    "section": "Getting involved",
    "text": "Getting involved\n\nSkrub website (QR code below!)\nSkrub materials website\nGit repository\nDiscord server\nBluesky"
  },
  {
    "objectID": "pages/listings.html",
    "href": "pages/listings.html",
    "title": "Latest",
    "section": "",
    "text": "Mar 26, 2025\n\n\nRiccardo Cappuzzo\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/listings.html#blog-posts",
    "href": "pages/listings.html#blog-posts",
    "title": "Latest",
    "section": "",
    "text": "Mar 26, 2025\n\n\nRiccardo Cappuzzo\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/listings.html#talks",
    "href": "pages/listings.html#talks",
    "title": "Latest",
    "section": "Talks",
    "text": "Talks"
  },
  {
    "objectID": "pages/notebooks/index_notebooks.html",
    "href": "pages/notebooks/index_notebooks.html",
    "title": "Skrub tutorials and long-form posts",
    "section": "",
    "text": "An introductory notebook for skrub\nComparing different categorical encoders.\nEuroSciPy 2025 Tutorial"
  }
]