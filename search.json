[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skrub learning materials index",
    "section": "",
    "text": "This is the Skrub learning materials website: this is where you’ll be able to find all the material that is used to teach and present Skrub to audiences.\nSlides for presentations, talks and lectures are in slides section. Notebooks and blog posts are instead located in the notebook section.\nNote that the material that is gathered here may become obsolete depending on the development of Skrub.\nFor up-to-date information on Skrub and its API refer to the documentation, while\nadditional examples are available in the main gallery. Material on the main website is constantly tested with the latest version of Skrub."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#plan-for-the-presentation",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#plan-for-the-presentation",
    "title": "A Skrub use case in academia",
    "section": "Plan for the presentation",
    "text": "Plan for the presentation\n\nContext and explanation of the problem\nThe Retrieve, Merge, Predict pipeline\nHow is this relevant?"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-1",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-2",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-2",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-3",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-augmenting-tables-3",
    "title": "A Skrub use case in academia",
    "section": "Example: augmenting tables",
    "text": "Example: augmenting tables"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-definitions",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-definitions",
    "title": "A Skrub use case in academia",
    "section": "Some definitions",
    "text": "Some definitions\n\nBase table: the table we want to augment (“Movies”, “Housing”…)\nQuery column: a column that should be used as join key (“Movie title”, “Address”…)\nData lake: an unstructured repository of many (thousands of…) “candidate tables”\nCandidate table: a table that may be useful for augmenting the base table (“Movie directors”…)\nCandidate column: a column in a candidate table that could be joined on the query column (“Title” in a table about filmographies)\nAugmented table: the result of joining the base table and a candidate table\n\n\n\n\n\n\n\n\nWarning\n\n\nThis terminology is slightly different from that used in the Skrub documentation"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#jaccard-containment",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#jaccard-containment",
    "title": "A Skrub use case in academia",
    "section": "Jaccard Containment",
    "text": "Jaccard Containment\n\n\n\nJaccard Similarity: \\(\\frac{|Q \\cap X|}{|Q \\cup X|}\\)\nJaccard Containment: \\(\\frac{|Q \\cap X|}{|Q|}\\)\n\n\n\nJaccard containment is a “normalized” intersection:\n\n\n\n\n\n\nImportant\n\n\nWhat fraction of of query set Q is in candidate column X?\n\n\n\n\n\n\n\n\nhttps://ekzhu.com/datasketch/lshensemble.html"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#the-focus-of-the-study",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#the-focus-of-the-study",
    "title": "A Skrub use case in academia",
    "section": "The focus of the study:",
    "text": "The focus of the study:\n\nFind the best way to discover candidates.\nWork within a defined computational budget\nWork with exact joins between a base table and multiple join candidates.\nGuarantee that results are reproducible."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#we-do-not-consider",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#we-do-not-consider",
    "title": "A Skrub use case in academia",
    "section": "We do not consider:",
    "text": "We do not consider:\n\nEntity matching or fuzzy joins (e.g., matching “NYT” and “The New York Times”).\nDiscovering the query column.\nMulti-key joins"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#working-with-data-lakes-is-hard",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#working-with-data-lakes-is-hard",
    "title": "A Skrub use case in academia",
    "section": "Working with data lakes is hard",
    "text": "Working with data lakes is hard\n\nSome CSVs don’t use commas\nSome CSVs have no (known) schema\nSome CSVs aren’t CSVs, they’re actually JSON files in disguise\nSome JSONs aren’t JSONs, they’re actually strings in disguise\n\n\nIf you don’t know the tables, everyone is sus"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#pipeline-schema",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#pipeline-schema",
    "title": "A Skrub use case in academia",
    "section": "Pipeline schema",
    "text": "Pipeline schema"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-retrieval",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-retrieval",
    "title": "A Skrub use case in academia",
    "section": "Candidate retrieval",
    "text": "Candidate retrieval\n\nExact Matching: measure the exact Jaccard containment (JC) for each column in the data lake.\nMinHash: estimate the Jaccard containment, query to get columns with a JC larger than a threshold.\nHybrid MinHash: query with MinHash, then measure the exact JC for the retrieved candidates.\nStarmie : use a language model to query candidate columns."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-selection",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#candidate-selection",
    "title": "A Skrub use case in academia",
    "section": "Candidate selection",
    "text": "Candidate selection\n\nHighest Containment Join: Rank candidates by Jaccard Containment.\nFull Join: Join all candidates.\nBest Single Join: Train a model on each candidate, select the best.\nStepwise Greedy Join: Like Best Single Join, but keep all good candidates."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation",
    "title": "A Skrub use case in academia",
    "section": "Aggregation",
    "text": "Aggregation"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#aggregation-1",
    "title": "A Skrub use case in academia",
    "section": "Aggregation",
    "text": "Aggregation\n\nAny: take one value at random from each group\nMean: for each group, take the mean of numerical values and mode of categorical values\nDeep Feature Synthesis (DFS): greedily generate new features (count, mean, sum…) to already present features."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#prediction",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#prediction",
    "title": "A Skrub use case in academia",
    "section": "Prediction",
    "text": "Prediction\n\nRidgeCV: linear baseline 📈\nCatBoost: GDBT 🌲\nResNet: Neural Networks 🧠\nRealMLP: Neural Networks 🧠"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-experimental-results",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#some-experimental-results",
    "title": "A Skrub use case in academia",
    "section": "Some experimental results",
    "text": "Some experimental results"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#total-compute-time",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#total-compute-time",
    "title": "A Skrub use case in academia",
    "section": "Total compute time",
    "text": "Total compute time\n\n\n\nML Model\nPlatform\nTotal compute time\n\n\n\n\nRidgeCV\nCPU\n4y 3m 10d 7h\n\n\nCatBoost\nCPU\n1y 3m 29d 21h\n\n\nResNet\nGPU\n5y 6m 23d 0h\n\n\nRealMLP\nGPU\n10y 7m 23d 3h\n\n\nTotal\nBoth\n21y 9m 26d 8h"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#features-of-research-code",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#features-of-research-code",
    "title": "A Skrub use case in academia",
    "section": "“Features” of research code",
    "text": "“Features” of research code\nResearch code…\n\nIs mostly custom-made for a specific experiment\nFeatures little to no testing\nOften is poorly documented, or not at all\nInvolves a lot of technical debt"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrub-to-the-rescue",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrub-to-the-rescue",
    "title": "A Skrub use case in academia",
    "section": "Skrub to the rescue",
    "text": "Skrub to the rescue\n\nWell tested code\nGood documentation\nFeatures cover much of the pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#using-skrub-features-in-the-pipeline",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#using-skrub-features-in-the-pipeline",
    "title": "A Skrub use case in academia",
    "section": "Using Skrub features in the pipeline",
    "text": "Using Skrub features in the pipeline\n\nThe Discover object can replace (part of) the retrieval step.\nAll the code for joining can be replaced by the AggJoiner or MultiAggJoiner.\nThe MultiAggJoiner is an additional baseline.\nThe TableVectorizer can handle automated preprocessing of the tables.\nJoined candidates can be examined quickly using the TableReport."
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline",
    "title": "A Skrub use case in academia",
    "section": "“Skrubified” pipeline",
    "text": "“Skrubified” pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline-1",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#skrubified-pipeline-1",
    "title": "A Skrub use case in academia",
    "section": "“Skrubified” pipeline",
    "text": "“Skrubified” pipeline"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-with-multiaggjoiner",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#example-with-multiaggjoiner",
    "title": "A Skrub use case in academia",
    "section": "Example with MultiAggJoiner",
    "text": "Example with MultiAggJoiner\n\n\nmerged = source_table.clone()\nhashes = []\nfor hash_, mdata in tqdm(\n    index_cand.items(),\n    total=len(index_cand),\n    leave=False,\n    desc=\"Full Join\",\n    position=2,\n):\n    cnd_md = mdata.candidate_metadata\n    hashes.append(cnd_md[\"hash\"])\n    candidate_table = pl.read_parquet(cnd_md[\"full_path\"])\n\n    left_on = mdata.left_on\n    right_on = mdata.right_on\n\n    aggr_right = aggregate_table(\n        candidate_table, right_on, aggregation_method=aggregation\n    )\n\n    merged = execute_join(\n        merged,\n        aggr_right,\n        left_on=left_on,\n        right_on=right_on,\n        how=\"left\",\n        suffix=\"_\" + hash_[:10],\n    )\n\n# MOCK-UP\nfrom skrub import MultiAggJoiner\njoiner = MultiAggJoiner(candidate_tables, keys=candidate_keys)\nmerged = joiner.fit_transform(source_table)"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#repositories",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#repositories",
    "title": "A Skrub use case in academia",
    "section": "Repositories",
    "text": "Repositories\n\n\nRetrieve, Merge, Predict website\nRetrieve, Merge, Predict repository"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#acknowledgements",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#acknowledgements",
    "title": "A Skrub use case in academia",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\n\n\n\n\n\nAuthors:\n\n\nRiccardo Cappuzzo (SODA, Dataiku)\nAimee Coelho (Dataiku)\nFelix Lefebvre (SODA)\nPaolo Papotti (Eurecom)\nGael Varoquaux (SODA)"
  },
  {
    "objectID": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#conclusions-and-summary",
    "href": "pages/slides/skrub-retrieve-merge-predict/skrub-retrieve-merge-predict.html#conclusions-and-summary",
    "title": "A Skrub use case in academia",
    "section": "Conclusions and summary",
    "text": "Conclusions and summary\n\n\n\n\n\nTree-based models are more effective and more resilient than the alternatives\nGood table retrieval affects the whole pipeline\nSimple methods produce results comparable or even better than more complex methods\n\n\n\n\nSkrub provides well-tested, well-documented code\nSkrub objects provide features that cover most of the pipeline\nIn return, the pipeline helped deciding on relevant features."
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object",
    "href": "pages/slides/discover/discover.html#the-discover-object",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object"
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object-1",
    "href": "pages/slides/discover/discover.html#the-discover-object-1",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object"
  },
  {
    "objectID": "pages/slides/discover/discover.html#the-discover-object-2",
    "href": "pages/slides/discover/discover.html#the-discover-object-2",
    "title": "Discover object",
    "section": "The discover object",
    "text": "The discover object\n\nGive a high level overview of the content of a data lake\nBuild an (approximate) schema of the data\nSuggest tables that are relevant to what the user provides"
  },
  {
    "objectID": "pages/slides/discover/discover.html#planned-features",
    "href": "pages/slides/discover/discover.html#planned-features",
    "title": "Discover object",
    "section": "Planned features",
    "text": "Planned features\n\nIf no query is provided:\n\n\nGiven a collection of tables, profile them and produce aggregated statistics.\nDtypes, null values, shape of the tables.\n\n\nIf a query table is provided\n\n\nMeasure various pairwise metrics between columns in the query table, and the columns in the collection of tables.\nRank the columns based on the metrics to find those that are most relevant.\nJaccard containment will be the first metric.\nStatistics remain available to perform feature selection."
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover\n\npath_to_tables = \"./many_tables/\"\ndiscover = Discover(path_to_tables)\n\ndataframe_stats = discover.fit_transform()"
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code-1",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code-1",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover\nimport pandas as pd\n\npath_to_tables = \"./many_tables/\"\nquery_table = pd.read_csv(\"this_table.csv\")\n\ndiscover = Discover(path_to_tables)\n\nranking_by_column = discover.fit_transform(query_table)"
  },
  {
    "objectID": "pages/slides/discover/discover.html#mock-up-of-the-code-2",
    "href": "pages/slides/discover/discover.html#mock-up-of-the-code-2",
    "title": "Discover object",
    "section": "Mock-up of the code",
    "text": "Mock-up of the code\nfrom skrub import Discover, MultiAggJoiner\nimport pandas as pd\n\npath_to_tables = \"./many_tables/\"\nquery_table = pd.read_csv(\"this_table.csv\")\n\ndiscover = Discover(path_to_tables)\n\nranking_by_column = discover.fit_transform(query_table)\n\njoiner = MultiAggJoiner(ranking_by_column)\njoined_table = joiner.fit_transform(query_table)"
  },
  {
    "objectID": "pages/slides/discover/discover.html#interface-with-the-data",
    "href": "pages/slides/discover/discover.html#interface-with-the-data",
    "title": "Discover object",
    "section": "Interface with the data",
    "text": "Interface with the data\n\nThe initial implementation will read from a path/glob\nLater version will target SQL databases\nWhat other technologies should we consider?"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#building-complex-pipelines",
    "href": "pages/slides/expressions/expressions.html#building-complex-pipelines",
    "title": "Skrub learning materials",
    "section": "Building complex pipelines",
    "text": "Building complex pipelines\n\nOur learner contains several data-processing steps\n\njoining tables\nselecting columns\napplying machine-learning estimators\n\nSome steps have state that needs to be fitted\nOften several tables and aggregations are involved"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#example",
    "href": "pages/slides/expressions/expressions.html#example",
    "title": "Skrub learning materials",
    "section": "Example",
    "text": "Example\n\nWe have e-commerce check-out baskets\nEach containing one or more products\nPredict if the transaction is fraudulent\n\n\nDataset"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#a-first-attempt",
    "href": "pages/slides/expressions/expressions.html#a-first-attempt",
    "title": "Skrub learning materials",
    "section": "A first attempt …",
    "text": "A first attempt …\n\nScikit-learn assumes a single table X of the right shape"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#loading-data",
    "href": "pages/slides/expressions/expressions.html#loading-data",
    "title": "Skrub learning materials",
    "section": "Loading data",
    "text": "Loading data\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = data.baskets[[\"ID\"]]\ny = data.baskets[\"fraud_flag\"]\nproducts = data.products"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\n\nvectorized_products = product_vectorizer.fit_transform(data.products)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-1",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-1",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\n\nvectorized_products = product_vectorizer.fit_transform(data.products)\n🤔\n\n\nHow to store product_vectorizer?\nFitted on whole products table: data leakage\nCannot tune hyper-parameters\nTransforming only some columns is hard\n\nColumnTransformer 😟😰"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#joining-the-product-features",
    "href": "pages/slides/expressions/expressions.html#joining-the-product-features",
    "title": "Skrub learning materials",
    "section": "Joining the product features",
    "text": "Joining the product features\naggregated_products = (\n    vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\n)\nX = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\").drop(\n    columns=[\"ID\", \"basket_ID\"]\n)\n🤔\n\n\nHow to keep track of these transformations?\nCannot tune choices"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator",
    "href": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator",
    "title": "Skrub learning materials",
    "section": "Adding the supervised estimator",
    "text": "Adding the supervised estimator\nclassifier = HistGradientBoostingClassifier()\n\ncross_val_score(classifier, X, y, scoring=\"roc_auc\", n_jobs=5)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#skrub-to-the-rescue",
    "href": "pages/slides/expressions/expressions.html#skrub-to-the-rescue",
    "title": "Skrub learning materials",
    "section": "Skrub to the rescue",
    "text": "Skrub to the rescue\n\nBuild complex pipelines involving multiple tables"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#loading-data-1",
    "href": "pages/slides/expressions/expressions.html#loading-data-1",
    "title": "Skrub learning materials",
    "section": "Loading data",
    "text": "Loading data\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = skrub.X(data.baskets[[\"ID\"]])\ny = skrub.y(data.baskets[\"fraud_flag\"])\nproducts = skrub.var(\"products\", data.products)\n\n\nX, y, products represent inputs to the model\nOperations on those objects are evaluated lazily\n\nRecorded rather than evaluated immediately\nBut a preview is computed for interactive development\n\nThey forward all operations to the result of their evaluation\n\nFull API of the underlying object is available"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-2",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-2",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-3",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-3",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X\nproduct_vectorizer is added to the model"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-4",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-4",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(n_components=5)\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can filter products based on X\nproduct_vectorizer is added to the model\nWe can select columns to transform"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#encoding-the-products-5",
    "href": "pages/slides/expressions/expressions.html#encoding-the-products-5",
    "title": "Skrub learning materials",
    "section": "Encoding the products",
    "text": "Encoding the products\nfrom skrub import selectors as s\n\nproducts = products[products[\"basket_ID\"].isin(X[\"ID\"])]\n\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(\n        n_components=skrub.choose_int(2, 20)\n    )\n)\nvectorized_products = products.skb.apply(\n    product_vectorizer, cols=s.all() - \"basket_ID\"\n)\n\nWe can tune hyperparameters (more later)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#joining-the-product-features-1",
    "href": "pages/slides/expressions/expressions.html#joining-the-product-features-1",
    "title": "Skrub learning materials",
    "section": "Joining the product features",
    "text": "Joining the product features\naggregated_products = (\n    vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\n)\nX = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\").drop(\n    columns=[\"ID\", \"basket_ID\"]\n)\n\nTransformations added to the model\nCan tune choices\nWhile having access to all the dataframe’s functionality"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator-1",
    "href": "pages/slides/expressions/expressions.html#adding-the-supervised-estimator-1",
    "title": "Skrub learning materials",
    "section": "Adding the supervised estimator",
    "text": "Adding the supervised estimator\nclassifier = HistGradientBoostingClassifier()\npred = X.skb.apply(classifier, y=y)\nEvaluation\npred.skb.cross_validate(scoring=\"roc_auc\", n_jobs=5)\nTraining & using a model\n\n\ntrain.py\n\nestimator = pred.skb.get_estimator(fitted=True)\nwith open(\"estimator.pickle\", \"wb\") as ostream:\n    pickle.dump(estimator, ostream)\n\n\n\npredict.py\n\nwith open(\"estimator.pickle\", \"rb\") as istream:\n    estimator = pickle.load(istream)\n\nestimator.predict({'X': unseen_baskets, 'products': unseen_products})"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#easy-inspection",
    "href": "pages/slides/expressions/expressions.html#easy-inspection",
    "title": "Skrub learning materials",
    "section": "Easy inspection",
    "text": "Easy inspection\npred.skb.full_report()\n\nreport"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\n\nAny choice in the pipeline can be tuned\nOptions are specified inline\nInspecting results is easy"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning-1",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning-1",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nWithout skrub: 😭😭😭\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#section",
    "href": "pages/slides/expressions/expressions.html#section",
    "title": "Skrub learning materials",
    "section": "",
    "text": "NO!"
  },
  {
    "objectID": "pages/slides/expressions/expressions.html#hyperparameter-tuning-2",
    "href": "pages/slides/expressions/expressions.html#hyperparameter-tuning-2",
    "title": "Skrub learning materials",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nWith skrub: replace any value with a range\nproduct_vectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.StringEncoder(\n        n_components=skrub.choose_int(2, 20)\n    )\n)\n\n# ...\n\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()\n\n\nparallel coordinates plot"
  },
  {
    "objectID": "pages/notebooks/index_notebooks.html",
    "href": "pages/notebooks/index_notebooks.html",
    "title": "Skrub notebooks",
    "section": "",
    "text": "An introductory notebook for skrub\nComparing different categorical encoders."
  },
  {
    "objectID": "pages/listings.html",
    "href": "pages/listings.html",
    "title": "Latest",
    "section": "",
    "text": "Mar 26, 2025\n\n\nRiccardo Cappuzzo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/listings.html#blog-posts",
    "href": "pages/listings.html#blog-posts",
    "title": "Latest",
    "section": "",
    "text": "Mar 26, 2025\n\n\nRiccardo Cappuzzo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/listings.html#talks",
    "href": "pages/listings.html#talks",
    "title": "Latest",
    "section": "Talks",
    "text": "Talks"
  },
  {
    "objectID": "pages/notebooks/categorical-encoders/categorical-encoders.html",
    "href": "pages/notebooks/categorical-encoders/categorical-encoders.html",
    "title": "What’s the best way to encode categorical features? A use case with Skrub encoders",
    "section": "",
    "text": "Encoding categorical values (such as names, addresses, but also textual data) is a very common problem when it comes to prepare tabular data for training ML models, and Skrub provides four encoders to this end:\n\nskrub.MinhashEncoder, a simple encoder based on hashing categories.\nskrub.GapEncoder, which encoders strings based on latent categories estimated from the data.\nskrub.TextEncoder, a language model-based encoder that uses pre-trained language models to produce vectors for each string.\nskrub.StringEncoder, an encoder that vectorizes data with tf-idf and then applies SVD to reduce the number of features.\n\nThe objective of this post is to test the performance of each encoder on a few datasets in order to find out which methods should be considered in various circumstances.\n\nPreparing the datasets\nWe begin by importing and preparing the datasets that should be used for the experiments:\nfrom skrub.datasets import fetch_toxicity, fetch_movielens, fetch_employee_salaries, fetch_open_payments\n\ndatasets = {}\n## Open Payments (Classification)\ndataset = fetch_open_payments()\nX, y = dataset.X, dataset.y\ny = y.map({\"disallowed\": 0, \"allowed\": 1})\ndatasets[\"Open Payments\"] = (X,y,\"classification\")\n\n## Toxicity (Classification)\ndataset = fetch_toxicity()\nX, y = dataset.X, dataset.y\nX[\"is_toxic\"] = y\ny = X.pop(\"is_toxic\").map({\"Toxic\": 1, \"Not Toxic\": 0})\ndatasets[\"Toxicity\"] = (X,y, \"classification\")\n\n## Movielens (Regression)\ndataset = fetch_movielens()\nX, y = pl.from_pandas(dataset.movies), pl.from_pandas(dataset.ratings)\nX = (\n    X.join(y, on=\"movieId\")\n    .group_by(\"movieId\", \"title\", \"genres\")\n    .agg(target=pl.mean(\"rating\"))\n)\ny = X[\"target\"].to_numpy()\nX = X.drop(\"target\")\ndatasets[\"Movielens\"]=(X,y, \"regression\")\n\n## Employee salaries (Regression)\ndataset = fetch_employee_salaries()\nX = pl.from_pandas(dataset.employee_salaries)\ny = X[\"current_annual_salary\"]\nX = X.drop(\"current_annual_salary\")\n\ndatasets[\"Employee salaries\"]=(X,y, \"regression\")\n\n\nSetting up the experiments\nWe can test each method by building a scikit-learn pipeline for each categorical encoder, using the default HistGradientBoostingClassifier and HistGradientBoostingRegressor as prediction model.\nThem, we use the cross_validate function to track the fit and score time, as well as the prediction performance of each pipeline over different splits. For simplicity, we are not performing hyperparameter optimization for either the categorical encoder or the learner.\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import (\n    HistGradientBoostingClassifier,\n    HistGradientBoostingRegressor,\n)\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import make_pipeline\nimport polars as pl\nfrom skrub import (\n    GapEncoder,\n    MinHashEncoder,\n    StringEncoder,\n    TableVectorizer,\n    TextEncoder,\n)\n\ndef run_experiments(X, y, task, dataset_name):\n    if task == \"regression\":\n        model = HistGradientBoostingRegressor()\n        scoring = \"r2\"\n    else:\n        model = HistGradientBoostingClassifier()\n        scoring = \"roc_auc\"\n\n    results = []\n\n    # For each encoder, create a new pipeline\n    gap_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=GapEncoder(n_components=30)), model\n    )\n    minhash_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=MinHashEncoder(n_components=30)), model\n    )\n    text_encoder = TextEncoder(\n        \"sentence-transformers/paraphrase-albert-small-v2\",\n        device=\"cpu\",\n    )\n    text_encoder_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=text_encoder),\n        model,\n    )\n    string_encoder = StringEncoder(ngram_range=(3, 4), analyzer=\"char_wb\")\n    string_encoder_pipe = make_pipeline(\n        TableVectorizer(high_cardinality=string_encoder),\n        model,\n    )\n\n    pipes = [\n        (\"GapEncoder\", gap_pipe),\n        (\"MinHashEncoder\", minhash_pipe),\n        (\"TextEncoder\", text_encoder_pipe),\n        (\"StringEncoder\", string_encoder_pipe),\n    ]\n\n    for name, p in pipes:\n        cross_validate_results = cross_validate(p, X, y, scoring=scoring)\n        results.append(\n            pl.DataFrame(add_results(name, dataset_name, cross_validate_results))\n        )\n    df_results = pl.concat(results).with_columns(task=pl.lit(task))\n    return df_results\n\n\nRunning the experiments and saving the results\nFinally, I ran the crossvalidation step for each encoder on all dataset and I recorded the files in a csv file. This step took quite some time, and was done offline in a separater script.\nall_results = []\nfor dataset_name,v in datasets.items():\n    X, y, task = v\n    results = run_experiments(X, y, task, dataset_name) \n    all_results.append(results)\ndf_all_results = pl.concat(all_results)\ndf_all_results.write_csv(\"results-encoder_benchmark.csv\")\n\n\nPlotting the results\nNow, we can load the results and start plotting the results. We first split the results in two subtables based on the specific task (either regression or classification), to avoid mixing metrics.\n\nimport polars as pl\nimport matplotlib.pyplot as plt\ndf = pl.read_csv(\"results-encoder_benchmark.csv\")\ndf_regression = df.filter(task=\"regression\")\ndf_classification = df.filter(task=\"classification\")\n\nTo see the tradeoff between fit time and prediction performance, we use a scatterplot with error bars to find the average performance and run time for each method.\nThen, we can plot the prediction performance as a function of the run time.\n\ndef plot_scatter_errorbar(df, ylabel, sharey=False, suptitle=\"\"):\n    # Fixing the colors for each cluster of points and the error bars\n    tab10_colors = plt.get_cmap('tab10').colors\n    colors = dict(zip(df[\"estimator\"].unique().sort().to_list(),tab10_colors[:4]))\n    fig, axs = plt.subplots(1,2, sharey=sharey, layout=\"constrained\", figsize=(8,3))\n    # Each dataset gets a subplot\n    for idx, (dataset, g) in enumerate(df.group_by(\"dataset\")):\n        ax=axs[idx]\n        # Each estimator is plotted separately as a cluster of points\n        for estimator, gdf in g.group_by(\"estimator\"):\n            estim = estimator[0]\n            color = colors[estim]\n            x = gdf[\"fit_time\"].to_numpy()\n            y = gdf[\"test_score\"].to_numpy()\n            label = estim if idx == 0 else \"_\" + estim\n            ax.scatter(x=x, y=y, label=label, color=color)\n            \n            # find the mean and the error bars \n            xerr_mean = gdf[\"fit_time\"].mean()\n            yerr_mean = gdf[\"test_score\"].mean()\n            x_err = gdf[\"fit_time\"].std()\n            y_err = gdf[\"test_score\"].std()\n            # plot the error bars\n            ax.errorbar(xerr_mean, yerr_mean, xerr=x_err, fmt=\"none\", color=color)\n            ax.errorbar(xerr_mean, yerr_mean, yerr=y_err, fmt=\"none\", color=color)\n            \n        ax.set_title(dataset[0])\n        ax.set_xlabel(\"Fit time (s)\")\n        ax.set_ylabel(ylabel)\n        ax.set_xscale(\"log\")\n    fig.suptitle(suptitle)\n    fig.legend(loc=\"lower center\", ncols=2)\n\n\nplot_scatter_errorbar(df_classification, \"ROC-AUC\", sharey=True, suptitle=\"Classification\")\nplot_scatter_errorbar(df_regression, \"R2 score\", sharey=False, suptitle=\"Regression\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prediction performance is fairly consistent across methods, although this depends on the table under observation. MinhashEncoder and StringEncoder are consistently faster than the alternatives.\nTextEncoder is always much slower than the other methods, however it must be noted that this example was run on a CPU, rather than a much faster GPU.\nTo have a better idea of why some methods may outperform others, we should take a look at the actual tables. We can do so very easily thanks to the skrub TableReport object.\n\nfrom skrub import TableReport\nfrom skrub.datasets import fetch_toxicity, fetch_movielens, fetch_employee_salaries, fetch_open_payments\n\n\n# OPEN PAYMENTS\ndataset = fetch_open_payments()\nX, y = dataset.X, dataset.y\nTableReport(X)\n\nProcessing column   5 / 5\n\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# TOXICITY\ndataset = fetch_toxicity()\nX, y = dataset.X, dataset.y\nTableReport(X)\n\nProcessing column   1 / 1\n\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# MOVIELENS\ndataset = fetch_movielens()\nX, y = pl.from_pandas(dataset.movies), pl.from_pandas(dataset.ratings)\nX = (\n    X.join(y, on=\"movieId\")\n    .group_by(\"movieId\", \"title\", \"genres\")\n    .agg(target=pl.mean(\"rating\"))\n)\nX = X.drop(\"target\")\n\nTableReport(X)\n\nProcessing column   3 / 3\n\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\n\n# EMPLOYEE SALARIES\ndataset = fetch_employee_salaries()\nX = pl.from_pandas(dataset.employee_salaries)\nX = X.drop(\"current_annual_salary\")\nTableReport(X)\n\nProcessing column   8 / 8\n\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nAll datasets include high cardinality features which must be encoded using one of the skrub Encoders. The Toxicity dataset is different from the others in that it involves free-flowing text as tweets, while all other tables include a (possibly) large number of unique categories.\nThis explains why the TextEncoder is so much better than the other encoders on Toxicity, while its performance on the other datasets is more in line with the others.\nOn the other hand, the StringEncodershows a strong performance in all cases, while being top-2 on average for the fit time.\n\n\nIn summary\nThe skrub TableVectorizer transforms categorical features into numbers so that ML models can make better use of the information they contain. The StringEncoder can be considered the best all-rounder, being fast to train in most cases, while maintaining strong performance in general. The TextEncoder shines when textual data is available as it can make full use of the pre-trained language models it relies on. The MinHashEncoder and the GapEncoder are more specialized models that may work better than the alternatives in specific circumstances."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html",
    "title": "Skrub",
    "section": "",
    "text": "skrub-data.org\nLess wrangling, more machine learning"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#skrub-helps-at-several-stages-of-a-tabular-learning-project",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#skrub-helps-at-several-stages-of-a-tabular-learning-project",
    "title": "Skrub",
    "section": "Skrub helps at several stages of a tabular learning project",
    "text": "Skrub helps at several stages of a tabular learning project\n\nWhat’s in the data? (EDA)\nCan we learn anything? (baselines)\nHow do I represent the data? (feature extraction)\nHow do I bring it all together? (building a pipeline)"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tablereport-interactive-display-of-a-dataframe",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tablereport-interactive-display-of-a-dataframe",
    "title": "Skrub",
    "section": "TableReport: interactive display of a dataframe",
    "text": "TableReport: interactive display of a dataframe\n\nskrub.TableReport(employees, verbose=0)\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nWe can tell skrub to patch the default display of polars and pandas dataframes.\n\nskrub.patch_display(verbose=0)"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tabular_learner-a-pre-made-robust-baseline",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tabular_learner-a-pre-made-robust-baseline",
    "title": "Skrub",
    "section": "tabular_learner: a pre-made robust baseline",
    "text": "tabular_learner: a pre-made robust baseline\n\nlearner = skrub.tabular_learner(\"regressor\")\nlearner\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())]) tablevectorizer: TableVectorizerTableVectorizer(low_cardinality=ToCategorical()) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder() low_cardinalityToCategoricalToCategorical() high_cardinalityStringEncoderStringEncoder() HistGradientBoostingRegressor?Documentation for HistGradientBoostingRegressorHistGradientBoostingRegressor() \n\n\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(learner, X, y, scoring=\"r2\")\n\narray([0.90121408, 0.8776637 , 0.91159274, 0.92363764, 0.92534873])\n\n\nThe tabular_learner adapts to the supervised estimator we choose\n\nfrom sklearn.linear_model import Ridge\n\nlearner = skrub.tabular_learner(Ridge())\nlearner\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())]) tablevectorizer: TableVectorizerTableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline')) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder(periodic_encoding='spline') low_cardinalityOneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False) high_cardinalityStringEncoderStringEncoder() SimpleImputer?Documentation for SimpleImputerSimpleImputer(add_indicator=True) StandardScaler?Documentation for StandardScalerStandardScaler() Ridge?Documentation for RidgeRidge() \n\n\n\ncross_val_score(learner, X, y, scoring=\"r2\")\n\narray([0.77747104, 0.74636601, 0.78608641, 0.77574873, 0.79073481])"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#tablevectorizer-apply-an-appropriate-transformer-to-each-column",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#tablevectorizer-apply-an-appropriate-transformer-to-each-column",
    "title": "Skrub",
    "section": "TableVectorizer: apply an appropriate transformer to each column",
    "text": "TableVectorizer: apply an appropriate transformer to each column\n\nvectorizer = skrub.TableVectorizer()\ntransformed = vectorizer.fit_transform(X)\n\nThe TableVectorizer identifies several kinds of columns:\n\ncategorical, low cardinality\ncategorical, high cardinality\ndatetime\nnumeric\n… we may add more\n\n\nfrom pprint import pprint\n\npprint(vectorizer.column_to_kind_)\n\n{'assignment_category': 'low_cardinality',\n 'date_first_hired': 'datetime',\n 'department': 'low_cardinality',\n 'department_name': 'low_cardinality',\n 'division': 'high_cardinality',\n 'employee_position_title': 'high_cardinality',\n 'gender': 'low_cardinality',\n 'year_first_hired': 'numeric'}\n\n\nFor each kind, it applies an appropriate transformer\n\nvectorizer.transformers_[\"department\"]  # low-cardinality categorical\n\nOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneHotEncoder?Documentation for OneHotEncoderiFittedOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False) \n\n\n\nvectorizer.transformers_[\"employee_position_title\"]  # high-cardinality categorical\n\nStringEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StringEncoderiFittedStringEncoder() \n\n\n\nvectorizer.transformers_[\"date_first_hired\"]  # datetime\n\nDatetimeEncoder()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DatetimeEncoderiFittedDatetimeEncoder() \n\n\n… and those transformers turn the input into numeric features that can be used for ML\n\ntransformed[vectorizer.input_to_outputs_[\"date_first_hired\"]]\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nFor high-cardinality categorical columns the default GapEncoder identifies sparse topics (more later).\n\ntransformed[vectorizer.input_to_outputs_[\"employee_position_title\"]]\n\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe transformer used for each column kind can be easily configured."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#preprocessing-in-the-tablevectorizer",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#preprocessing-in-the-tablevectorizer",
    "title": "Skrub",
    "section": "Preprocessing in the TableVectorizer",
    "text": "Preprocessing in the TableVectorizer\nThe TableVectorizer actually performs a lot of preprocessing before applying the final transformers, such as:\n\nensuring consistent column names\ndetecting missing values such as \"N/A\"\ndropping empty columns\nhandling pandas dtypes – float64, nan vs Float64, NA\nparsing numbers\nparsing dates, ensuring consistent dtype and timezone\nconverting numbers to float32 for faster computation & less memory downstream\n…\n\n\npprint(vectorizer.all_processing_steps_[\"date_first_hired\"])\n\n[CleanNullStrings(),\n DropUninformative(),\n ToDatetime(),\n DatetimeEncoder(),\n {'date_first_hired_day': ToFloat32(), 'date_first_hired_month': ToFloat32(), ...}]"
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#extracting-good-features",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#extracting-good-features",
    "title": "Skrub",
    "section": "Extracting good features",
    "text": "Extracting good features\nSkrub offers several encoders to extract features from different columns. In particular from categorical columns.\n\nGapEncoder\nCategories are somewhere between text and an enumeration… The GapEncoder is somewhere between a topic model and a one-hot encoder!\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\ngap = skrub.GapEncoder()\npos_title = X[\"employee_position_title\"]\nloadings = gap.fit_transform(pos_title).set_index(pos_title.values).head()\n\nloadings.columns = [c.split(\": \")[1] for c in loadings.columns]\nsns.heatmap(loadings)\n_ = plt.setp(plt.gca().get_xticklabels(), rotation=45, ha=\"right\")\n\n\n\n\n\n\n\n\n\n\nTextEncoder\nExtract embeddings from a text column using any model from the HuggingFace Hub.\n\nimport pandas as pd\n\nX = pd.Series([\"airport\", \"flight\", \"plane\", \"pineapple\", \"fruit\"])\nencoder = skrub.TextEncoder(model_name=\"all-MiniLM-L6-v2\", n_components=None)\nembeddings = encoder.fit_transform(X).set_index(X.values)\n\nsns.heatmap(embeddings @ embeddings.T)\n\n\n\n\n\n\n\n\n\n\nMinHashEncoder\nA fast, stateless way of encoding strings that works especially well with models based on decision trees (gradient boosting, random forest)."
  },
  {
    "objectID": "pages/notebooks/skrub-intro/skrub-intro.html#more-interactive-and-expressive-pipelines",
    "href": "pages/notebooks/skrub-intro/skrub-intro.html#more-interactive-and-expressive-pipelines",
    "title": "Skrub",
    "section": "More interactive and expressive pipelines",
    "text": "More interactive and expressive pipelines\nTo go further than what can be done with scikit-learn Pipelines and the skrub transformers shown above, we are developing new utilities to easily define and inspect flexible pipelines that can process several dataframes.\nA prototype will be shown in a separate notebook."
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#fun-facts",
    "href": "pages/slides/soda-kickoff/slides.html#fun-facts",
    "title": "Skrub",
    "section": "Fun facts",
    "text": "Fun facts\n\nI’m Italian, but I don’t drink coffee, wine, and I like pizza with fries\nI did my PhD in Côte d’Azur, and I moved away because it was too sunny and I don’t like the sea"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#fun-facts-1",
    "href": "pages/slides/soda-kickoff/slides.html#fun-facts-1",
    "title": "Skrub",
    "section": "Fun facts",
    "text": "Fun facts\n\n\nI’m mildly obsessed with matplotlib"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#an-example-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#an-example-pipeline",
    "title": "Skrub",
    "section": "An example pipeline",
    "text": "An example pipeline\n\nGather some data\nExplore the data\nPre-process the data\nPerform feature engineering\nBuild a scikit-learn pipeline\n???\nProfit?"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data",
    "title": "Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skrub\n\ndataset = skrub.datasets.fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\ndf = pd.DataFrame(employees)\n\n# Plot the distribution of the numerical values using a histogram\nfig, axs = plt.subplots(2,1, figsize=(10, 6))\nax1, ax2 = axs\n\nax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\nax1.set_xlabel('Year first hired')\nax1.set_ylabel('Frequency')\nax1.grid(True, linestyle='--', alpha=0.5)\n\n# Count the frequency of each category\ncategory_counts = df['department'].value_counts()\n\n# Create a bar plot\ncategory_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n\n# Add labels and title\nax2.set_xlabel('Department')\nax2.set_ylabel('Frequency')\nax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n\nfig.suptitle(\"Distribution of values\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data-1",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data-1",
    "title": "Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#exploring-the-data-with-skrub",
    "href": "pages/slides/soda-kickoff/slides.html#exploring-the-data-with-skrub",
    "title": "Skrub",
    "section": "Exploring the data with skrub",
    "text": "Exploring the data with skrub\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\n\nMain features:\n\nObtain high-level statistics about the data\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas",
    "href": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas",
    "title": "Skrub",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\nimport pandas as pd\nimport numpy as np\n\ndata = {\n    'A': [1, 1, 1],  # Single unique value\n    'B': [2, 3, 2],  # Multiple unique values\n    'C': ['x', 'x', 'x'],  # Single unique value\n    'D': [4, 5, 6],  # Multiple unique values\n    'E': [np.nan, np.nan, np.nan],  # All missing values \n    'F': ['', '', ''],  # All empty strings\n    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n}\ndf = pd.DataFrame(data)\ndisplay(df)\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nDate\n\n\n\n\n0\n1\n2\nx\n4\nNaN\n\n01/01/2023\n\n\n1\n1\n3\nx\n5\nNaN\n\n02/01/2023\n\n\n2\n1\n2\nx\n6\nNaN\n\n03/01/2023"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas-1",
    "href": "pages/slides/soda-kickoff/slides.html#data-cleaning-with-pandas-1",
    "title": "Skrub",
    "section": "Data cleaning with Pandas",
    "text": "Data cleaning with Pandas\n\n# Parse the datetime strings with a specific format\ndf['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n\n# Drop columns with only a single unique value\ndf_cleaned = df.loc[:, df.nunique(dropna=True) &gt; 1]\n\n# Function to drop columns with only missing values or empty strings\ndef drop_empty_columns(df):\n    # Drop columns with only missing values\n    df_cleaned = df.dropna(axis=1, how='all')\n    # Drop columns with only empty strings\n    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n    return df_cleaned\n\n# Apply the function to the DataFrame\ndf_cleaned = drop_empty_columns(df_cleaned)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#lightweight-data-cleaning-cleaner",
    "href": "pages/slides/soda-kickoff/slides.html#lightweight-data-cleaning-cleaner",
    "title": "Skrub",
    "section": "Lightweight data cleaning: Cleaner",
    "text": "Lightweight data cleaning: Cleaner\n\nfrom skrub import Cleaner\ncleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\ndf_cleaned = cleaner.fit_transform(df)\ndisplay(df_cleaned)\n\n\n\n\n\n\n\n\nB\nD\nDate\n\n\n\n\n0\n2\n4\n2023-01-01\n\n\n1\n3\n5\n2023-01-02\n\n\n2\n2\n6\n2023-01-03"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas",
    "title": "Skrub",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\nimport pandas as pd\ndata = {\n    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n    'value': [10, 20, 30]\n}\ndf = pd.DataFrame(data)\ndf_expanded = df.copy()\ndatetime_column = \"date\"\ndf_expanded[datetime_column] = pd.to_datetime(df_expanded[datetime_column], errors='coerce')\n\ndf_expanded['year'] = df_expanded[datetime_column].dt.year\ndf_expanded['month'] = df_expanded[datetime_column].dt.month\ndf_expanded['day'] = df_expanded[datetime_column].dt.day\ndf_expanded['hour'] = df_expanded[datetime_column].dt.hour\ndf_expanded['minute'] = df_expanded[datetime_column].dt.minute\ndf_expanded['second'] = df_expanded[datetime_column].dt.second"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas-1",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-with-pandas-1",
    "title": "Skrub",
    "section": "Encoding datetime features with Pandas",
    "text": "Encoding datetime features with Pandas\n\ndf_expanded['hour_sin'] = np.sin(2 * np.pi * df_expanded['hour'] / 24)\ndf_expanded['hour_cos'] = np.cos(2 * np.pi * df_expanded['hour'] / 24)\n\ndf_expanded['month_sin'] = np.sin(2 * np.pi * df_expanded['month'] / 12)\ndf_expanded['month_cos'] = np.cos(2 * np.pi * df_expanded['month'] / 12)\n\nprint(\"Original DataFrame:\")\nprint(df)\nprint(\"\\nDataFrame with expanded datetime columns:\")\nprint(df_expanded)\n\nOriginal DataFrame:\n                  date  value\n0  2023-01-01 12:34:56     10\n1  2023-02-15 08:45:23     20\n2  2023-03-20 18:12:45     30\n\nDataFrame with expanded datetime columns:\n                 date  value  year  month  day  hour  minute  second  \\\n0 2023-01-01 12:34:56     10  2023      1    1    12      34      56   \n1 2023-02-15 08:45:23     20  2023      2   15     8      45      23   \n2 2023-03-20 18:12:45     30  2023      3   20    18      12      45   \n\n       hour_sin      hour_cos  month_sin     month_cos  \n0  1.224647e-16 -1.000000e+00   0.500000  8.660254e-01  \n1  8.660254e-01 -5.000000e-01   0.866025  5.000000e-01  \n2 -1.000000e+00 -1.836970e-16   1.000000  6.123234e-17"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "href": "pages/slides/soda-kickoff/slides.html#encoding-datetime-features-skrub.datetimeencoder",
    "title": "Skrub",
    "section": "Encoding datetime features skrub.DatetimeEncoder",
    "text": "Encoding datetime features skrub.DatetimeEncoder\n\nfrom skrub import DatetimeEncoder, ToDatetime\n\nde = DatetimeEncoder(periodic_encoding=\"circular\")\nX_date = ToDatetime().fit_transform(df[\"date\"])\nX_enc = de.fit_transform(X_date)\nprint(X_enc)\n\n   date_year  date_total_seconds  date_month_circular_0  \\\n0     2023.0        1.672577e+09               0.500000   \n1     2023.0        1.676451e+09               0.866025   \n2     2023.0        1.679336e+09               1.000000   \n\n   date_month_circular_1  date_day_circular_0  date_day_circular_1  \\\n0           8.660254e-01         2.079117e-01             0.978148   \n1           5.000000e-01         1.224647e-16            -1.000000   \n2           6.123234e-17        -8.660254e-01            -0.500000   \n\n   date_hour_circular_0  date_hour_circular_1  \n0          1.224647e-16         -1.000000e+00  \n1          8.660254e-01         -5.000000e-01  \n2         -1.000000e+00         -1.836970e-16  \n\n\n} ## Encoding all the features: TableVectorizer {.smaller auto-animate=“true”}"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-1",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-2",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "href": "pages/slides/soda-kickoff/slides.html#build-a-predictive-pipeline-with-tabular_learner",
    "title": "Skrub",
    "section": "Build a predictive pipeline with tabular_learner",
    "text": "Build a predictive pipeline with tabular_learner\n\nimport skrub\nfrom sklearn.linear_model import Ridge\nmodel = skrub.tabular_learner(Ridge())\nmodel\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())]) tablevectorizer: TableVectorizerTableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline')) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder(periodic_encoding='spline') low_cardinalityOneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False) high_cardinalityStringEncoderStringEncoder() SimpleImputer?Documentation for SimpleImputerSimpleImputer(add_indicator=True) StandardScaler?Documentation for StandardScalerStandardScaler() Ridge?Documentation for RidgeRidge()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#we-now-have-a-pipeline",
    "href": "pages/slides/soda-kickoff/slides.html#we-now-have-a-pipeline",
    "title": "Skrub",
    "section": "We now have a pipeline!",
    "text": "We now have a pipeline!\n\nGather some data\n\nskrub.datasets, or user data\n\nExplore the data\n\nskrub.TableReport\n\nPre-process the data\n\nskrub.TableVectorizer, Cleaner, DatetimeEncoder …\n\nPerform feature engineering\n\nskrub.TableVectorizer, TextEncoder, StringEncoder…\n\nBuild a scikit-learn pipeline\n\ntabular_learner, sklearn.pipeline.make_pipeline …\n\n???\nProfit 📈"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#skrub-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#skrub-expressions",
    "title": "Skrub",
    "section": "skrub expressions",
    "text": "skrub expressions\nWhen a normal pipe is not enough…\n\nExpressions come to the rescue 🚒:\n\n\n\nKeep track of train, validation and test splits to avoid data leakage\nSimplify hyperparameter tuning and reporting\nHandle complex pipelines that involve multiple tables and custom\nPersist all objects for reproducibility"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#starting-with-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#starting-with-expressions",
    "title": "Skrub",
    "section": "Starting with expressions",
    "text": "Starting with expressions\ndata = skrub.datasets.fetch_credit_fraud()\n\nX = skrub.X(data.baskets[[\"ID\"]]) # mark as \"X\"\ny = skrub.y(data.baskets[\"fraud_flag\"]) # mark as \"y\"\nproducts = skrub.var(\"products\", data.products) # add a new variable\n\n\nX, y, products represent inputs to the pipeline\nskrub keeps track of splits"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#build-and-inspect-complex-pipelines",
    "href": "pages/slides/soda-kickoff/slides.html#build-and-inspect-complex-pipelines",
    "title": "Skrub",
    "section": "Build and inspect complex pipelines",
    "text": "Build and inspect complex pipelines\npred.skb.full_report()\n\nreport"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-in-scikit-learn",
    "href": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-in-scikit-learn",
    "title": "Skrub",
    "section": "Hyperparameter tuning in scikit-learn",
    "text": "Hyperparameter tuning in scikit-learn\npipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\ngrid = [\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [Ridge()],\n        \"regressor__alpha\": loguniform(0.1, 10.0),\n    },\n    {\n        \"dim_reduction\": [PCA()],\n        \"dim_reduction__n_components\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n    {\n        \"dim_reduction\": [SelectKBest()],\n        \"dim_reduction__k\": [10, 20, 30],\n        \"regressor\": [RandomForestClassifier()],\n        \"regressor__n_estimators\": loguniform(20, 200),\n    },\n]\nmodel = RandomizedSearchCV(pipe, grid)"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-with-skrub-expressions",
    "href": "pages/slides/soda-kickoff/slides.html#hyperparameter-tuning-with-skrub-expressions",
    "title": "Skrub",
    "section": "Hyperparameter tuning with skrub expressions",
    "text": "Hyperparameter tuning with skrub expressions\ndim_reduction = X.skb.apply(\n    skrub.choose_from(\n        {\n            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n        }, name=\"dim_reduction\"\n    )\n)\nregressor = dim_reduction.skb.apply(\n    skrub.choose_from(\n        {\n            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n            \"RandomForest\": RandomForestClassifier(\n                n_estimators=skrub.choose_int(20, 200, log=True)\n            )\n        }, name=\"regressor\"\n    )\n)\nregressor.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#observe-the-impact-of-the-hyperparameters",
    "href": "pages/slides/soda-kickoff/slides.html#observe-the-impact-of-the-hyperparameters",
    "title": "Skrub",
    "section": "Observe the impact of the hyperparameters",
    "text": "Observe the impact of the hyperparameters\nsearch = pred.skb.get_randomized_search(scoring=\"roc_auc\", fitted=True)\n\nsearch.plot_parallel_coord()"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#tldw",
    "href": "pages/slides/soda-kickoff/slides.html#tldw",
    "title": "Skrub",
    "section": "tl;dw",
    "text": "tl;dw\nskrub provides\n\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering\nsoon™️, complex pipelines, hyperparameter tuning, (almost) no leakage"
  },
  {
    "objectID": "pages/slides/soda-kickoff/slides.html#getting-involved",
    "href": "pages/slides/soda-kickoff/slides.html#getting-involved",
    "title": "Skrub",
    "section": "Getting involved",
    "text": "Getting involved\n\n\nSkrub website (QR code below!)\nSkrub materials website\nGit repository\nDiscord server\nBluesky"
  },
  {
    "objectID": "pages/slides/index_slides.html",
    "href": "pages/slides/index_slides.html",
    "title": "Skrub slides and talks",
    "section": "",
    "text": "This page is used to track all the publicly available material about Skrub.\n\nIntroductory talks\n\n🤝 Introduction to Skrub and main features - Skrub Workshop, January 2025\n Skrub, Machine learning with dataframes - SODA Kickoff, June 2025\n\n📑 Examples of TableReport\n\n\n\nFuture developments\n\n🌠 Skrub recipe and expressions - Skrub Workshop, January 2025\n⌚ Timeseries and KKBox - Skrub Workshop, January 2025\n🔎 The Discover object - Skrub Workshop, January 2025\n🎓 A Skrub use case in Academia - Skrub Workshop, January 2025"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#plan-for-the-presentation",
    "href": "pages/slides/skrub-intro/skrub-intro.html#plan-for-the-presentation",
    "title": "Skrub",
    "section": "Plan for the presentation",
    "text": "Plan for the presentation\n\nIntroducing skrub\n\nExample use case\nDetailed explanation of the features\nGetting involved"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#in-the-beginning",
    "href": "pages/slides/skrub-intro/skrub-intro.html#in-the-beginning",
    "title": "Skrub",
    "section": "In the beginning…",
    "text": "In the beginning…\nSkrub stems from the development of dirty_cat, a package that provided support for handling dirty columns and perform fuzzy joins across tables.\n\nIt has since evolved into a package that provides:\n\n\ninteractive data exploration\nautomated pre-processing of pandas and polars dataframes\npowerful feature engineering"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#skrubs-vision",
    "href": "pages/slides/skrub-intro/skrub-intro.html#skrubs-vision",
    "title": "Skrub",
    "section": "Skrub’s vision",
    "text": "Skrub’s vision\nThe goal of skrub is to facilitate building and deploying machine-learning models on pandas and polars dataframes (later, SQL databases…)\n\n\n\nSkrub is high-level, with a philosophy and an API matching that of scikit-learn. It strives to bridge the worlds of databases and machine-learning, enabling imperfect assembly and representations of the data when it is noisy."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#an-example-use-case",
    "href": "pages/slides/skrub-intro/skrub-intro.html#an-example-use-case",
    "title": "Skrub",
    "section": "An example use case",
    "text": "An example use case\n\nGather some data\n\nEmployee salaries, census, customer churn…\n\nExplore the data\n\nNull values, dtypes, correlated features…\n\nPre-process the data\nBuild a scikit-learn estimator\n???\nProfit 📈"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data",
    "href": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data",
    "title": "Skrub",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nimport skrub\nimport pandas as pd\nfrom skrub.datasets import fetch_employee_salaries\n\ndataset = fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\n\nemployees.describe(include=\"all\")\n\n\n\n\n\n\n\n\ngender\ndepartment\ndepartment_name\ndivision\nassignment_category\nemployee_position_title\ndate_first_hired\nyear_first_hired\n\n\n\n\ncount\n9211\n9228\n9228\n9228\n9228\n9228\n9228\n9228.000000\n\n\nunique\n2\n37\n37\n694\n2\n443\n2264\nNaN\n\n\ntop\nM\nPOL\nDepartment of Police\nSchool Health Services\nFulltime-Regular\nBus Operator\n12/12/2016\nNaN\n\n\nfreq\n5481\n1844\n1844\n300\n8394\n638\n87\nNaN\n\n\nmean\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2003.597529\n\n\nstd\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n9.327078\n\n\nmin\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1965.000000\n\n\n25%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1998.000000\n\n\n50%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2005.000000\n\n\n75%\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2012.000000\n\n\nmax\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2016.000000"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data-interactively",
    "href": "pages/slides/skrub-intro/skrub-intro.html#exploring-the-data-interactively",
    "title": "Skrub",
    "section": "Exploring the data… interactively!",
    "text": "Exploring the data… interactively!\nfrom skrub import TableReport\nTableReport(employee_salaries)\nPreview\n\nMain features:\n\n\nObtain high-level statistics about the data (number of uniques, missing values…)\nExplore the distribution of values and find outliers\nDiscover highly correlated columns\nExport and share the report as an html file\n\n\nMore examples here"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-1",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nmodel = Ridge()"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-2",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nmodel = make_pipeline(StandardScaler(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-3",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nmodel = make_pipeline(SimpleImputer(), StandardScaler(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-4",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-5",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-5",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import make_column_transformer\n\ncategorical_columns = selector(dtype_include=object)(employees)\nnumerical_columns = selector(dtype_exclude=object)(employees)\n\nct = make_column_transformer(\n      (StandardScaler(),\n       numerical_columns),\n      (OneHotEncoder(handle_unknown=\"ignore\"),\n       categorical_columns))\n\nmodel = make_pipeline(ct, SimpleImputer(), Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-6",
    "href": "pages/slides/skrub-intro/skrub-intro.html#build-a-predictive-pipeline-6",
    "title": "Skrub",
    "section": "Build a predictive pipeline",
    "text": "Build a predictive pipeline\n\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['year_first_hired']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['gender', 'department',\n                                                   'department_name',\n                                                   'division',\n                                                   'assignment_category',\n                                                   'employee_position_title',\n                                                   'date_first_hired'])])),\n                ('simpleimputer', SimpleImputer()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  ['year_first_hired']),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['gender', 'department',\n                                                   'department_name',\n                                                   'division',\n                                                   'assignment_category',\n                                                   'employee_position_title',\n                                                   'date_first_hired'])])),\n                ('simpleimputer', SimpleImputer()), ('ridge', Ridge())]) columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 ['year_first_hired']),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['gender', 'department', 'department_name',\n                                  'division', 'assignment_category',\n                                  'employee_position_title',\n                                  'date_first_hired'])]) standardscaler['year_first_hired'] StandardScaler?Documentation for StandardScalerStandardScaler() onehotencoder['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title', 'date_first_hired'] OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') SimpleImputer?Documentation for SimpleImputerSimpleImputer() Ridge?Documentation for RidgeRidge()"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner",
    "title": "Skrub",
    "section": "Enter: tabular_learner",
    "text": "Enter: tabular_learner\nimport skrub\nfrom sklearn.linear_model import Ridge\ntl = skrub.tabular_learner(Ridge())"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#enter-tabular_learner-1",
    "title": "Skrub",
    "section": "Enter: tabular_learner",
    "text": "Enter: tabular_learner\n\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline'))),\n                ('simpleimputer', SimpleImputer(add_indicator=True)),\n                ('standardscaler', StandardScaler()), ('ridge', Ridge())]) tablevectorizer: TableVectorizerTableVectorizer(datetime=DatetimeEncoder(periodic_encoding='spline')) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder(periodic_encoding='spline') low_cardinalityOneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',\n              sparse_output=False) high_cardinalityStringEncoderStringEncoder() SimpleImputer?Documentation for SimpleImputerSimpleImputer(add_indicator=True) StandardScaler?Documentation for StandardScalerStandardScaler() Ridge?Documentation for RidgeRidge()"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#a-robust-baseline-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#a-robust-baseline-tabular_learner",
    "title": "Skrub",
    "section": "A robust baseline: tabular_learner",
    "text": "A robust baseline: tabular_learner\nGiven a scikit-learn estimator, tabular_learner:\n\nextracts numerical features\nimputes missing values with SimpleImputer (optional)\nscales the data with StandardScaler (optional)\n\n\nYou can also write “tabular_learner(\"regressor\")”:\n\n\nPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('tablevectorizer',\n                 TableVectorizer(low_cardinality=ToCategorical())),\n                ('histgradientboostingregressor',\n                 HistGradientBoostingRegressor())]) tablevectorizer: TableVectorizerTableVectorizer(low_cardinality=ToCategorical()) numericPassThroughPassThrough() datetimeDatetimeEncoderDatetimeEncoder() low_cardinalityToCategoricalToCategorical() high_cardinalityStringEncoderStringEncoder() HistGradientBoostingRegressor?Documentation for HistGradientBoostingRegressorHistGradientBoostingRegressor()"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#unmasking-the-tabular_learner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#unmasking-the-tabular_learner",
    "title": "Skrub",
    "section": "Unmasking the tabular_learner",
    "text": "Unmasking the tabular_learner"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer",
    "title": "Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\n\nPre-process the data\nConvert complex data types (datetimes, text) into numerical features"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-1",
    "title": "Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\nPre-process the data\n\n\nEnsure consistent column names\nDetect missing values such as “N/A”\nDrop empty columns\nCheck and convert dtypes to np.float32\nParse dates, ensuring consistent dtype and timezone\nIdentify which categorical features are low- and high-cardinality"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-2",
    "title": "Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-3",
    "title": "Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\nConvert complex data types (datetimes, text) into numerical features\n\nEncode dates with DateTimeEncoder\nEncode low-cardinality features (&lt;=30 cat.) with OneHotEncoder\nEncode high-cardinality features (&gt;30 cat.) with:\n\nGapEncoder: Relatively slow, easily interpretable, good quality embeddings. Target encoding and hashing.\nMinHashEncoder: Very fast, somewhat low quality embeddings. Hashing ngrams.\nTextEncoder: Very slow, relies on language models, best solution for text and when context is available.\nStringEncoder: Best trade-off between compute cost and embeddings quality. Tf-idf followed by SVD.\n\n\n\nHigh-cardinality encoders are robust in presence of typos and dirty data."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#under-the-hood-tablevectorizer-4",
    "title": "Skrub",
    "section": "Under the hood: TableVectorizer",
    "text": "Under the hood: TableVectorizer\n\nvectorizer = skrub.TableVectorizer()\ntransformed = vectorizer.fit_transform(employees)\nfrom pprint import pprint\n\npprint(vectorizer.column_to_kind_)\n\n{'assignment_category': 'low_cardinality',\n 'date_first_hired': 'datetime',\n 'department': 'low_cardinality',\n 'department_name': 'low_cardinality',\n 'division': 'high_cardinality',\n 'employee_position_title': 'high_cardinality',\n 'gender': 'low_cardinality',\n 'year_first_hired': 'numeric'}\n\n\n\npprint(vectorizer.all_processing_steps_[\"date_first_hired\"])\n\n[CleanNullStrings(),\n DropUninformative(),\n ToDatetime(),\n DatetimeEncoder(),\n {'date_first_hired_day': ToFloat32(), 'date_first_hired_month': ToFloat32(), ...}]"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features",
    "title": "Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nimport pandas as pd\n\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/skrub-data/datasets/master\"\n    \"/data/bike-sharing-dataset.csv\"\n)\n# Extract our input data (X) and the target column (y)\ny = data[\"cnt\"]\nX = data[[\"date\", \"holiday\", \"temp\", \"hum\", \"windspeed\", \"weathersit\"]]\n\nX\n\n\n\n\n\n\n\n\ndate\nholiday\ntemp\nhum\nwindspeed\nweathersit\n\n\n\n\n0\n2011-01-01 00:00:00\n0\n0.24\n0.81\n0.0000\n1\n\n\n1\n2011-01-01 01:00:00\n0\n0.22\n0.80\n0.0000\n1\n\n\n2\n2011-01-01 02:00:00\n0\n0.22\n0.80\n0.0000\n1\n\n\n3\n2011-01-01 03:00:00\n0\n0.24\n0.75\n0.0000\n1\n\n\n4\n2011-01-01 04:00:00\n0\n0.24\n0.75\n0.0000\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n17374\n2012-12-31 19:00:00\n0\n0.26\n0.60\n0.1642\n2\n\n\n17375\n2012-12-31 20:00:00\n0\n0.26\n0.60\n0.1642\n2\n\n\n17376\n2012-12-31 21:00:00\n0\n0.26\n0.60\n0.1642\n1\n\n\n17377\n2012-12-31 22:00:00\n0\n0.26\n0.56\n0.1343\n1\n\n\n17378\n2012-12-31 23:00:00\n0\n0.26\n0.65\n0.1343\n1\n\n\n\n\n17379 rows × 6 columns"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-1",
    "title": "Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nfrom pprint import pprint\nfrom skrub import TableVectorizer, DatetimeEncoder\n\ntable_vec_weekday = TableVectorizer(datetime=DatetimeEncoder(add_weekday=True)).fit(X)\npprint(table_vec_weekday.get_feature_names_out())\n\narray(['date_year', 'date_month', 'date_day', 'date_hour',\n       'date_total_seconds', 'date_weekday', 'holiday', 'temp', 'hum',\n       'windspeed', 'weathersit'], dtype='&lt;U18')"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-2",
    "title": "Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\npipeline_weekday = make_pipeline(table_vec_weekday, HistGradientBoostingRegressor())\n\ncross_val_score(\n    pipeline_weekday, X, y, scoring=\"neg_mean_squared_error\",\n    cv=TimeSeriesSplit(n_splits=5),\n)\n\narray([ -3694.45159469,  -3180.1148674 , -15183.44808403,  -5119.10808046,\n        -5285.04971533])"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-3",
    "title": "Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#encoding-datetime-features-4",
    "title": "Skrub",
    "section": "Encoding datetime features",
    "text": "Encoding datetime features\n\n\n\nTimeseries support in skrub is still in its early stages! Please stay tuned for new developments."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining",
    "title": "Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-1",
    "title": "Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-2",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-2",
    "title": "Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-3",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-3",
    "title": "Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values",
    "href": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values",
    "title": "Skrub",
    "section": "AggJoiner: automatically aggregate values",
    "text": "AggJoiner: automatically aggregate values"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#aggjoiner-automatically-aggregate-values-1",
    "title": "Skrub",
    "section": "AggJoiner: automatically aggregate values",
    "text": "AggJoiner: automatically aggregate values\n\nimport skrub\nimport pandas as pd\n\ndf1 = pd.DataFrame({\n    \"UID\": [28, 32, 28], \n    \"Basket ID\": [1100, 1300, 1400]})\ndf2 = pd.DataFrame({\n    \"Basket ID\": [1100, 1100, 1100, 1300, 1400], \n    \"Product ID\": [\"A521\", \"B695\", \"F221\", \"W214\", \"B695\",], \n    \"Price\": [25, 30, 10, 320, 30]})\n\njoiner = skrub.AggJoiner(df2, operations=\"sum\", key=\"Basket ID\", cols=\"Price\")\njoiner.fit_transform(df1)\n\n\n\n\n\n\n\n\nUID\nBasket ID\nPrice_sum\n\n\n\n\n0\n28\n1100\n65\n\n\n1\n32\n1300\n320\n\n\n2\n28\n1400\n30"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-infer-missing-values",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-infer-missing-values",
    "title": "Skrub",
    "section": "InterpolationJoiner: infer missing values",
    "text": "InterpolationJoiner: infer missing values\n\nimport pandas as pd\n\nfrom skrub.datasets import fetch_flight_delays\n\ndataset = fetch_flight_delays()\nweather = dataset.weather\nweather = weather.sample(100_000, random_state=0, ignore_index=True)\nstations = dataset.stations\nweather = stations.merge(weather, on=\"ID\")[\n    [\"LATITUDE\", \"LONGITUDE\", \"YEAR/MONTH/DAY\", \"TMAX\", \"PRCP\", \"SNOW\"]\n]\nweather[\"YEAR/MONTH/DAY\"] = pd.to_datetime(weather[\"YEAR/MONTH/DAY\"])\n\n\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nYEAR/MONTH/DAY\nTMAX\nPRCP\nSNOW\n\n\n\n\n0\n25.333\n55.517\n2008-11-16\n297.0\nNaN\nNaN\n\n\n1\n25.333\n55.517\n2008-04-12\n333.0\nNaN\nNaN\n\n\n2\n25.255\n55.364\n2008-08-28\n430.0\n0.0\nNaN\n\n\n3\n25.255\n55.364\n2008-02-17\n264.0\n0.0\nNaN\n\n\n4\n25.255\n55.364\n2008-11-25\n291.0\nNaN\nNaN"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner",
    "title": "Skrub",
    "section": "InterpolationJoiner",
    "text": "InterpolationJoiner\n\nfrom skrub import InterpolationJoiner\njoiner = InterpolationJoiner(\n    aux_table,\n    key=[\"LATITUDE\", \"LONGITUDE\", \"YEAR/MONTH/DAY\"],\n    suffix=\"_predicted\",\n).fit(main_table)\njoin = joiner.transform(main_table)\njoin.head()\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nYEAR/MONTH/DAY\nTMAX\nPRCP\nSNOW\nTMAX_predicted\nPRCP_predicted\nSNOW_predicted\n\n\n\n\n0\n25.333\n55.517\n2008-11-16\n297.0\nNaN\nNaN\n263.362368\n20.871707\n0.064757\n\n\n1\n25.333\n55.517\n2008-04-12\n333.0\nNaN\nNaN\n288.285193\n20.092428\n0.238166\n\n\n2\n25.255\n55.364\n2008-08-28\n430.0\n0.0\nNaN\n334.163706\n41.938701\n-0.053538\n\n\n3\n25.255\n55.364\n2008-02-17\n264.0\n0.0\nNaN\n284.628558\n33.558587\n0.490206\n\n\n4\n25.255\n55.364\n2008-11-25\n291.0\nNaN\nNaN\n263.449083\n18.027183\n0.083244"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-1",
    "href": "pages/slides/skrub-intro/skrub-intro.html#interpolationjoiner-1",
    "title": "Skrub",
    "section": "InterpolationJoiner",
    "text": "InterpolationJoiner"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-4",
    "href": "pages/slides/skrub-intro/skrub-intro.html#augmenting-features-through-joining-4",
    "title": "Skrub",
    "section": "Augmenting features through joining",
    "text": "Augmenting features through joining\n\nJoiner: Perform fuzzy-joining: join columns that contain similar-looking values.\nAggJoiner Aggregate an auxiliary dataframe before joining it on a base dataframe, and create new features that aggregate (sum, mean, mode…) the values in the columns.\nMultiAggJoiner extends AggJoiner to a multi-table scenario.\nInterpolationJoiner Perform an equi-join and estimate what missing rows would contain if they existed in the table.\n\n\nAll Joiner objects are scikit-learn estimators, so they can be used in a Pipeline."
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-deduplication",
    "href": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-deduplication",
    "title": "Skrub",
    "section": "Additional goodies: deduplication",
    "text": "Additional goodies: deduplication\ndeduplicate misspelled categories\n\nfrom skrub import deduplicate\npprint(duplicated)\ndeduplicate_correspondence = deduplicate(duplicated)\npprint(deduplicate_correspondence.to_dict())\n\n['ulack', 'black', 'black', 'xudte', 'white', 'white']\n{'black': 'black', 'ulack': 'black', 'white': 'white', 'xudte': 'white'}\n\n\nDoc"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-wikipedia-embeddings-as-features",
    "href": "pages/slides/skrub-intro/skrub-intro.html#additional-goodies-wikipedia-embeddings-as-features",
    "title": "Skrub",
    "section": "Additional goodies: Wikipedia embeddings as features",
    "text": "Additional goodies: Wikipedia embeddings as features\nKEN embeddings capture relational information about all entities in Wikipedia.\nfrom skrub.datasets import fetch_ken_embeddings\nembedding_games = fetch_ken_embeddings(\n    search_types=\"game\",\n    exclude=\"companies|developer\",\n    embedding_table_id=\"games\",\n)\n\nDoc"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#install-skrub",
    "href": "pages/slides/skrub-intro/skrub-intro.html#install-skrub",
    "title": "Skrub",
    "section": "Install skrub",
    "text": "Install skrub\n\nBase installation:\n# with pip\npip install skrub -U\n# with conda\nconda install -c conda-forge skrub\n\n\nFor deep learning features such as TextEncoder:\n# with pip\npip install skrub[transformers] -U\n# with conda\nconda install -c conda-forge skrub[transformers]\n\n\nDocumentation"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#join-the-community",
    "href": "pages/slides/skrub-intro/skrub-intro.html#join-the-community",
    "title": "Skrub",
    "section": "Join the community",
    "text": "Join the community\n\nSkrub website\nSkrub materials website\nGit repository\nDiscord server\nBluesky"
  },
  {
    "objectID": "pages/slides/skrub-intro/skrub-intro.html#contribute-to-skrub",
    "href": "pages/slides/skrub-intro/skrub-intro.html#contribute-to-skrub",
    "title": "Skrub",
    "section": "Contribute to skrub",
    "text": "Contribute to skrub\n\nOpen an issue on GitHub\nCheck out the documentation on how to contribute"
  }
]