---
title: A Skrub use case in academia
author: "Riccardo Cappuzzo"
institute: "Inria"

format: 
    revealjs:
        slide-number: c/t
        show-slide-number: all
        preview-links: auto
        embed-resources: false
        transition: slide
        theme: league
        logo: images/skrub.svg
        footer: https://github.com/skrub-data/skrub
incremental: true
---

## Plan for the presentation
- Context and explanation of the problem
- The Retrieve, Merge, Predict pipeline
- Discussion and conclusions

# Riddle me this...
::: {.fragment}
I have a table I want to train a model on. I also have access to a large collection of tables. 
:::


::: {.fragment}
::: {.callout}
How do I combine the two to train a better model? 
:::
:::

## Example: augmenting tables {auto-animate="true" auto-animate-unmatched="false"}

![](images/alice_retrieve_merge_predict_1.png)

## Example: augmenting tables {auto-animate="true" auto-animate-unmatched="false"}
![](images/alice_retrieve_merge_predict_2.png)


## Example: augmenting tables {auto-animate="true" auto-animate-unmatched="false"}
![](images/alice_retrieve_merge_predict_3.png)

## Some definitions {.smaller}
- **Base table**: the table we want to augment ("Movies", "Housing"...)
- **Query column**: a column that should be used as join key ("Movie title", "Address"...)
- **Data lake**: an unstructured repository of many (thousands of...) "candidate tables"
- **Candidate table**: a table that may be useful for augmenting the base table ("Movie directors"...)
- **Candidate column**: a column in a candidate table that could be joined on the query column ("Title" in a table about filmographies)
- **Augmented table**: the result of joining the base table and a candidate table


::: {.fragment}
This terminology is slightly different from that used in the Skrub documentation
:::

## The focus of the study:
- Find the best way to discover candidates.
- Work within a defined computational budget
- Work with exact joins between a base table and multiple join candidates. 
- Guarantee that results are reproducible. 

## We do not consider:
- Entity matching or fuzzy joins (e.g., matching ‚ÄúNYT‚Äù and ‚ÄúThe New York Times‚Äù).
- Discovering the query column. 
- Multi-key joins

## Working with data lakes is hard 
- Some CSVs don't use commas
- Some CSVs have no (known) schema
- Some CSVs aren't CSVs, they're actually JSON files in disguise
- Some JSONs aren't JSONs, they're actually strings in disguise

::: {.fragment}
If you don't know the tables, **everyone is sus** ![](images/Amogus_non-free-left.png){.absolute bottom=0 right=30 width=10%}
:::

# Retrieve, Merge, Predict

## Pipeline schema

![](images/benchmark-pipeline-v5.5.png)

## Candidate retrieval {.smaller}

- **Exact Matching**: measure the exact Jaccard containment (JC) for each column in the data lake.
- **MinHash**: estimate the Jaccard containment, query to get columns with a JC larger than a threshold.
- **Hybrid MinHash**: query with MinHash, then measure the exact JC for the retrieved candidates.
- **Starmie** : use a language model to query candidate columns.

## Candidate selection

- **NoJoin**: Reference selector. 
- **Highest Containment Join**: Rank candidates by Jaccard Containment. 
- **Top-k Full Join**: Join the top-k candidates. 
- **Best Single Join**: Train a model on each candidate, select the best. 
- **Stepwise Greedy Join**: Like Best Single Join, but keep all good candidates. 


## Aggregation {auto-animate="true"}
![](images/left_join-v2.png)

## Aggregation {auto-animate="true"}
- **Any**: take one value at random from each group
- **Mean**: for each group, take the mean of numerical values and mode of categorical values
- **Deep Feature Synthesis (DFS)** [@DFS]: greedily generate new features (count, mean, sum‚Ä¶) to already present features. 

## Prediction 
- RidgeCV: linear baseline üìà 
- CatBoost: GDBT  üå≤
- ResNet: Neural Networks üß†
- RealMLP: Neural Networks üß†


## Some experimental results
![](images/dep_pair_full.png)

## Total compute time
| ML Model | Platform | Total compute time |
|:--------:|:--------:|:------------------:|
|  RidgeCV |    CPU   |      4y 3m 10d 7h     |
| CatBoost |    CPU   |     1y 3m 29d 21h     |
|  ResNet  |    GPU   |      5y 6m 23d 0h     |
|  RealMLP |    GPU   |     10y 7m 23d 3h     |
|   Total  |   Both   |     21y 9m 26d 8h     |



# What does this have to do with Skrub?

## "Features" of research code
Research code...

- Is mostly custom-made for a specific experiment
- Features little to no testing
- Often is poorly documented, or not at all
- Involves a lot of technical debt

## Skrub to the rescue {auto-animate="true"}
- Well tested code
- Good documentation
- Features cover much of the pipeline

## Skrub to the rescue {auto-animate="true"}
- The `Discover` object can replace (part of) the retrieval step.
- All the code for joining can be replaced by the `AggJoiner` or `MultiAggJoiner`.
- The `MultiAggJoiner` is an additional baseline.
- The `TableVectorizer` can handle automated preprocessing of the tables.
- Joined candidates can be examined quickly using the `TableReport`.

## "Skrubified" pipeline  {auto-animate="true"}
![](images/benchmark-pipeline-v5.5.png){data-id="img1"}

## "Skrubified" pipeline  {auto-animate="true"}
![](images/benchmark-pipeline-v7-all-skrub.png){data-id="img1"}

## With MultiAggJoiner {.smaller}

::: {.columns}
::: {.column}
```{.python}
def execute_join_all_candidates(
    source_table: pl.DataFrame, 
    index_cand: dict, 
    aggregation: str
):
    merged = source_table.clone()
    hashes = []
    for hash_, mdata in tqdm(
        index_cand.items(),
        total=len(index_cand),
        leave=False,
        desc="Full Join",
        position=2,
    ):
        cnd_md = mdata.candidate_metadata
        hashes.append(cnd_md["hash"])
        candidate_table = pl.read_parquet(cnd_md["full_path"])

        left_on = mdata.left_on
        right_on = mdata.right_on

        aggr_right = aggregate_table(
            candidate_table, right_on, aggregation_method=aggregation
        )

        merged = execute_join(
            merged,
            aggr_right,
            left_on=left_on,
            right_on=right_on,
            how="left",
            suffix="_" + hash_[:10],
        )

    return merged
```
:::
::: {.column}
```{.python}
# MOCK-UP
from skrub import MultiAggJoiner
def execute_join_all_candidates(
    source_table: pl.DataFrame, 
    candidate_tables: list, 
    candidate_keys: list
):
    joiner = MultiAggJoiner(candidate_tables, keys=candidate_keys)
    merged = joiner.fit_transform(source_table)

    return merged
```

:::
:::



# Wrapping up

## Repositories
::: {.nonincremental}

- [Retrieve, Merge, Predict website](https://soda-inria.github.io/retrieve-merge-predict/)
- [Retrieve, Merge, Predict repository](https://github.com/rcap107/retrieve-merge-predict)
- [YADL archive on Zenodo](https://zenodo.org/records/10624396)
- [YADL preparation repository](https://github.com/rcap107/YADL)
:::

## Conclusions and summary {.smaller}

::: {.column width="45%"}

::: {.fragment}
:::
- Tree-based models are more effective and more resilient than the alternatives
- Good table retrieval affects the whole pipeline
- Simple methods produce results comparable or even better  than more complex methods
:::


::: {.column width="50%"}
::: {.fragment}
- Skrub provides well-tested, well-documented code
- Skrub objects provide features that cover most of the pipeline
- In return, the pipeline helped deciding on relevant features. 
:::

::: {.fragment}
![](images/qr-code-bench-repo.png){width=40% fig-align="right"}
:::
